Oct 19 09:56:28 k8s-master kubelet[28168]: E1019 09:56:28.568852   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:56:26 k8s-master kubelet[28168]: E1019 09:56:26.955218   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:56:25 k8s-master kubelet[28168]: E1019 09:56:25.910987   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:56:23 k8s-master kubelet[28168]: E1019 09:56:23.331506   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:56:17 k8s-master kubelet[28168]: E1019 09:56:17.154756   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:56:15 k8s-master kubelet[28168]: E1019 09:56:15.959312   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:56:15 k8s-master kubelet[28168]: E1019 09:56:15.955072   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:56:15 k8s-master kubelet[28168]: - exit status 1
Oct 19 09:56:15 k8s-master kubelet[28168]: ; err: exit status 1, extraDiskErr: du command failed on /var/lib/docker/containers/464e17fa2054e8f821eb7914bba2b4513d8c333471a215cf90f13cec63f7c803 with output stdout: , stderr: du: cannot access ‘/var/lib/docker/containers/464e17fa2054e8f821eb7914bba2b4513d8c333471a215cf90f13cec63f7c803’: No such file or directory
Oct 19 09:56:15 k8s-master kubelet[28168]: - exit status 1, rootInodeErr: cmd [find /var/lib/docker/overlay/6d03311f1a4dadb37af71ab60098fd1fd14df4add864426a37015b82a2f46a50 -xdev -printf .] failed. stderr: find: ‘/var/lib/docker/overlay/6d03311f1a4dadb37af71ab60098fd1fd14df4add864426a37015b82a2f46a50’: No such file or directory
Oct 19 09:56:15 k8s-master kubelet[28168]: E1019 09:56:15.243061   28168 fsHandler.go:121] failed to collect filesystem stats - rootDiskErr: du command failed on /var/lib/docker/overlay/6d03311f1a4dadb37af71ab60098fd1fd14df4add864426a37015b82a2f46a50 with output stdout: , stderr: du: cannot access ‘/var/lib/docker/overlay/6d03311f1a4dadb37af71ab60098fd1fd14df4add864426a37015b82a2f46a50’: No such file or directory
Oct 19 09:56:14 k8s-master kubelet[28168]: - exit status 1
Oct 19 09:56:14 k8s-master kubelet[28168]: ; err: exit status 1, extraDiskErr: du command failed on /var/lib/docker/containers/0eafb455d94592e737e7b37eab5d915b7bc4acf6c05e85d3ebc2bc29484e057a with output stdout: , stderr: du: cannot access ‘/var/lib/docker/containers/0eafb455d94592e737e7b37eab5d915b7bc4acf6c05e85d3ebc2bc29484e057a’: No such file or directory
Oct 19 09:56:14 k8s-master kubelet[28168]: - exit status 1, rootInodeErr: cmd [find /var/lib/docker/overlay/7ac9cb4358a1e5ce0d25141c58d24dc94902de9330fa58175e54f6411457d9c0 -xdev -printf .] failed. stderr: find: ‘/var/lib/docker/overlay/7ac9cb4358a1e5ce0d25141c58d24dc94902de9330fa58175e54f6411457d9c0’: No such file or directory
Oct 19 09:56:14 k8s-master kubelet[28168]: E1019 09:56:14.641071   28168 fsHandler.go:121] failed to collect filesystem stats - rootDiskErr: du command failed on /var/lib/docker/overlay/7ac9cb4358a1e5ce0d25141c58d24dc94902de9330fa58175e54f6411457d9c0 with output stdout: , stderr: du: cannot access ‘/var/lib/docker/overlay/7ac9cb4358a1e5ce0d25141c58d24dc94902de9330fa58175e54f6411457d9c0’: No such file or directory
Oct 19 09:56:13 k8s-master kubelet[28168]: E1019 09:56:13.991126   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:56:13 k8s-master kubelet[28168]: E1019 09:56:13.674140   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:56:10 k8s-master kubelet[28168]: E1019 09:56:10.406067   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:56:09 k8s-master kubelet[28168]: E1019 09:56:09.806476   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:56:09 k8s-master kubelet[28168]: E1019 09:56:09.542278   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:56:06 k8s-master kubelet[28168]: E1019 09:56:06.634926   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:56:05 k8s-master kubelet[28168]: E1019 09:56:05.975261   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:56:05 k8s-master kubelet[28168]: E1019 09:56:05.354344   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:55:59 k8s-master kubelet[28168]: E1019 09:55:59.041149   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:55:55 k8s-master kubelet[28168]: I1019 09:55:55.166286   28168 reconciler.go:290] Volume detached for volume "kube-dns-config" (UniqueName: "kubernetes.io/configmap/43c65148-b450-11e7-9a5b-08002782873b-kube-dns-config") on node "k8s-master" DevicePath ""
Oct 19 09:55:55 k8s-master kubelet[28168]: I1019 09:55:55.166247   28168 reconciler.go:290] Volume detached for volume "kube-dns-token-72jgd" (UniqueName: "kubernetes.io/secret/43c65148-b450-11e7-9a5b-08002782873b-kube-dns-token-72jgd") on node "k8s-master" DevicePath ""
Oct 19 09:55:55 k8s-master kubelet[28168]: I1019 09:55:55.080888   28168 operation_generator.go:535] UnmountVolume.TearDown succeeded for volume "kubernetes.io/configmap/43c65148-b450-11e7-9a5b-08002782873b-kube-dns-config" (OuterVolumeSpecName: "kube-dns-config") pod "43c65148-b450-11e7-9a5b-08002782873b" (UID: "43c65148-b450-11e7-9a5b-08002782873b"). InnerVolumeSpecName "kube-dns-config". PluginName "kubernetes.io/configmap", VolumeGidValue ""
Oct 19 09:55:55 k8s-master kubelet[28168]: I1019 09:55:55.080671   28168 operation_generator.go:535] UnmountVolume.TearDown succeeded for volume "kubernetes.io/secret/43c65148-b450-11e7-9a5b-08002782873b-kube-dns-token-72jgd" (OuterVolumeSpecName: "kube-dns-token-72jgd") pod "43c65148-b450-11e7-9a5b-08002782873b" (UID: "43c65148-b450-11e7-9a5b-08002782873b"). InnerVolumeSpecName "kube-dns-token-72jgd". PluginName "kubernetes.io/secret", VolumeGidValue ""
Oct 19 09:55:55 k8s-master kubelet[28168]: I1019 09:55:55.065661   28168 reconciler.go:186] operationExecutor.UnmountVolume started for volume "kube-dns-token-72jgd" (UniqueName: "kubernetes.io/secret/43c65148-b450-11e7-9a5b-08002782873b-kube-dns-token-72jgd") pod "43c65148-b450-11e7-9a5b-08002782873b" (UID: "43c65148-b450-11e7-9a5b-08002782873b")
Oct 19 09:55:55 k8s-master kubelet[28168]: I1019 09:55:55.065597   28168 reconciler.go:186] operationExecutor.UnmountVolume started for volume "kube-dns-config" (UniqueName: "kubernetes.io/configmap/43c65148-b450-11e7-9a5b-08002782873b-kube-dns-config") pod "43c65148-b450-11e7-9a5b-08002782873b" (UID: "43c65148-b450-11e7-9a5b-08002782873b")
Oct 19 09:55:54 k8s-master kubelet[28168]: W1019 09:55:54.911281   28168 pod_container_deletor.go:77] Container "8a7d097f6822d7c60b3b3ae9397e69f3b9900beaf4145cac76b2d7213ea1ed28" not found in pod's containers
Oct 19 09:55:54 k8s-master kubelet[28168]: E1019 09:55:54.297502   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:55:54 k8s-master kubelet[28168]: E1019 09:55:54.291073   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:55:54 k8s-master kubelet[28168]: W1019 09:55:54.290939   28168 container.go:354] Failed to create summary reader for "/libcontainer_836_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 09:55:54 k8s-master kubelet[28168]: W1019 09:55:54.290782   28168 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_836_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_836_systemd_test_default.slice: no such file or directory
Oct 19 09:55:54 k8s-master kubelet[28168]: E1019 09:55:54.251732   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:55:54 k8s-master kubelet[28168]: W1019 09:55:54.251599   28168 container.go:354] Failed to create summary reader for "/libcontainer_833_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 09:55:54 k8s-master kubelet[28168]: W1019 09:55:54.240383   28168 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_833_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_833_systemd_test_default.slice: no such file or directory
Oct 19 09:55:47 k8s-master kubelet[28168]: W1019 09:55:47.878297   28168 prober.go:98] No ref for container "docker://2cb7146c1a914401405b3fa316cfc406c84b4d7ea44ce10a3259f8d9b0429dd0" (kube-dns-545bc4bfd4-kwztd_kube-system(43c65148-b450-11e7-9a5b-08002782873b):sidecar)
Oct 19 09:55:37 k8s-master kubelet[28168]: W1019 09:55:37.878193   28168 prober.go:98] No ref for container "docker://2cb7146c1a914401405b3fa316cfc406c84b4d7ea44ce10a3259f8d9b0429dd0" (kube-dns-545bc4bfd4-kwztd_kube-system(43c65148-b450-11e7-9a5b-08002782873b):sidecar)
Oct 19 09:55:27 k8s-master kubelet[28168]: W1019 09:55:27.878226   28168 prober.go:98] No ref for container "docker://2cb7146c1a914401405b3fa316cfc406c84b4d7ea44ce10a3259f8d9b0429dd0" (kube-dns-545bc4bfd4-kwztd_kube-system(43c65148-b450-11e7-9a5b-08002782873b):sidecar)
Oct 19 09:55:27 k8s-master kubelet[28168]: E1019 09:55:27.010709   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:55:24 k8s-master kubelet[28168]: I1019 09:55:24.225321   28168 container.go:471] Failed to update stats for container "/libcontainer_669_systemd_test_default.slice": failed to parse memory.usage_in_bytes - read /sys/fs/cgroup/memory/libcontainer_669_systemd_test_default.slice/memory.usage_in_bytes: no such device, continuing to push stats
Oct 19 09:55:24 k8s-master kubelet[28168]: E1019 09:55:24.032086   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:55:24 k8s-master kubelet[28168]: W1019 09:55:24.031806   28168 container.go:354] Failed to create summary reader for "/libcontainer_650_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 09:55:24 k8s-master kubelet[28168]: W1019 09:55:24.012727   28168 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_650_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_650_systemd_test_default.slice: no such file or directory
Oct 19 09:50:18 k8s-master kubelet[28168]: E1019 09:50:18.554604   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:40:18 k8s-master kubelet[28168]: E1019 09:40:18.779603   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:30:28 k8s-master kubelet[28168]: E1019 09:30:28.608090   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:30:17 k8s-master kubelet[28168]: E1019 09:30:17.525938   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:20:21 k8s-master kubelet[28168]: E1019 09:20:21.155003   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:10:23 k8s-master kubelet[28168]: E1019 09:10:23.434466   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:10:12 k8s-master kubelet[28168]: E1019 09:10:12.971747   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:01:21 k8s-master kubelet[28168]: E1019 09:01:21.040846   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:00:24 k8s-master kubelet[28168]: E1019 09:00:24.048772   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 09:00:12 k8s-master kubelet[28168]: E1019 09:00:12.707220   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 08:50:17 k8s-master kubelet[28168]: E1019 08:50:17.691009   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 08:40:12 k8s-master kubelet[28168]: E1019 08:40:12.860297   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 08:30:19 k8s-master kubelet[28168]: E1019 08:30:19.333927   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 08:20:18 k8s-master kubelet[28168]: E1019 08:20:18.161259   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 08:10:14 k8s-master kubelet[28168]: E1019 08:10:14.542767   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 08:01:28 k8s-master kubelet[28168]: E1019 08:01:28.096155   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 08:01:15 k8s-master kubelet[28168]: E1019 08:01:15.794383   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 08:00:26 k8s-master kubelet[28168]: E1019 08:00:26.349957   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 08:00:13 k8s-master kubelet[28168]: E1019 08:00:13.059330   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 07:50:19 k8s-master kubelet[28168]: E1019 07:50:19.787782   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 07:40:16 k8s-master kubelet[28168]: E1019 07:40:16.674115   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 07:30:18 k8s-master kubelet[28168]: E1019 07:30:18.451082   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 07:20:12 k8s-master kubelet[28168]: E1019 07:20:12.863048   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 07:10:25 k8s-master kubelet[28168]: E1019 07:10:25.564199   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 07:10:12 k8s-master kubelet[28168]: E1019 07:10:12.444946   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 07:01:20 k8s-master kubelet[28168]: E1019 07:01:20.046490   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 07:00:19 k8s-master kubelet[28168]: E1019 07:00:19.691003   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:50:18 k8s-master kubelet[28168]: E1019 06:50:18.640990   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:40:17 k8s-master kubelet[28168]: E1019 06:40:17.830568   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:30:18 k8s-master kubelet[28168]: E1019 06:30:18.508321   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:20:23 k8s-master kubelet[28168]: E1019 06:20:23.682734   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:20:13 k8s-master kubelet[28168]: E1019 06:20:13.516276   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:10:17 k8s-master kubelet[28168]: E1019 06:10:17.532982   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:05:26 k8s-master kubelet[28168]: E1019 06:05:26.722068   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:05:26 k8s-master kubelet[28168]: E1019 06:05:26.497031   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:05:23 k8s-master kubelet[28168]: E1019 06:05:23.878630   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:05:20 k8s-master kubelet[28168]: E1019 06:05:20.170380   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:05:19 k8s-master kubelet[28168]: E1019 06:05:19.385837   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:05:18 k8s-master kubelet[28168]: E1019 06:05:18.418975   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:05:17 k8s-master kubelet[28168]: E1019 06:05:17.744590   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:05:17 k8s-master kubelet[28168]: E1019 06:05:17.205399   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:05:15 k8s-master kubelet[28168]: E1019 06:05:15.233824   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:05:14 k8s-master kubelet[28168]: E1019 06:05:14.561982   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:05:14 k8s-master kubelet[28168]: E1019 06:05:14.317864   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:05:10 k8s-master kubelet[28168]: E1019 06:05:10.681005   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:05:05 k8s-master kubelet[28168]: E1019 06:05:05.604606   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:05:04 k8s-master kubelet[28168]: E1019 06:05:04.455169   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:05:02 k8s-master kubelet[28168]: E1019 06:05:02.949566   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:05:01 k8s-master kubelet[28168]: E1019 06:05:01.168667   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:54 k8s-master kubelet[28168]: E1019 06:04:54.023498   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:52 k8s-master kubelet[28168]: E1019 06:04:52.701786   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:51 k8s-master kubelet[28168]: E1019 06:04:51.351428   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:50 k8s-master kubelet[28168]: E1019 06:04:50.239672   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:48 k8s-master kubelet[28168]: E1019 06:04:48.655619   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:48 k8s-master kubelet[28168]: E1019 06:04:48.391264   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:47 k8s-master kubelet[28168]: E1019 06:04:47.053894   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:46 k8s-master kubelet[28168]: E1019 06:04:46.701500   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:46 k8s-master kubelet[28168]: E1019 06:04:46.418012   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:46 k8s-master kubelet[28168]: E1019 06:04:46.117583   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:36 k8s-master kubelet[28168]: E1019 06:04:36.913148   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:36 k8s-master kubelet[28168]: W1019 06:04:36.913037   28168 container.go:354] Failed to create summary reader for "/libcontainer_28977_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 06:04:36 k8s-master kubelet[28168]: W1019 06:04:36.861977   28168 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_28977_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_28977_systemd_test_default.slice: no such file or directory
Oct 19 06:04:36 k8s-master kubelet[28168]: E1019 06:04:36.772867   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:36 k8s-master kubelet[28168]: W1019 06:04:36.772696   28168 container.go:354] Failed to create summary reader for "/libcontainer_28965_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 06:04:36 k8s-master kubelet[28168]: W1019 06:04:36.761118   28168 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_28965_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_28965_systemd_test_default.slice: no such file or directory
Oct 19 06:04:36 k8s-master kubelet[28168]: I1019 06:04:36.622555   28168 kuberuntime_manager.go:499] Container {Name:kube-flannel Image:quay.io/coreos/flannel:v0.9.0-amd64 Command:[/opt/bin/flanneld --ip-masq --kube-subnet-mgr] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[{Name:POD_NAME Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {Name:POD_NAMESPACE Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}}] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:run ReadOnly:false MountPath:/run SubPath: MountPropagation:<nil>} {Name:flannel-cfg ReadOnly:false MountPath:/etc/kube-flannel/ SubPath: MountPropagation:<nil>} {Name:flannel-token-5h2vm ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 19 06:04:35 k8s-master kubelet[28168]: E1019 06:04:35.765191   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:35 k8s-master kubelet[28168]: W1019 06:04:35.765053   28168 container.go:354] Failed to create summary reader for "/libcontainer_28944_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 06:04:35 k8s-master kubelet[28168]: W1019 06:04:35.747354   28168 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_28944_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_28944_systemd_test_default.slice: no such file or directory
Oct 19 06:04:35 k8s-master kubelet[28168]: E1019 06:04:35.711290   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:35 k8s-master kubelet[28168]: W1019 06:04:35.710890   28168 container.go:354] Failed to create summary reader for "/libcontainer_28941_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 06:04:35 k8s-master kubelet[28168]: W1019 06:04:35.701106   28168 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_28941_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_28941_systemd_test_default.slice: no such file or directory
Oct 19 06:04:35 k8s-master kubelet[28168]: E1019 06:04:35.665382   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:35 k8s-master kubelet[28168]: W1019 06:04:35.665227   28168 container.go:354] Failed to create summary reader for "/libcontainer_28935_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 06:04:35 k8s-master kubelet[28168]: W1019 06:04:35.662079   28168 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_28935_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_28935_systemd_test_default.slice: no such file or directory
Oct 19 06:04:35 k8s-master kubelet[28168]: E1019 06:04:35.620409   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:35 k8s-master kubelet[28168]: E1019 06:04:35.620343   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:35 k8s-master kubelet[28168]: W1019 06:04:35.620234   28168 container.go:354] Failed to create summary reader for "/libcontainer_28930_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 06:04:35 k8s-master kubelet[28168]: W1019 06:04:35.574290   28168 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_28930_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_28930_systemd_test_default.slice: no such file or directory
Oct 19 06:04:35 k8s-master kubelet[28168]: E1019 06:04:35.478600   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:35 k8s-master kubelet[28168]: W1019 06:04:35.478567   28168 container.go:354] Failed to create summary reader for "/libcontainer_28912_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 06:04:35 k8s-master kubelet[28168]: W1019 06:04:35.308419   28168 pod_container_deletor.go:77] Container "d8472b22294c06470c916a7f0c62d0bd38953091cef6befba77dadb319356f6c" not found in pod's containers
Oct 19 06:04:35 k8s-master kubelet[28168]: W1019 06:04:35.296184   28168 container.go:354] Failed to create summary reader for "/libcontainer_28896_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 06:04:35 k8s-master kubelet[28168]: W1019 06:04:35.237905   28168 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_28896_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_28896_systemd_test_default.slice: no such file or directory
Oct 19 06:04:35 k8s-master kubelet[28168]: E1019 06:04:35.156554   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:35 k8s-master kubelet[28168]: W1019 06:04:35.156165   28168 container.go:354] Failed to create summary reader for "/libcontainer_28882_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 06:04:35 k8s-master kubelet[28168]: W1019 06:04:35.137283   28168 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_28882_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_28882_systemd_test_default.slice: no such file or directory
Oct 19 06:04:34 k8s-master kubelet[28168]: I1019 06:04:34.675853   28168 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "flannel-cfg" (UniqueName: "kubernetes.io/configmap/533ed1ae-b450-11e7-9a5b-08002782873b-flannel-cfg") pod "kube-flannel-ds-tk965" (UID: "533ed1ae-b450-11e7-9a5b-08002782873b")
Oct 19 06:04:34 k8s-master kubelet[28168]: I1019 06:04:34.675830   28168 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "flannel-token-5h2vm" (UniqueName: "kubernetes.io/secret/533ed1ae-b450-11e7-9a5b-08002782873b-flannel-token-5h2vm") pod "kube-flannel-ds-tk965" (UID: "533ed1ae-b450-11e7-9a5b-08002782873b")
Oct 19 06:04:34 k8s-master kubelet[28168]: I1019 06:04:34.675801   28168 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "cni" (UniqueName: "kubernetes.io/host-path/533ed1ae-b450-11e7-9a5b-08002782873b-cni") pod "kube-flannel-ds-tk965" (UID: "533ed1ae-b450-11e7-9a5b-08002782873b")
Oct 19 06:04:34 k8s-master kubelet[28168]: I1019 06:04:34.675754   28168 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "run" (UniqueName: "kubernetes.io/host-path/533ed1ae-b450-11e7-9a5b-08002782873b-run") pod "kube-flannel-ds-tk965" (UID: "533ed1ae-b450-11e7-9a5b-08002782873b")
Oct 19 06:04:28 k8s-master kubelet[28168]: E1019 06:04:28.545553   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:27 k8s-master kubelet[28168]: E1019 06:04:27.537674   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:26 k8s-master kubelet[28168]: E1019 06:04:26.870106   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:26 k8s-master kubelet[28168]: E1019 06:04:26.829380   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:26 k8s-master kubelet[28168]: E1019 06:04:26.319813   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:26 k8s-master kubelet[28168]: E1019 06:04:26.279390   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:26 k8s-master kubelet[28168]: E1019 06:04:26.272043   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:26 k8s-master kubelet[28168]: E1019 06:04:26.221500   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:25 k8s-master kubelet[28168]: E1019 06:04:25.543136   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:25 k8s-master kubelet[28168]: E1019 06:04:25.189714   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:23 k8s-master kubelet[28168]: E1019 06:04:23.149421   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:23 k8s-master kubelet[28168]: E1019 06:04:23.091063   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:22 k8s-master kubelet[28168]: E1019 06:04:22.325414   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:22 k8s-master kubelet[28168]: E1019 06:04:22.117040   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:21 k8s-master kubelet[28168]: E1019 06:04:21.768998   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:20 k8s-master kubelet[28168]: E1019 06:04:20.983363   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:20 k8s-master kubelet[28168]: E1019 06:04:20.483700   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:18 k8s-master kubelet[28168]: E1019 06:04:18.654639   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:16 k8s-master kubelet[28168]: I1019 06:04:16.395485   28168 kubelet_network.go:276] Setting Pod CIDR:  -> 10.244.0.0/24
Oct 19 06:04:16 k8s-master kubelet[28168]: I1019 06:04:16.395292   28168 docker_service.go:306] docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}
Oct 19 06:04:16 k8s-master kubelet[28168]: I1019 06:04:16.395096   28168 kuberuntime_manager.go:898] updating runtime config through cri with podcidr 10.244.0.0/24
Oct 19 06:04:16 k8s-master kubelet[28168]: E1019 06:04:16.156688   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:13 k8s-master kubelet[28168]: E1019 06:04:13.726025   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:11 k8s-master kubelet[28168]: E1019 06:04:11.490867   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:11 k8s-master kubelet[28168]: W1019 06:04:11.464062   28168 container.go:354] Failed to create summary reader for "/libcontainer_28801_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 06:04:11 k8s-master kubelet[28168]: W1019 06:04:11.035926   28168 kuberuntime_container.go:191] Non-root verification doesn't support non-numeric user (nobody)
Oct 19 06:04:11 k8s-master kubelet[28168]: E1019 06:04:10.984105   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:11 k8s-master kubelet[28168]: W1019 06:04:10.983965   28168 container.go:354] Failed to create summary reader for "/libcontainer_28737_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 06:04:10 k8s-master kubelet[28168]: W1019 06:04:10.873000   28168 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_28737_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_28737_systemd_test_default.slice: no such file or directory
Oct 19 06:04:10 k8s-master kubelet[28168]: E1019 06:04:10.541825   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:10 k8s-master kubelet[28168]: W1019 06:04:10.541681   28168 container.go:354] Failed to create summary reader for "/libcontainer_28698_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 06:04:09 k8s-master kubelet[28168]: W1019 06:04:09.913416   28168 pod_container_deletor.go:77] Container "8a7d097f6822d7c60b3b3ae9397e69f3b9900beaf4145cac76b2d7213ea1ed28" not found in pod's containers
Oct 19 06:04:09 k8s-master kubelet[28168]: E1019 06:04:09.913069   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:09 k8s-master kubelet[28168]: E1019 06:04:09.908255   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:09 k8s-master kubelet[28168]: W1019 06:04:09.908157   28168 container.go:354] Failed to create summary reader for "/libcontainer_28644_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 06:04:09 k8s-master kubelet[28168]: W1019 06:04:09.908036   28168 container.go:354] Failed to create summary reader for "/libcontainer_28620_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 06:04:09 k8s-master kubelet[28168]: W1019 06:04:09.800321   28168 pod_container_deletor.go:77] Container "99d6fe19a6f3944474b885ad57ef08c7985fcc0681faf49e9f970d2b767d6619" not found in pod's containers
Oct 19 06:04:09 k8s-master kubelet[28168]: E1019 06:04:09.791868   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:09 k8s-master kubelet[28168]: W1019 06:04:09.768783   28168 container.go:354] Failed to create summary reader for "/libcontainer_28608_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 06:04:09 k8s-master kubelet[28168]: W1019 06:04:09.683240   28168 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_28620_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_28620_systemd_test_default.slice: no such file or directory
Oct 19 06:04:08 k8s-master kubelet[28168]: E1019 06:04:08.996670   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:08 k8s-master kubelet[28168]: I1019 06:04:08.712224   28168 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-dns-config" (UniqueName: "kubernetes.io/configmap/43c65148-b450-11e7-9a5b-08002782873b-kube-dns-config") pod "kube-dns-545bc4bfd4-kwztd" (UID: "43c65148-b450-11e7-9a5b-08002782873b")
Oct 19 06:04:08 k8s-master kubelet[28168]: I1019 06:04:08.712192   28168 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-proxy-token-mpwvh" (UniqueName: "kubernetes.io/secret/43c6f54f-b450-11e7-9a5b-08002782873b-kube-proxy-token-mpwvh") pod "kube-proxy-x6xqq" (UID: "43c6f54f-b450-11e7-9a5b-08002782873b")
Oct 19 06:04:08 k8s-master kubelet[28168]: I1019 06:04:08.712155   28168 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "xtables-lock" (UniqueName: "kubernetes.io/host-path/43c6f54f-b450-11e7-9a5b-08002782873b-xtables-lock") pod "kube-proxy-x6xqq" (UID: "43c6f54f-b450-11e7-9a5b-08002782873b")
Oct 19 06:04:08 k8s-master kubelet[28168]: I1019 06:04:08.712115   28168 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-proxy" (UniqueName: "kubernetes.io/configmap/43c6f54f-b450-11e7-9a5b-08002782873b-kube-proxy") pod "kube-proxy-x6xqq" (UID: "43c6f54f-b450-11e7-9a5b-08002782873b")
Oct 19 06:04:08 k8s-master kubelet[28168]: I1019 06:04:08.712071   28168 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-dns-token-72jgd" (UniqueName: "kubernetes.io/secret/43c65148-b450-11e7-9a5b-08002782873b-kube-dns-token-72jgd") pod "kube-dns-545bc4bfd4-kwztd" (UID: "43c65148-b450-11e7-9a5b-08002782873b")
Oct 19 06:04:05 k8s-master kubelet[28168]: E1019 06:04:05.010819   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:01 k8s-master kubelet[28168]: E1019 06:04:01.278192   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:04:00 k8s-master kubelet[28168]: E1019 06:04:00.329033   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:03:58 k8s-master kubelet[28168]: E1019 06:03:58.833595   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:03:56 k8s-master kubelet[28168]: E1019 06:03:56.628365   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:03:53 k8s-master kubelet[28168]: E1019 06:03:53.444649   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:03:52 k8s-master kubelet[28168]: E1019 06:03:52.643554   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:03:52 k8s-master kubelet[28168]: E1019 06:03:52.397257   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:03:48 k8s-master kubelet[28168]: E1019 06:03:48.242639   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:03:48 k8s-master kubelet[28168]: E1019 06:03:48.032146   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:03:47 k8s-master kubelet[28168]: E1019 06:03:47.952258   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:03:46 k8s-master kubelet[28168]: E1019 06:03:46.494277   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:03:46 k8s-master kubelet[28168]: E1019 06:03:46.206975   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:03:46 k8s-master kubelet[28168]: E1019 06:03:46.072689   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:03:45 k8s-master kubelet[28168]: I1019 06:03:45.991934   28168 kubelet_node_status.go:86] Successfully registered node k8s-master
Oct 19 06:03:41 k8s-master kubelet[28168]: I1019 06:03:41.896513   28168 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 19 06:03:41 k8s-master kubelet[28168]: I1019 06:03:41.893250   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:41 k8s-master kubelet[28168]: E1019 06:03:41.303833   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:41 k8s-master kubelet[28168]: E1019 06:03:41.125155   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:40 k8s-master kubelet[28168]: E1019 06:03:40.510662   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:40 k8s-master kubelet[28168]: E1019 06:03:40.293852   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:40 k8s-master kubelet[28168]: E1019 06:03:40.112217   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:39 k8s-master kubelet[28168]: E1019 06:03:39.492154   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:39 k8s-master kubelet[28168]: E1019 06:03:39.288874   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:39 k8s-master kubelet[28168]: E1019 06:03:39.233996   28168 eviction_manager.go:238] eviction manager: unexpected err: failed to get node info: node 'k8s-master' not found
Oct 19 06:03:39 k8s-master kubelet[28168]: E1019 06:03:39.101935   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:38 k8s-master kubelet[28168]: E1019 06:03:38.898519   28168 kubelet.go:1612] Failed creating a mirror pod for "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:38 k8s-master kubelet[28168]: E1019 06:03:38.703571   28168 kubelet.go:1612] Failed creating a mirror pod for "kube-controller-manager-k8s-master_kube-system(bb9d91cce4fabaa2cee45daaca8f4424)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:38 k8s-master kubelet[28168]: E1019 06:03:38.487161   28168 kubelet.go:1612] Failed creating a mirror pod for "kube-apiserver-k8s-master_kube-system(3d8aa988a2efa6e85f760a7d0537316d)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:38 k8s-master kubelet[28168]: I1019 06:03:38.435815   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:38 k8s-master kubelet[28168]: I1019 06:03:38.435470   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:38 k8s-master kubelet[28168]: I1019 06:03:38.433974   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:38 k8s-master kubelet[28168]: E1019 06:03:38.291739   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:38 k8s-master kubelet[28168]: E1019 06:03:38.096723   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:37 k8s-master kubelet[28168]: E1019 06:03:37.975602   28168 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/default/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 19 06:03:37 k8s-master kubelet[28168]: E1019 06:03:37.910543   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:37 k8s-master kubelet[28168]: W1019 06:03:37.695670   28168 status_manager.go:431] Failed to get status for pod "kube-apiserver-k8s-master_kube-system(3d8aa988a2efa6e85f760a7d0537316d)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:37 k8s-master kubelet[28168]: E1019 06:03:37.498711   28168 kubelet.go:1612] Failed creating a mirror pod for "kube-apiserver-k8s-master_kube-system(3d8aa988a2efa6e85f760a7d0537316d)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:37 k8s-master kubelet[28168]: I1019 06:03:37.424136   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:37 k8s-master kubelet[28168]: E1019 06:03:37.419735   28168 kubelet.go:1612] Failed creating a mirror pod for "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:37 k8s-master kubelet[28168]: I1019 06:03:37.413201   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:37 k8s-master kubelet[28168]: E1019 06:03:37.409309   28168 kubelet.go:1612] Failed creating a mirror pod for "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:37 k8s-master kubelet[28168]: W1019 06:03:37.409227   28168 status_manager.go:431] Failed to get status for pod "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:37 k8s-master kubelet[28168]: I1019 06:03:37.380421   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:37 k8s-master kubelet[28168]: E1019 06:03:37.376937   28168 kubelet.go:1612] Failed creating a mirror pod for "kube-controller-manager-k8s-master_kube-system(bb9d91cce4fabaa2cee45daaca8f4424)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:37 k8s-master kubelet[28168]: W1019 06:03:37.376860   28168 status_manager.go:431] Failed to get status for pod "kube-controller-manager-k8s-master_kube-system(bb9d91cce4fabaa2cee45daaca8f4424)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:37 k8s-master kubelet[28168]: I1019 06:03:37.370292   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:36 k8s-master kubelet[28168]: E1019 06:03:36.927735   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:36 k8s-master kubelet[28168]: E1019 06:03:36.927499   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:36 k8s-master kubelet[28168]: E1019 06:03:36.905215   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:36 k8s-master kubelet[28168]: W1019 06:03:36.428301   28168 pod_container_deletor.go:77] Container "399c417a47d66ccf24be34694e35e25f023564799ddbb6998a738ff7a29ef006" not found in pod's containers
Oct 19 06:03:36 k8s-master kubelet[28168]: E1019 06:03:36.411232   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:03:36 k8s-master kubelet[28168]: E1019 06:03:36.382890   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:03:36 k8s-master kubelet[28168]: W1019 06:03:36.382775   28168 container.go:354] Failed to create summary reader for "/libcontainer_28435_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 06:03:36 k8s-master kubelet[28168]: W1019 06:03:36.382634   28168 container.go:354] Failed to create summary reader for "/libcontainer_28432_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 06:03:36 k8s-master kubelet[28168]: I1019 06:03:36.347914   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:36 k8s-master kubelet[28168]: W1019 06:03:36.143331   28168 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_28435_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_28435_systemd_test_default.slice: no such file or directory
Oct 19 06:03:36 k8s-master kubelet[28168]: W1019 06:03:36.088628   28168 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_28432_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_28432_systemd_test_default.slice: no such file or directory
Oct 19 06:03:35 k8s-master kubelet[28168]: E1019 06:03:35.926585   28168 kubelet.go:1612] Failed creating a mirror pod for "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:35 k8s-master kubelet[28168]: W1019 06:03:35.926511   28168 status_manager.go:431] Failed to get status for pod "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/etcd-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:35 k8s-master kubelet[28168]: I1019 06:03:35.919813   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:35 k8s-master kubelet[28168]: W1019 06:03:35.906808   28168 pod_container_deletor.go:77] Container "45335b173626af792b29ebe9f3cd35467bdf8c81e2c576b7c7da535b20f51a27" not found in pod's containers
Oct 19 06:03:35 k8s-master kubelet[28168]: E1019 06:03:35.904390   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:35 k8s-master kubelet[28168]: E1019 06:03:35.904336   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:35 k8s-master kubelet[28168]: E1019 06:03:35.904277   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:35 k8s-master kubelet[28168]: E1019 06:03:35.867435   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:03:35 k8s-master kubelet[28168]: W1019 06:03:35.867327   28168 container.go:354] Failed to create summary reader for "/libcontainer_28376_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 06:03:35 k8s-master kubelet[28168]: I1019 06:03:35.840869   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:35 k8s-master kubelet[28168]: W1019 06:03:35.699240   28168 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_28376_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_28376_systemd_test_default.slice: no such file or directory
Oct 19 06:03:35 k8s-master kubelet[28168]: E1019 06:03:35.492551   28168 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:35 k8s-master kubelet[28168]: I1019 06:03:35.492179   28168 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 19 06:03:35 k8s-master kubelet[28168]: I1019 06:03:35.489265   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:35 k8s-master kubelet[28168]: W1019 06:03:35.427686   28168 pod_container_deletor.go:77] Container "9afa9c30de7d27ea30ae9beddd3ef2409f5f455de528c5d1b75685b597f91c72" not found in pod's containers
Oct 19 06:03:35 k8s-master kubelet[28168]: W1019 06:03:35.419493   28168 container.go:354] Failed to create summary reader for "/libcontainer_28325_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 06:03:35 k8s-master kubelet[28168]: I1019 06:03:35.383514   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:35 k8s-master kubelet[28168]: W1019 06:03:35.288551   28168 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_28325_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_28325_systemd_test_default.slice: no such file or directory
Oct 19 06:03:34 k8s-master kubelet[28168]: E1019 06:03:34.870592   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:34 k8s-master kubelet[28168]: E1019 06:03:34.870455   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:34 k8s-master kubelet[28168]: E1019 06:03:34.870187   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:34 k8s-master kubelet[28168]: E1019 06:03:34.603371   28168 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 06:03:34 k8s-master kubelet[28168]: W1019 06:03:34.603241   28168 container.go:354] Failed to create summary reader for "/libcontainer_28229_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 06:03:34 k8s-master kubelet[28168]: W1019 06:03:34.592805   28168 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_28229_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_28229_systemd_test_default.slice: no such file or directory
Oct 19 06:03:34 k8s-master kubelet[28168]: I1019 06:03:34.145054   28168 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/f3e5e477637a31dd77e4d4e3534d2e23-kubeconfig") pod "kube-scheduler-k8s-master" (UID: "f3e5e477637a31dd77e4d4e3534d2e23")
Oct 19 06:03:34 k8s-master kubelet[28168]: I1019 06:03:34.038668   28168 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/bb9d91cce4fabaa2cee45daaca8f4424-kubeconfig") pod "kube-controller-manager-k8s-master" (UID: "bb9d91cce4fabaa2cee45daaca8f4424")
Oct 19 06:03:34 k8s-master kubelet[28168]: I1019 06:03:34.038646   28168 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs-etc-pki" (UniqueName: "kubernetes.io/host-path/3d8aa988a2efa6e85f760a7d0537316d-ca-certs-etc-pki") pod "kube-apiserver-k8s-master" (UID: "3d8aa988a2efa6e85f760a7d0537316d")
Oct 19 06:03:34 k8s-master kubelet[28168]: I1019 06:03:34.038627   28168 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs" (UniqueName: "kubernetes.io/host-path/3d8aa988a2efa6e85f760a7d0537316d-ca-certs") pod "kube-apiserver-k8s-master" (UID: "3d8aa988a2efa6e85f760a7d0537316d")
Oct 19 06:03:34 k8s-master kubelet[28168]: I1019 06:03:34.038606   28168 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs-etc-pki" (UniqueName: "kubernetes.io/host-path/bb9d91cce4fabaa2cee45daaca8f4424-ca-certs-etc-pki") pod "kube-controller-manager-k8s-master" (UID: "bb9d91cce4fabaa2cee45daaca8f4424")
Oct 19 06:03:34 k8s-master kubelet[28168]: I1019 06:03:34.038580   28168 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs" (UniqueName: "kubernetes.io/host-path/bb9d91cce4fabaa2cee45daaca8f4424-ca-certs") pod "kube-controller-manager-k8s-master" (UID: "bb9d91cce4fabaa2cee45daaca8f4424")
Oct 19 06:03:34 k8s-master kubelet[28168]: I1019 06:03:34.038559   28168 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "k8s-certs" (UniqueName: "kubernetes.io/host-path/bb9d91cce4fabaa2cee45daaca8f4424-k8s-certs") pod "kube-controller-manager-k8s-master" (UID: "bb9d91cce4fabaa2cee45daaca8f4424")
Oct 19 06:03:34 k8s-master kubelet[28168]: I1019 06:03:34.038538   28168 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "k8s-certs" (UniqueName: "kubernetes.io/host-path/3d8aa988a2efa6e85f760a7d0537316d-k8s-certs") pod "kube-apiserver-k8s-master" (UID: "3d8aa988a2efa6e85f760a7d0537316d")
Oct 19 06:03:34 k8s-master kubelet[28168]: I1019 06:03:34.038513   28168 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "etcd" (UniqueName: "kubernetes.io/host-path/40eb0889c614345e2a2714d4ee7d1cc0-etcd") pod "etcd-k8s-master" (UID: "40eb0889c614345e2a2714d4ee7d1cc0")
Oct 19 06:03:34 k8s-master kubelet[28168]: I1019 06:03:34.038466   28168 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "flexvolume-dir" (UniqueName: "kubernetes.io/host-path/bb9d91cce4fabaa2cee45daaca8f4424-flexvolume-dir") pod "kube-controller-manager-k8s-master" (UID: "bb9d91cce4fabaa2cee45daaca8f4424")
Oct 19 06:03:34 k8s-master kubelet[28168]: E1019 06:03:34.015678   28168 kubelet.go:1612] Failed creating a mirror pod for "kube-controller-manager-k8s-master_kube-system(bb9d91cce4fabaa2cee45daaca8f4424)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:34 k8s-master kubelet[28168]: E1019 06:03:34.015232   28168 kubelet.go:1612] Failed creating a mirror pod for "kube-apiserver-k8s-master_kube-system(3d8aa988a2efa6e85f760a7d0537316d)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:33 k8s-master kubelet[28168]: E1019 06:03:33.959971   28168 kubelet.go:1612] Failed creating a mirror pod for "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:33 k8s-master kubelet[28168]: W1019 06:03:33.959901   28168 status_manager.go:431] Failed to get status for pod "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:33 k8s-master kubelet[28168]: W1019 06:03:33.955416   28168 status_manager.go:431] Failed to get status for pod "kube-controller-manager-k8s-master_kube-system(bb9d91cce4fabaa2cee45daaca8f4424)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:33 k8s-master kubelet[28168]: I1019 06:03:33.955086   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:33 k8s-master kubelet[28168]: W1019 06:03:33.948743   28168 status_manager.go:431] Failed to get status for pod "kube-apiserver-k8s-master_kube-system(3d8aa988a2efa6e85f760a7d0537316d)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:33 k8s-master kubelet[28168]: I1019 06:03:33.948426   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:33 k8s-master kubelet[28168]: I1019 06:03:33.947249   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:33 k8s-master kubelet[28168]: E1019 06:03:33.943422   28168 kubelet.go:1612] Failed creating a mirror pod for "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:33 k8s-master kubelet[28168]: W1019 06:03:33.943375   28168 status_manager.go:431] Failed to get status for pod "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/etcd-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:33 k8s-master kubelet[28168]: I1019 06:03:33.942915   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:33 k8s-master kubelet[28168]: I1019 06:03:33.942025   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:33 k8s-master kubelet[28168]: I1019 06:03:33.938890   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:33 k8s-master kubelet[28168]: I1019 06:03:33.938532   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:33 k8s-master kubelet[28168]: I1019 06:03:33.936500   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:33 k8s-master kubelet[28168]: E1019 06:03:33.852654   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:33 k8s-master kubelet[28168]: E1019 06:03:33.851324   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:33 k8s-master kubelet[28168]: E1019 06:03:33.848781   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:32 k8s-master kubelet[28168]: E1019 06:03:32.852166   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:32 k8s-master kubelet[28168]: E1019 06:03:32.850773   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:32 k8s-master kubelet[28168]: E1019 06:03:32.847824   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:32 k8s-master kubelet[28168]: E1019 06:03:32.255777   28168 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:32 k8s-master kubelet[28168]: I1019 06:03:32.254158   28168 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 19 06:03:32 k8s-master kubelet[28168]: I1019 06:03:32.251428   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:31 k8s-master kubelet[28168]: E1019 06:03:31.851561   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:31 k8s-master kubelet[28168]: E1019 06:03:31.849631   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:31 k8s-master kubelet[28168]: E1019 06:03:31.844231   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:30 k8s-master kubelet[28168]: E1019 06:03:30.850899   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:30 k8s-master kubelet[28168]: E1019 06:03:30.848853   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:30 k8s-master kubelet[28168]: E1019 06:03:30.839269   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:30 k8s-master kubelet[28168]: E1019 06:03:30.650552   28168 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:30 k8s-master kubelet[28168]: I1019 06:03:30.650094   28168 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 19 06:03:30 k8s-master kubelet[28168]: I1019 06:03:30.644674   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:29 k8s-master kubelet[28168]: E1019 06:03:29.850240   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:29 k8s-master kubelet[28168]: E1019 06:03:29.847523   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:29 k8s-master kubelet[28168]: E1019 06:03:29.844359   28168 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:29 k8s-master kubelet[28168]: I1019 06:03:29.843999   28168 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 19 06:03:29 k8s-master kubelet[28168]: I1019 06:03:29.840888   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:29 k8s-master kubelet[28168]: E1019 06:03:29.833778   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:29 k8s-master kubelet[28168]: E1019 06:03:29.440405   28168 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:29 k8s-master kubelet[28168]: I1019 06:03:29.439951   28168 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 19 06:03:29 k8s-master kubelet[28168]: I1019 06:03:29.436532   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:29 k8s-master kubelet[28168]: E1019 06:03:29.235753   28168 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:29 k8s-master kubelet[28168]: I1019 06:03:29.235328   28168 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 19 06:03:29 k8s-master kubelet[28168]: E1019 06:03:29.233472   28168 eviction_manager.go:238] eviction manager: unexpected err: failed to get node info: node 'k8s-master' not found
Oct 19 06:03:29 k8s-master kubelet[28168]: I1019 06:03:29.114755   28168 manager.go:316] Recovery completed
Oct 19 06:03:29 k8s-master kubelet[28168]: I1019 06:03:29.108285   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:29 k8s-master kubelet[28168]: I1019 06:03:28.998948   28168 manager.go:311] Starting recovery of all containers
Oct 19 06:03:29 k8s-master kubelet[28168]: I1019 06:03:28.997539   28168 manager.go:1140] Started watching for new ooms in manager
Oct 19 06:03:29 k8s-master kubelet[28168]: I1019 06:03:28.997411   28168 factory.go:86] Registering Raw factory
Oct 19 06:03:29 k8s-master kubelet[28168]: I1019 06:03:28.997277   28168 factory.go:54] Registering systemd factory
Oct 19 06:03:29 k8s-master kubelet[28168]: W1019 06:03:28.997265   28168 manager.go:276] Registration of the crio container factory failed: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 19 06:03:29 k8s-master kubelet[28168]: W1019 06:03:28.997127   28168 manager.go:265] Registration of the rkt container factory failed: unable to communicate with Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 19 06:03:29 k8s-master kubelet[28168]: I1019 06:03:28.997097   28168 factory.go:355] Registering Docker factory
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.937347   28168 volume_manager.go:246] Starting Kubelet Volume Manager
Oct 19 06:03:28 k8s-master kubelet[28168]: E1019 06:03:28.937320   28168 container_manager_linux.go:603] [ContainerManager]: Fail to get rootfs information unable to find data for container /
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.936049   28168 kubelet.go:1779] skipping pod synchronization - [container runtime is down PLEG is not healthy: pleg was last seen active 2562047h47m16.854775807s ago; threshold is 3m0s]
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.936035   28168 kubelet.go:1768] Starting kubelet main sync loop.
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.936020   28168 status_manager.go:140] Starting to sync pod status with apiserver
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.935978   28168 fs_resource_analyzer.go:66] Starting FS ResourceAnalyzer
Oct 19 06:03:28 k8s-master kubelet[28168]: E1019 06:03:28.917111   28168 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/default/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.914026   28168 server.go:296] Adding debug handlers to kubelet server.
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.913070   28168 server.go:128] Starting to listen on 0.0.0.0:10250
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.912666   28168 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 06:03:28 k8s-master kubelet[28168]: E1019 06:03:28.911745   28168 kubelet.go:1234] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data for container /
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.911014   28168 server.go:718] Started kubelet v1.8.1
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.908036   28168 kuberuntime_manager.go:177] Container runtime docker initialized, version: 17.10.0-ce, apiVersion: 1.33.0
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.903724   28168 remote_runtime.go:43] Connecting to runtime service unix:///var/run/dockershim.sock
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.879974   28168 docker_service.go:224] Setting cgroupDriver to systemd
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.862497   28168 docker_service.go:207] Docker cri networking managed by kubernetes.io/no-op
Oct 19 06:03:28 k8s-master kubelet[28168]: W1019 06:03:28.854665   28168 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.849822   28168 kubelet.go:517] Hairpin mode set to "hairpin-veth"
Oct 19 06:03:28 k8s-master kubelet[28168]: W1019 06:03:28.849793   28168 kubelet_network.go:69] Hairpin mode set to "promiscuous-bridge" but kubenet is not enabled, falling back to "hairpin-veth"
Oct 19 06:03:28 k8s-master kubelet[28168]: E1019 06:03:28.826135   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:28 k8s-master kubelet[28168]: E1019 06:03:28.826100   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:28 k8s-master kubelet[28168]: E1019 06:03:28.826042   28168 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.767643   28168 kubelet.go:283] Watching apiserver
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.767597   28168 kubelet.go:273] Adding manifest file: /etc/kubernetes/manifests
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.767486   28168 container_manager_linux.go:288] Creating device plugin handler: false
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.767358   28168 container_manager_linux.go:257] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: ContainerRuntime:docker CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[{Signal:memory.available Operator:LessThan Value:{Quantity:100Mi Percentage:0} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.1} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.inodesFree Operator:LessThan Value:{Quantity:<nil> Percentage:0.05} GracePeriod:0s MinReclaim:<nil>}]} ExperimentalQOSReserved:map[] ExperimentalCPUManagerPolicy:none ExperimentalCPUManagerReconcilePeriod:10s}
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.767310   28168 container_manager_linux.go:252] container manager verified user specified cgroup-root exists: /
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.763643   28168 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.762874   28168 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.760464   28168 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true}] DiskMap:map[8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq} 253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:flannel.1 MacAddress:aa:be:27:bb:bf:3e Speed:0 Mtu:1450} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.754602   28168 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0}]
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.754564   28168 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 19 06:03:28 k8s-master kubelet[28168]: W1019 06:03:28.735616   28168 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 19 06:03:28 k8s-master kubelet[28168]: W1019 06:03:28.735433   28168 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.712471   28168 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 19 06:03:28 k8s-master kubelet[28168]: E1019 06:03:28.711726   28168 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.710503   28168 certificate_manager.go:361] Requesting new certificate.
Oct 19 06:03:28 k8s-master kubelet[28168]: W1019 06:03:28.683434   28168 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.683276   28168 feature_gate.go:156] feature gates: map[]
Oct 19 06:03:28 k8s-master kubelet[28168]: W1019 06:03:28.672793   28168 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.670907   28168 client.go:95] Start docker client with request timeout=2m0s
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.670863   28168 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.658983   28168 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.658977   28168 controller.go:114] kubelet config controller: starting controller
Oct 19 06:03:28 k8s-master kubelet[28168]: I1019 06:03:28.658866   28168 feature_gate.go:156] feature gates: map[]
Oct 19 06:03:28 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 19 06:03:28 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 19 06:03:28 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 19 06:03:18 k8s-master systemd[1]: kubelet.service failed.
Oct 19 06:03:18 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 19 06:03:18 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 19 06:03:18 k8s-master kubelet[28157]: error: failed to run Kubelet: unable to load bootstrap kubeconfig: stat /etc/kubernetes/bootstrap-kubelet.conf: no such file or directory
Oct 19 06:03:18 k8s-master kubelet[28157]: W1019 06:03:18.263747   28157 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 19 06:03:18 k8s-master kubelet[28157]: I1019 06:03:18.263593   28157 feature_gate.go:156] feature gates: map[]
Oct 19 06:03:18 k8s-master kubelet[28157]: W1019 06:03:18.202063   28157 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 19 06:03:18 k8s-master kubelet[28157]: I1019 06:03:18.199251   28157 client.go:95] Start docker client with request timeout=2m0s
Oct 19 06:03:18 k8s-master kubelet[28157]: I1019 06:03:18.199210   28157 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 19 06:03:17 k8s-master kubelet[28157]: I1019 06:03:17.626598   28157 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 19 06:03:17 k8s-master kubelet[28157]: I1019 06:03:17.626592   28157 controller.go:114] kubelet config controller: starting controller
Oct 19 06:03:17 k8s-master kubelet[28157]: I1019 06:03:17.626276   28157 feature_gate.go:156] feature gates: map[]
Oct 19 06:03:17 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 19 06:03:17 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 19 06:00:56 k8s-master systemd[1]: kubelet.service failed.
Oct 19 06:00:56 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 19 06:00:56 k8s-master systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Oct 19 06:00:56 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 19 06:00:56 k8s-master systemd[1]: Stopping kubelet: The Kubernetes Node Agent...
Oct 19 05:50:11 k8s-master kubelet[24413]: E1019 05:50:11.946274   24413 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:47:05 k8s-master kubelet[24413]: E1019 05:47:05.481776   24413 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:46:52 k8s-master kubelet[24413]: E1019 05:46:52.263429   24413 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:44:11 k8s-master kubelet[24413]: E1019 05:44:11.883534   24413 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:44:06 k8s-master kubelet[24413]: E1019 05:44:06.568661   24413 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:44:01 k8s-master kubelet[24413]: E1019 05:44:01.597171   24413 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:43:53 k8s-master kubelet[24413]: E1019 05:43:53.419719   24413 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:43:39 k8s-master kubelet[24413]: E1019 05:43:39.736544   24413 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:43:32 k8s-master kubelet[24413]: E1019 05:43:32.937197   24413 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:43:22 k8s-master kubelet[24413]: I1019 05:43:22.618315   24413 transport.go:88] certificate rotation detected, shutting down client connections to start using new credentials
Oct 19 05:43:19 k8s-master kubelet[24413]: E1019 05:43:19.955979   24413 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:43:19 k8s-master kubelet[24413]: W1019 05:43:19.955878   24413 container.go:354] Failed to create summary reader for "/libcontainer_24670_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:43:19 k8s-master kubelet[24413]: W1019 05:43:19.914399   24413 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24670_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24670_systemd_test_default.slice: no such file or directory
Oct 19 05:43:19 k8s-master kubelet[24413]: E1019 05:43:19.811027   24413 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:43:19 k8s-master kubelet[24413]: W1019 05:43:19.810765   24413 container.go:354] Failed to create summary reader for "/libcontainer_24656_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:43:19 k8s-master kubelet[24413]: W1019 05:43:19.805472   24413 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24656_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): readdirent: no such file or directory
Oct 19 05:43:19 k8s-master kubelet[24413]: I1019 05:43:19.410063   24413 kuberuntime_manager.go:738] checking backoff for container "kube-flannel" in pod "kube-flannel-ds-n747n_kube-system(b6af75e4-b41a-11e7-8c24-08002782873b)"
Oct 19 05:43:19 k8s-master kubelet[24413]: I1019 05:43:19.409945   24413 kuberuntime_manager.go:499] Container {Name:kube-flannel Image:quay.io/coreos/flannel:v0.9.0-amd64 Command:[/opt/bin/flanneld --ip-masq --kube-subnet-mgr] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[{Name:POD_NAME Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {Name:POD_NAMESPACE Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}}] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:run ReadOnly:false MountPath:/run SubPath: MountPropagation:<nil>} {Name:flannel-cfg ReadOnly:false MountPath:/etc/kube-flannel/ SubPath: MountPropagation:<nil>} {Name:flannel-token-2sfpp ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 19 05:43:18 k8s-master kubelet[24413]: I1019 05:43:18.018587   24413 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs-etc-pki" (UniqueName: "kubernetes.io/host-path/79f7c686bb6d415a1d71ac57fc06f7aa-ca-certs-etc-pki") pod "kube-controller-manager-k8s-master" (UID: "79f7c686bb6d415a1d71ac57fc06f7aa")
Oct 19 05:43:18 k8s-master kubelet[24413]: I1019 05:43:18.018568   24413 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "flexvolume-dir" (UniqueName: "kubernetes.io/host-path/79f7c686bb6d415a1d71ac57fc06f7aa-flexvolume-dir") pod "kube-controller-manager-k8s-master" (UID: "79f7c686bb6d415a1d71ac57fc06f7aa")
Oct 19 05:43:18 k8s-master kubelet[24413]: I1019 05:43:18.018549   24413 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs" (UniqueName: "kubernetes.io/host-path/79f7c686bb6d415a1d71ac57fc06f7aa-ca-certs") pod "kube-controller-manager-k8s-master" (UID: "79f7c686bb6d415a1d71ac57fc06f7aa")
Oct 19 05:43:18 k8s-master kubelet[24413]: I1019 05:43:18.018526   24413 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "k8s-certs" (UniqueName: "kubernetes.io/host-path/79f7c686bb6d415a1d71ac57fc06f7aa-k8s-certs") pod "kube-controller-manager-k8s-master" (UID: "79f7c686bb6d415a1d71ac57fc06f7aa")
Oct 19 05:43:18 k8s-master kubelet[24413]: I1019 05:43:18.018508   24413 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "xtables-lock" (UniqueName: "kubernetes.io/host-path/b2564e2e-b419-11e7-8c24-08002782873b-xtables-lock") pod "kube-proxy-rrmr6" (UID: "b2564e2e-b419-11e7-8c24-08002782873b")
Oct 19 05:43:18 k8s-master kubelet[24413]: I1019 05:43:18.018489   24413 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs-etc-pki" (UniqueName: "kubernetes.io/host-path/d857b5bcdfd340d1ff3546b68f7f27c0-ca-certs-etc-pki") pod "kube-apiserver-k8s-master" (UID: "d857b5bcdfd340d1ff3546b68f7f27c0")
Oct 19 05:43:18 k8s-master kubelet[24413]: I1019 05:43:18.018467   24413 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "flannel-cfg" (UniqueName: "kubernetes.io/configmap/b6af75e4-b41a-11e7-8c24-08002782873b-flannel-cfg") pod "kube-flannel-ds-n747n" (UID: "b6af75e4-b41a-11e7-8c24-08002782873b")
Oct 19 05:43:18 k8s-master kubelet[24413]: I1019 05:43:18.018446   24413 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-proxy-token-jx5p4" (UniqueName: "kubernetes.io/secret/b2564e2e-b419-11e7-8c24-08002782873b-kube-proxy-token-jx5p4") pod "kube-proxy-rrmr6" (UID: "b2564e2e-b419-11e7-8c24-08002782873b")
Oct 19 05:43:18 k8s-master kubelet[24413]: I1019 05:43:18.018423   24413 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-proxy" (UniqueName: "kubernetes.io/configmap/b2564e2e-b419-11e7-8c24-08002782873b-kube-proxy") pod "kube-proxy-rrmr6" (UID: "b2564e2e-b419-11e7-8c24-08002782873b")
Oct 19 05:43:18 k8s-master kubelet[24413]: I1019 05:43:18.018400   24413 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/79f7c686bb6d415a1d71ac57fc06f7aa-kubeconfig") pod "kube-controller-manager-k8s-master" (UID: "79f7c686bb6d415a1d71ac57fc06f7aa")
Oct 19 05:43:18 k8s-master kubelet[24413]: I1019 05:43:18.018377   24413 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs" (UniqueName: "kubernetes.io/host-path/d857b5bcdfd340d1ff3546b68f7f27c0-ca-certs") pod "kube-apiserver-k8s-master" (UID: "d857b5bcdfd340d1ff3546b68f7f27c0")
Oct 19 05:43:18 k8s-master kubelet[24413]: I1019 05:43:18.018357   24413 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "k8s-certs" (UniqueName: "kubernetes.io/host-path/d857b5bcdfd340d1ff3546b68f7f27c0-k8s-certs") pod "kube-apiserver-k8s-master" (UID: "d857b5bcdfd340d1ff3546b68f7f27c0")
Oct 19 05:43:18 k8s-master kubelet[24413]: I1019 05:43:18.018324   24413 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "cni" (UniqueName: "kubernetes.io/host-path/b6af75e4-b41a-11e7-8c24-08002782873b-cni") pod "kube-flannel-ds-n747n" (UID: "b6af75e4-b41a-11e7-8c24-08002782873b")
Oct 19 05:43:18 k8s-master kubelet[24413]: I1019 05:43:18.018298   24413 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "run" (UniqueName: "kubernetes.io/host-path/b6af75e4-b41a-11e7-8c24-08002782873b-run") pod "kube-flannel-ds-n747n" (UID: "b6af75e4-b41a-11e7-8c24-08002782873b")
Oct 19 05:43:18 k8s-master kubelet[24413]: I1019 05:43:18.018278   24413 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/f3e5e477637a31dd77e4d4e3534d2e23-kubeconfig") pod "kube-scheduler-k8s-master" (UID: "f3e5e477637a31dd77e4d4e3534d2e23")
Oct 19 05:43:18 k8s-master kubelet[24413]: I1019 05:43:18.018256   24413 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "etcd" (UniqueName: "kubernetes.io/host-path/40eb0889c614345e2a2714d4ee7d1cc0-etcd") pod "etcd-k8s-master" (UID: "40eb0889c614345e2a2714d4ee7d1cc0")
Oct 19 05:43:18 k8s-master kubelet[24413]: I1019 05:43:18.018232   24413 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "flannel-token-2sfpp" (UniqueName: "kubernetes.io/secret/b6af75e4-b41a-11e7-8c24-08002782873b-flannel-token-2sfpp") pod "kube-flannel-ds-n747n" (UID: "b6af75e4-b41a-11e7-8c24-08002782873b")
Oct 19 05:43:18 k8s-master kubelet[24413]: I1019 05:43:18.018205   24413 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-dns-token-xdjjs" (UniqueName: "kubernetes.io/secret/b2593c3c-b419-11e7-8c24-08002782873b-kube-dns-token-xdjjs") pod "kube-dns-545bc4bfd4-l2xsw" (UID: "b2593c3c-b419-11e7-8c24-08002782873b")
Oct 19 05:43:18 k8s-master kubelet[24413]: I1019 05:43:18.018157   24413 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-dns-config" (UniqueName: "kubernetes.io/configmap/b2593c3c-b419-11e7-8c24-08002782873b-kube-dns-config") pod "kube-dns-545bc4bfd4-l2xsw" (UID: "b2593c3c-b419-11e7-8c24-08002782873b")
Oct 19 05:43:17 k8s-master kubelet[24413]: W1019 05:43:17.908156   24413 pod_container_deletor.go:77] Container "b3d4007b7a1975f340eee2f9610ad3e0f556787989e3ed7d69c4a3e212682d6c" not found in pod's containers
Oct 19 05:43:17 k8s-master kubelet[24413]: W1019 05:43:17.908016   24413 pod_container_deletor.go:77] Container "d3a074dda4d57ef9ef26c6864debc7b88b7a821bcde403287f41ff9f452229f5" not found in pod's containers
Oct 19 05:43:17 k8s-master kubelet[24413]: W1019 05:43:17.907969   24413 pod_container_deletor.go:77] Container "7a23c426cfe17e8905996fa3422d545de550a7d9721abd90ef2a73eb84192569" not found in pod's containers
Oct 19 05:43:17 k8s-master kubelet[24413]: W1019 05:43:17.907925   24413 pod_container_deletor.go:77] Container "bfffdedfc9c2566eb58bcfbd52daed7bb9185a2eb35712c09568e9c780d01a63" not found in pod's containers
Oct 19 05:43:17 k8s-master kubelet[24413]: W1019 05:43:17.907856   24413 pod_container_deletor.go:77] Container "3126fa7f4c1433d55c8ae9e611633a4e0ea4357758ff221747442e886df61a51" not found in pod's containers
Oct 19 05:43:17 k8s-master kubelet[24413]: W1019 05:43:17.907801   24413 pod_container_deletor.go:77] Container "0aa57a2b259f35efb69f184207a304c3083fa863be74defafb752caba73c44dd" not found in pod's containers
Oct 19 05:43:17 k8s-master kubelet[24413]: W1019 05:43:17.907725   24413 pod_container_deletor.go:77] Container "e3e7eaf2f6ae026074b50ec52de9ca4fe2918d33f90c46e1bfe03780afe6f34f" not found in pod's containers
Oct 19 05:43:14 k8s-master kubelet[24413]: W1019 05:43:14.074608   24413 docker_sandbox.go:343] failed to read pod IP from plugin/docker: Couldn't find network status for kube-system/kube-dns-545bc4bfd4-l2xsw through plugin: invalid network status for
Oct 19 05:43:13 k8s-master kubelet[24413]: I1019 05:43:13.791129   24413 manager.go:316] Recovery completed
Oct 19 05:43:13 k8s-master kubelet[24413]: I1019 05:43:13.513603   24413 kubelet_node_status.go:787] Node became not ready: {Type:Ready Status:False LastHeartbeatTime:2017-10-19 05:43:13.5135772 +0800 CST LastTransitionTime:2017-10-19 05:43:13.5135772 +0800 CST Reason:KubeletNotReady Message:container runtime is down}
Oct 19 05:43:13 k8s-master kubelet[24413]: I1019 05:43:13.475726   24413 kubelet_network.go:276] Setting Pod CIDR:  -> 10.244.0.0/24
Oct 19 05:43:13 k8s-master kubelet[24413]: I1019 05:43:13.474544   24413 docker_service.go:306] docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}
Oct 19 05:43:13 k8s-master kubelet[24413]: I1019 05:43:13.439539   24413 kuberuntime_manager.go:898] updating runtime config through cri with podcidr 10.244.0.0/24
Oct 19 05:43:13 k8s-master kubelet[24413]: I1019 05:43:13.423381   24413 kubelet_node_status.go:86] Successfully registered node k8s-master
Oct 19 05:43:13 k8s-master kubelet[24413]: I1019 05:43:13.423287   24413 kubelet_node_status.go:134] Node k8s-master was previously registered
Oct 19 05:43:13 k8s-master kubelet[24413]: I1019 05:43:13.274561   24413 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 19 05:43:13 k8s-master kubelet[24413]: I1019 05:43:13.112379   24413 manager.go:311] Starting recovery of all containers
Oct 19 05:43:13 k8s-master kubelet[24413]: I1019 05:43:13.111119   24413 manager.go:1140] Started watching for new ooms in manager
Oct 19 05:43:13 k8s-master kubelet[24413]: I1019 05:43:13.110887   24413 factory.go:86] Registering Raw factory
Oct 19 05:43:13 k8s-master kubelet[24413]: I1019 05:43:13.110648   24413 factory.go:54] Registering systemd factory
Oct 19 05:43:13 k8s-master kubelet[24413]: W1019 05:43:13.110635   24413 manager.go:276] Registration of the crio container factory failed: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 19 05:43:13 k8s-master kubelet[24413]: W1019 05:43:13.110485   24413 manager.go:265] Registration of the rkt container factory failed: unable to communicate with Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 19 05:43:13 k8s-master kubelet[24413]: I1019 05:43:13.110452   24413 factory.go:355] Registering Docker factory
Oct 19 05:43:13 k8s-master kubelet[24413]: I1019 05:43:13.022116   24413 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 19 05:43:12 k8s-master kubelet[24413]: I1019 05:43:12.915740   24413 volume_manager.go:246] Starting Kubelet Volume Manager
Oct 19 05:43:12 k8s-master kubelet[24413]: E1019 05:43:12.913589   24413 container_manager_linux.go:603] [ContainerManager]: Fail to get rootfs information unable to find data for container /
Oct 19 05:43:12 k8s-master kubelet[24413]: I1019 05:43:12.908109   24413 server.go:296] Adding debug handlers to kubelet server.
Oct 19 05:43:12 k8s-master kubelet[24413]: I1019 05:43:12.907259   24413 server.go:128] Starting to listen on 0.0.0.0:10250
Oct 19 05:43:12 k8s-master kubelet[24413]: I1019 05:43:12.907101   24413 kubelet.go:1779] skipping pod synchronization - [container runtime is down PLEG is not healthy: pleg was last seen active 2562047h47m16.854775807s ago; threshold is 3m0s]
Oct 19 05:43:12 k8s-master kubelet[24413]: I1019 05:43:12.907083   24413 kubelet.go:1768] Starting kubelet main sync loop.
Oct 19 05:43:12 k8s-master kubelet[24413]: I1019 05:43:12.907067   24413 status_manager.go:140] Starting to sync pod status with apiserver
Oct 19 05:43:12 k8s-master kubelet[24413]: I1019 05:43:12.907028   24413 fs_resource_analyzer.go:66] Starting FS ResourceAnalyzer
Oct 19 05:43:12 k8s-master kubelet[24413]: E1019 05:43:12.904867   24413 kubelet.go:1234] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data for container /
Oct 19 05:43:12 k8s-master kubelet[24413]: I1019 05:43:12.904502   24413 server.go:718] Started kubelet v1.8.1
Oct 19 05:43:12 k8s-master kubelet[24413]: I1019 05:43:12.901480   24413 kuberuntime_manager.go:177] Container runtime docker initialized, version: 17.10.0-ce, apiVersion: 1.33.0
Oct 19 05:43:12 k8s-master kubelet[24413]: I1019 05:43:12.898136   24413 remote_runtime.go:43] Connecting to runtime service unix:///var/run/dockershim.sock
Oct 19 05:43:12 k8s-master kubelet[24413]: I1019 05:43:12.859265   24413 docker_service.go:224] Setting cgroupDriver to systemd
Oct 19 05:43:12 k8s-master kubelet[24413]: I1019 05:43:12.840705   24413 docker_service.go:207] Docker cri networking managed by kubernetes.io/no-op
Oct 19 05:43:12 k8s-master kubelet[24413]: I1019 05:43:12.821567   24413 kubelet.go:517] Hairpin mode set to "hairpin-veth"
Oct 19 05:43:12 k8s-master kubelet[24413]: W1019 05:43:12.821537   24413 kubelet_network.go:69] Hairpin mode set to "promiscuous-bridge" but kubenet is not enabled, falling back to "hairpin-veth"
Oct 19 05:43:12 k8s-master kubelet[24413]: I1019 05:43:12.788068   24413 kubelet.go:283] Watching apiserver
Oct 19 05:43:12 k8s-master kubelet[24413]: I1019 05:43:12.788007   24413 kubelet.go:273] Adding manifest file: /etc/kubernetes/manifests
Oct 19 05:43:12 k8s-master kubelet[24413]: I1019 05:43:12.787865   24413 container_manager_linux.go:288] Creating device plugin handler: false
Oct 19 05:43:12 k8s-master kubelet[24413]: I1019 05:43:12.787685   24413 container_manager_linux.go:257] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: ContainerRuntime:docker CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[{Signal:memory.available Operator:LessThan Value:{Quantity:100Mi Percentage:0} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.1} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.inodesFree Operator:LessThan Value:{Quantity:<nil> Percentage:0.05} GracePeriod:0s MinReclaim:<nil>}]} ExperimentalQOSReserved:map[] ExperimentalCPUManagerPolicy:none ExperimentalCPUManagerReconcilePeriod:10s}
Oct 19 05:43:12 k8s-master kubelet[24413]: I1019 05:43:12.787647   24413 container_manager_linux.go:252] container manager verified user specified cgroup-root exists: /
Oct 19 05:43:12 k8s-master kubelet[24413]: I1019 05:43:12.785310   24413 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 19 05:43:12 k8s-master kubelet[24413]: I1019 05:43:12.784716   24413 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 19 05:43:12 k8s-master kubelet[24413]: I1019 05:43:12.781357   24413 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true} {Device:shm DeviceMajor:0 DeviceMinor:43 Capacity:67108864 Type:vfs Inodes:235563 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:flannel.1 MacAddress:aa:be:27:bb:bf:3e Speed:0 Mtu:1450} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 19 05:43:12 k8s-master kubelet[24413]: I1019 05:43:12.777762   24413 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0} shm:{mountpoint:/var/lib/docker/containers/5ef75e7d64880799e0c240e299336fad9e4d06071f89ac6dbd636b396db805c6/shm major:0 minor:43 fsType:tmpfs blockSize:0}]
Oct 19 05:43:12 k8s-master kubelet[24413]: I1019 05:43:12.777713   24413 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 19 05:43:12 k8s-master kubelet[24413]: W1019 05:43:12.754394   24413 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 19 05:43:12 k8s-master kubelet[24413]: W1019 05:43:12.754237   24413 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 19 05:43:12 k8s-master kubelet[24413]: I1019 05:43:12.698619   24413 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 19 05:43:12 k8s-master kubelet[24413]: I1019 05:43:12.630059   24413 certificate_manager.go:361] Requesting new certificate.
Oct 19 05:43:11 k8s-master kubelet[24413]: W1019 05:43:11.972410   24413 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 19 05:43:11 k8s-master kubelet[24413]: I1019 05:43:11.972243   24413 feature_gate.go:156] feature gates: map[]
Oct 19 05:43:11 k8s-master kubelet[24413]: I1019 05:43:11.947650   24413 client.go:95] Start docker client with request timeout=2m0s
Oct 19 05:43:11 k8s-master kubelet[24413]: I1019 05:43:11.947607   24413 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 19 05:43:08 k8s-master kubelet[24413]: I1019 05:43:08.517162   24413 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 19 05:43:08 k8s-master kubelet[24413]: I1019 05:43:08.517136   24413 controller.go:114] kubelet config controller: starting controller
Oct 19 05:43:08 k8s-master kubelet[24413]: I1019 05:43:08.516857   24413 feature_gate.go:156] feature gates: map[]
Oct 19 05:43:04 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 19 05:43:04 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 19 05:43:04 k8s-master systemd[1]: kubelet.service failed.
Oct 19 05:43:04 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 19 05:43:04 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 19 05:43:04 k8s-master systemd[1]: Stopping kubelet: The Kubernetes Node Agent...
Oct 19 05:43:04 k8s-master kubelet[25301]: E1019 05:43:04.100160   25301 pod_workers.go:182] Error syncing pod b6af75e4-b41a-11e7-8c24-08002782873b ("kube-flannel-ds-n747n_kube-system(b6af75e4-b41a-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kube-flannel" with CrashLoopBackOff: "Back-off 20s restarting failed container=kube-flannel pod=kube-flannel-ds-n747n_kube-system(b6af75e4-b41a-11e7-8c24-08002782873b)"
Oct 19 05:43:04 k8s-master kubelet[25301]: I1019 05:43:04.100128   25301 kuberuntime_manager.go:748] Back-off 20s restarting failed container=kube-flannel pod=kube-flannel-ds-n747n_kube-system(b6af75e4-b41a-11e7-8c24-08002782873b)
Oct 19 05:43:04 k8s-master kubelet[25301]: I1019 05:43:04.100006   25301 kuberuntime_manager.go:738] checking backoff for container "kube-flannel" in pod "kube-flannel-ds-n747n_kube-system(b6af75e4-b41a-11e7-8c24-08002782873b)"
Oct 19 05:43:04 k8s-master kubelet[25301]: I1019 05:43:04.099732   25301 kuberuntime_manager.go:499] Container {Name:kube-flannel Image:quay.io/coreos/flannel:v0.9.0-amd64 Command:[/opt/bin/flanneld --ip-masq --kube-subnet-mgr] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[{Name:POD_NAME Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {Name:POD_NAMESPACE Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}}] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:run ReadOnly:false MountPath:/run SubPath: MountPropagation:<nil>} {Name:flannel-cfg ReadOnly:false MountPath:/etc/kube-flannel/ SubPath: MountPropagation:<nil>} {Name:flannel-token-2sfpp ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 19 05:43:02 k8s-master kubelet[25301]: E1019 05:43:02.449169   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:43:02 k8s-master kubelet[25301]: W1019 05:43:02.449040   25301 container.go:354] Failed to create summary reader for "/libcontainer_24388_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:43:02 k8s-master kubelet[25301]: W1019 05:43:02.435958   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24388_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24388_systemd_test_default.slice: no such file or directory
Oct 19 05:43:02 k8s-master kubelet[25301]: W1019 05:43:02.392025   25301 container.go:354] Failed to create summary reader for "/libcontainer_24385_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:43:02 k8s-master kubelet[25301]: W1019 05:43:02.382654   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24385_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24385_systemd_test_default.slice: no such file or directory
Oct 19 05:43:02 k8s-master kubelet[25301]: E1019 05:43:02.341343   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:43:02 k8s-master kubelet[25301]: W1019 05:43:02.341106   25301 container.go:354] Failed to create summary reader for "/libcontainer_24382_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:43:02 k8s-master kubelet[25301]: W1019 05:43:02.338791   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24382_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24382_systemd_test_default.slice: no such file or directory
Oct 19 05:43:01 k8s-master kubelet[25301]: - exit status 1
Oct 19 05:43:01 k8s-master kubelet[25301]: ; err: exit status 1, extraDiskErr: du command failed on /var/lib/docker/containers/dba5b9aacbc397838201742778227ccabda915ed75976c2975e9ab0a619e354a with output stdout: , stderr: du: cannot access ‘/var/lib/docker/containers/dba5b9aacbc397838201742778227ccabda915ed75976c2975e9ab0a619e354a’: No such file or directory
Oct 19 05:43:01 k8s-master kubelet[25301]: - exit status 1, rootInodeErr: cmd [find /var/lib/docker/overlay/61cc0fc62471c80849105a67040e16c3f4e3fc9992fe832b6564e63e105cd34c -xdev -printf .] failed. stderr: find: ‘/var/lib/docker/overlay/61cc0fc62471c80849105a67040e16c3f4e3fc9992fe832b6564e63e105cd34c’: No such file or directory
Oct 19 05:43:01 k8s-master kubelet[25301]: E1019 05:43:01.656337   25301 fsHandler.go:121] failed to collect filesystem stats - rootDiskErr: du command failed on /var/lib/docker/overlay/61cc0fc62471c80849105a67040e16c3f4e3fc9992fe832b6564e63e105cd34c with output stdout: , stderr: du: cannot access ‘/var/lib/docker/overlay/61cc0fc62471c80849105a67040e16c3f4e3fc9992fe832b6564e63e105cd34c’: No such file or directory
Oct 19 05:42:48 k8s-master kubelet[25301]: E1019 05:42:48.969250   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:48 k8s-master kubelet[25301]: E1019 05:42:48.876336   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:48 k8s-master kubelet[25301]: E1019 05:42:48.820242   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:48 k8s-master kubelet[25301]: E1019 05:42:48.688822   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:48 k8s-master kubelet[25301]: E1019 05:42:48.665178   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:48 k8s-master kubelet[25301]: E1019 05:42:48.643174   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:47 k8s-master kubelet[25301]: E1019 05:42:47.755140   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:47 k8s-master kubelet[25301]: E1019 05:42:47.418470   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:47 k8s-master kubelet[25301]: E1019 05:42:47.279082   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:46 k8s-master kubelet[25301]: E1019 05:42:46.958229   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:46 k8s-master kubelet[25301]: E1019 05:42:46.953610   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:46 k8s-master kubelet[25301]: E1019 05:42:46.708436   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:46 k8s-master kubelet[25301]: E1019 05:42:46.425917   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:46 k8s-master kubelet[25301]: E1019 05:42:46.285177   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:45 k8s-master kubelet[25301]: E1019 05:42:45.718653   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:45 k8s-master kubelet[25301]: E1019 05:42:45.581247   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:45 k8s-master kubelet[25301]: E1019 05:42:45.514363   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:45 k8s-master kubelet[25301]: E1019 05:42:45.384969   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:45 k8s-master kubelet[25301]: E1019 05:42:45.299522   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:45 k8s-master kubelet[25301]: E1019 05:42:45.253009   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:45 k8s-master kubelet[25301]: E1019 05:42:45.190015   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:44 k8s-master kubelet[25301]: E1019 05:42:44.698410   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:44 k8s-master kubelet[25301]: E1019 05:42:44.542242   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:44 k8s-master kubelet[25301]: E1019 05:42:44.406869   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:43 k8s-master kubelet[25301]: E1019 05:42:43.837941   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:43 k8s-master kubelet[25301]: E1019 05:42:43.801079   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:43 k8s-master kubelet[25301]: E1019 05:42:43.391863   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:43 k8s-master kubelet[25301]: E1019 05:42:43.339251   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:43 k8s-master kubelet[25301]: E1019 05:42:43.021552   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:43 k8s-master kubelet[25301]: E1019 05:42:43.018957   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:42 k8s-master kubelet[25301]: E1019 05:42:42.699828   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:42 k8s-master kubelet[25301]: E1019 05:42:42.579400   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:42 k8s-master kubelet[25301]: E1019 05:42:42.499137   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:42 k8s-master kubelet[25301]: E1019 05:42:42.008764   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:41 k8s-master kubelet[25301]: E1019 05:42:41.926194   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:41 k8s-master kubelet[25301]: E1019 05:42:41.379556   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:41 k8s-master kubelet[25301]: E1019 05:42:41.269629   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:41 k8s-master kubelet[25301]: E1019 05:42:41.075019   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:40 k8s-master kubelet[25301]: E1019 05:42:40.987234   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:40 k8s-master kubelet[25301]: E1019 05:42:40.898642   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:40 k8s-master kubelet[25301]: E1019 05:42:40.889175   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:40 k8s-master kubelet[25301]: E1019 05:42:40.186714   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:40 k8s-master kubelet[25301]: E1019 05:42:40.133417   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:39 k8s-master kubelet[25301]: E1019 05:42:39.924233   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:39 k8s-master kubelet[25301]: E1019 05:42:39.649483   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:39 k8s-master kubelet[25301]: E1019 05:42:39.408617   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:39 k8s-master kubelet[25301]: E1019 05:42:39.107982   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:39 k8s-master kubelet[25301]: E1019 05:42:39.075639   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:38 k8s-master kubelet[25301]: E1019 05:42:38.959996   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:38 k8s-master kubelet[25301]: E1019 05:42:38.864212   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:37 k8s-master kubelet[25301]: E1019 05:42:37.989220   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:37 k8s-master kubelet[25301]: E1019 05:42:37.983234   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:37 k8s-master kubelet[25301]: E1019 05:42:37.928918   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:37 k8s-master kubelet[25301]: E1019 05:42:37.883434   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:37 k8s-master kubelet[25301]: E1019 05:42:37.673931   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:37 k8s-master kubelet[25301]: E1019 05:42:37.605537   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:37 k8s-master kubelet[25301]: E1019 05:42:37.453056   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:37 k8s-master kubelet[25301]: E1019 05:42:37.311631   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:37 k8s-master kubelet[25301]: E1019 05:42:37.265860   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:37 k8s-master kubelet[25301]: E1019 05:42:37.256805   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:37 k8s-master kubelet[25301]: E1019 05:42:37.235716   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:37 k8s-master kubelet[25301]: E1019 05:42:37.055168   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:36 k8s-master kubelet[25301]: E1019 05:42:36.966532   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:36 k8s-master kubelet[25301]: E1019 05:42:36.960874   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:36 k8s-master kubelet[25301]: E1019 05:42:36.854300   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:36 k8s-master kubelet[25301]: E1019 05:42:36.617906   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:35 k8s-master kubelet[25301]: E1019 05:42:35.840727   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:35 k8s-master kubelet[25301]: E1019 05:42:35.716604   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:35 k8s-master kubelet[25301]: E1019 05:42:35.700175   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:35 k8s-master kubelet[25301]: E1019 05:42:35.506052   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:35 k8s-master kubelet[25301]: E1019 05:42:35.190204   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:34 k8s-master kubelet[25301]: E1019 05:42:34.782082   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:34 k8s-master kubelet[25301]: E1019 05:42:34.623641   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:34 k8s-master kubelet[25301]: E1019 05:42:34.525983   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:33 k8s-master kubelet[25301]: E1019 05:42:33.955767   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:33 k8s-master kubelet[25301]: E1019 05:42:33.719001   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:33 k8s-master kubelet[25301]: E1019 05:42:33.438004   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:33 k8s-master kubelet[25301]: E1019 05:42:33.307369   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:33 k8s-master kubelet[25301]: E1019 05:42:33.203013   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:32 k8s-master kubelet[25301]: E1019 05:42:32.961597   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:32 k8s-master kubelet[25301]: E1019 05:42:32.332168   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:32 k8s-master kubelet[25301]: E1019 05:42:32.267167   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:32 k8s-master kubelet[25301]: E1019 05:42:32.111789   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:32 k8s-master kubelet[25301]: E1019 05:42:32.111702   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:32 k8s-master kubelet[25301]: E1019 05:42:32.087979   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:32 k8s-master kubelet[25301]: W1019 05:42:32.087865   25301 container.go:354] Failed to create summary reader for "/libcontainer_24278_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:32 k8s-master kubelet[25301]: W1019 05:42:32.087763   25301 container.go:354] Failed to create summary reader for "/libcontainer_24267_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:32 k8s-master kubelet[25301]: W1019 05:42:32.087640   25301 container.go:354] Failed to create summary reader for "/libcontainer_24241_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:32 k8s-master kubelet[25301]: E1019 05:42:32.081846   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:32 k8s-master kubelet[25301]: E1019 05:42:32.010632   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:32 k8s-master kubelet[25301]: E1019 05:42:32.009645   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:31 k8s-master kubelet[25301]: W1019 05:42:31.917395   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24278_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24278_systemd_test_default.slice: no such file or directory
Oct 19 05:42:31 k8s-master kubelet[25301]: W1019 05:42:31.810008   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24267_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24267_systemd_test_default.slice: no such file or directory
Oct 19 05:42:31 k8s-master kubelet[25301]: E1019 05:42:31.594191   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:31 k8s-master kubelet[25301]: E1019 05:42:31.372272   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:31 k8s-master kubelet[25301]: E1019 05:42:31.165833   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:30 k8s-master kubelet[25301]: E1019 05:42:30.963082   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:30 k8s-master kubelet[25301]: E1019 05:42:30.772298   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:30 k8s-master kubelet[25301]: E1019 05:42:30.360889   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:30 k8s-master kubelet[25301]: E1019 05:42:30.194167   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:30 k8s-master kubelet[25301]: E1019 05:42:30.161569   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:29 k8s-master kubelet[25301]: E1019 05:42:29.962029   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:29 k8s-master kubelet[25301]: W1019 05:42:29.763213   25301 status_manager.go:431] Failed to get status for pod "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/etcd-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:29 k8s-master kubelet[25301]: W1019 05:42:29.559063   25301 status_manager.go:431] Failed to get status for pod "kube-flannel-ds-n747n_kube-system(b6af75e4-b41a-11e7-8c24-08002782873b)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-flannel-ds-n747n: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:29 k8s-master kubelet[25301]: W1019 05:42:29.363132   25301 status_manager.go:431] Failed to get status for pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-dns-545bc4bfd4-l2xsw: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:29 k8s-master kubelet[25301]: W1019 05:42:29.208854   25301 status_manager.go:431] Failed to get status for pod "kube-proxy-rrmr6_kube-system(b2564e2e-b419-11e7-8c24-08002782873b)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-proxy-rrmr6: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:29 k8s-master kubelet[25301]: W1019 05:42:29.208693   25301 status_manager.go:431] Failed to get status for pod "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:29 k8s-master kubelet[25301]: W1019 05:42:29.208211   25301 status_manager.go:431] Failed to get status for pod "kube-controller-manager-k8s-master_kube-system(79f7c686bb6d415a1d71ac57fc06f7aa)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:29 k8s-master kubelet[25301]: W1019 05:42:29.207916   25301 status_manager.go:431] Failed to get status for pod "kube-apiserver-k8s-master_kube-system(d857b5bcdfd340d1ff3546b68f7f27c0)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:29 k8s-master kubelet[25301]: E1019 05:42:29.177927   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:29 k8s-master kubelet[25301]: E1019 05:42:29.069140   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:28 k8s-master kubelet[25301]: E1019 05:42:28.996092   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:28 k8s-master kubelet[25301]: E1019 05:42:28.800836   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:28 k8s-master kubelet[25301]: E1019 05:42:28.672593   25301 kubelet_node_status.go:378] Unable to update node status: update node status exceeds retry count
Oct 19 05:42:28 k8s-master kubelet[25301]: E1019 05:42:28.672585   25301 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:28 k8s-master kubelet[25301]: E1019 05:42:28.672446   25301 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:28 k8s-master kubelet[25301]: E1019 05:42:28.672323   25301 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:28 k8s-master kubelet[25301]: E1019 05:42:28.672182   25301 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:28 k8s-master kubelet[25301]: E1019 05:42:28.671918   25301 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:28 k8s-master kubelet[25301]: E1019 05:42:28.505689   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:28 k8s-master kubelet[25301]: E1019 05:42:28.453871   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:28 k8s-master kubelet[25301]: E1019 05:42:28.173148   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:27 k8s-master kubelet[25301]: E1019 05:42:27.992757   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:27 k8s-master kubelet[25301]: E1019 05:42:27.913410   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:27 k8s-master kubelet[25301]: E1019 05:42:27.790992   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:27 k8s-master kubelet[25301]: W1019 05:42:27.566026   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24241_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24241_systemd_test_default.slice: no such file or directory
Oct 19 05:42:27 k8s-master kubelet[25301]: E1019 05:42:27.328247   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:27 k8s-master kubelet[25301]: E1019 05:42:27.162996   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:27 k8s-master kubelet[25301]: E1019 05:42:27.129636   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:26 k8s-master kubelet[25301]: E1019 05:42:26.981197   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:26 k8s-master kubelet[25301]: E1019 05:42:26.768982   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:26 k8s-master kubelet[25301]: E1019 05:42:26.669992   25301 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 19 05:42:26 k8s-master kubelet[25301]: I1019 05:42:26.484293   25301 kuberuntime_manager.go:738] checking backoff for container "etcd" in pod "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)"
Oct 19 05:42:26 k8s-master kubelet[25301]: I1019 05:42:26.484165   25301 kuberuntime_manager.go:499] Container {Name:etcd Image:gcr.io/google_containers/etcd-amd64:3.1.10 Command:[etcd --listen-client-urls=http://127.0.0.1:2379 --advertise-client-urls=http://127.0.0.1:2379 --data-dir=/var/lib/etcd] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:etcd ReadOnly:false MountPath:/var/lib/etcd SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/health,Port:2379,Host:127.0.0.1,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:15,TimeoutSeconds:15,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:8,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 19 05:42:26 k8s-master kubelet[25301]: I1019 05:42:26.378797   25301 kuberuntime_manager.go:738] checking backoff for container "kube-flannel" in pod "kube-flannel-ds-n747n_kube-system(b6af75e4-b41a-11e7-8c24-08002782873b)"
Oct 19 05:42:26 k8s-master kubelet[25301]: I1019 05:42:26.378675   25301 kuberuntime_manager.go:499] Container {Name:kube-flannel Image:quay.io/coreos/flannel:v0.9.0-amd64 Command:[/opt/bin/flanneld --ip-masq --kube-subnet-mgr] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[{Name:POD_NAME Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {Name:POD_NAMESPACE Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}}] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:run ReadOnly:false MountPath:/run SubPath: MountPropagation:<nil>} {Name:flannel-cfg ReadOnly:false MountPath:/etc/kube-flannel/ SubPath: MountPropagation:<nil>} {Name:flannel-token-2sfpp ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 19 05:42:26 k8s-master kubelet[25301]: I1019 05:42:26.262873   25301 kuberuntime_manager.go:738] checking backoff for container "kube-controller-manager" in pod "kube-controller-manager-k8s-master_kube-system(79f7c686bb6d415a1d71ac57fc06f7aa)"
Oct 19 05:42:26 k8s-master kubelet[25301]: I1019 05:42:26.262723   25301 kuberuntime_manager.go:499] Container {Name:kube-controller-manager Image:gcr.io/google_containers/kube-controller-manager-amd64:v1.8.1-beta.0 Command:[kube-controller-manager --address=127.0.0.1 --leader-elect=true --root-ca-file=/etc/kubernetes/pki/ca.crt --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --use-service-account-credentials=true --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --service-account-private-key-file=/etc/kubernetes/pki/sa.key --allocate-node-cidrs=true --cluster-cidr=10.244.0.0/16 --node-cidr-mask-size=24] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:200 scale:-3} d:{Dec:<nil>} s:200m Format:DecimalSI}]} VolumeMounts:[{Name:k8s-certs ReadOnly:true MountPath:/etc/kubernetes/pki SubPath: MountPropagation:<nil>} {Name:ca-certs ReadOnly:true MountPath:/etc/ssl/certs SubPath: MountPropagation:<nil>} {Name:kubeconfig ReadOnly:true MountPath:/etc/kubernetes/controller-manager.conf SubPath: MountPropagation:<nil>} {Name:flexvolume-dir ReadOnly:false MountPath:/usr/libexec/kubernetes/kubelet-plugins/volume/exec SubPath: MountPropagation:<nil>} {Name:ca-certs-etc-pki ReadOnly:true MountPath:/etc/pki SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:10252,Host:127.0.0.1,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:15,TimeoutSeconds:15,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:8,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 19 05:42:26 k8s-master kubelet[25301]: W1019 05:42:26.077539   25301 status_manager.go:431] Failed to get status for pod "kube-flannel-ds-n747n_kube-system(b6af75e4-b41a-11e7-8c24-08002782873b)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-flannel-ds-n747n: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:26 k8s-master kubelet[25301]: W1019 05:42:26.060428   25301 status_manager.go:431] Failed to get status for pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-dns-545bc4bfd4-l2xsw: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:26 k8s-master kubelet[25301]: E1019 05:42:26.049844   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:25 k8s-master kubelet[25301]: W1019 05:42:25.988309   25301 status_manager.go:431] Failed to get status for pod "kube-proxy-rrmr6_kube-system(b2564e2e-b419-11e7-8c24-08002782873b)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-proxy-rrmr6: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:25 k8s-master kubelet[25301]: E1019 05:42:25.949623   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:25 k8s-master kubelet[25301]: E1019 05:42:25.594131   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:25 k8s-master kubelet[25301]: E1019 05:42:25.561243   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:25 k8s-master kubelet[25301]: E1019 05:42:25.382082   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:25 k8s-master kubelet[25301]: E1019 05:42:25.284892   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:25 k8s-master kubelet[25301]: E1019 05:42:25.215888   25301 pod_workers.go:182] Error syncing pod 79f7c686bb6d415a1d71ac57fc06f7aa ("kube-controller-manager-k8s-master_kube-system(79f7c686bb6d415a1d71ac57fc06f7aa)"), skipping: failed to "StartContainer" for "kube-controller-manager" with CrashLoopBackOff: "Back-off 10s restarting failed container=kube-controller-manager pod=kube-controller-manager-k8s-master_kube-system(79f7c686bb6d415a1d71ac57fc06f7aa)"
Oct 19 05:42:25 k8s-master kubelet[25301]: I1019 05:42:25.215701   25301 kuberuntime_manager.go:748] Back-off 10s restarting failed container=kube-controller-manager pod=kube-controller-manager-k8s-master_kube-system(79f7c686bb6d415a1d71ac57fc06f7aa)
Oct 19 05:42:25 k8s-master kubelet[25301]: I1019 05:42:25.214786   25301 kuberuntime_manager.go:738] checking backoff for container "kube-controller-manager" in pod "kube-controller-manager-k8s-master_kube-system(79f7c686bb6d415a1d71ac57fc06f7aa)"
Oct 19 05:42:25 k8s-master kubelet[25301]: I1019 05:42:25.214223   25301 kuberuntime_manager.go:499] Container {Name:kube-controller-manager Image:gcr.io/google_containers/kube-controller-manager-amd64:v1.8.1-beta.0 Command:[kube-controller-manager --address=127.0.0.1 --leader-elect=true --root-ca-file=/etc/kubernetes/pki/ca.crt --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt --cluster-signing-key-file=/etc/kubernetes/pki/ca.key --use-service-account-credentials=true --controllers=*,bootstrapsigner,tokencleaner --kubeconfig=/etc/kubernetes/controller-manager.conf --service-account-private-key-file=/etc/kubernetes/pki/sa.key --allocate-node-cidrs=true --cluster-cidr=10.244.0.0/16 --node-cidr-mask-size=24] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:200 scale:-3} d:{Dec:<nil>} s:200m Format:DecimalSI}]} VolumeMounts:[{Name:k8s-certs ReadOnly:true MountPath:/etc/kubernetes/pki SubPath: MountPropagation:<nil>} {Name:ca-certs ReadOnly:true MountPath:/etc/ssl/certs SubPath: MountPropagation:<nil>} {Name:kubeconfig ReadOnly:true MountPath:/etc/kubernetes/controller-manager.conf SubPath: MountPropagation:<nil>} {Name:flexvolume-dir ReadOnly:false MountPath:/usr/libexec/kubernetes/kubelet-plugins/volume/exec SubPath: MountPropagation:<nil>} {Name:ca-certs-etc-pki ReadOnly:true MountPath:/etc/pki SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthz,Port:10252,Host:127.0.0.1,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:15,TimeoutSeconds:15,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:8,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 19 05:42:25 k8s-master kubelet[25301]: E1019 05:42:25.185025   25301 pod_workers.go:182] Error syncing pod 40eb0889c614345e2a2714d4ee7d1cc0 ("etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)"), skipping: failed to "StartContainer" for "etcd" with CrashLoopBackOff: "Back-off 10s restarting failed container=etcd pod=etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)"
Oct 19 05:42:25 k8s-master kubelet[25301]: I1019 05:42:25.184982   25301 kuberuntime_manager.go:748] Back-off 10s restarting failed container=etcd pod=etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)
Oct 19 05:42:25 k8s-master kubelet[25301]: I1019 05:42:25.184844   25301 kuberuntime_manager.go:738] checking backoff for container "etcd" in pod "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)"
Oct 19 05:42:25 k8s-master kubelet[25301]: I1019 05:42:25.184688   25301 kuberuntime_manager.go:499] Container {Name:etcd Image:gcr.io/google_containers/etcd-amd64:3.1.10 Command:[etcd --listen-client-urls=http://127.0.0.1:2379 --advertise-client-urls=http://127.0.0.1:2379 --data-dir=/var/lib/etcd] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:etcd ReadOnly:false MountPath:/var/lib/etcd SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/health,Port:2379,Host:127.0.0.1,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:15,TimeoutSeconds:15,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:8,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 19 05:42:25 k8s-master kubelet[25301]: E1019 05:42:25.026201   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:24 k8s-master kubelet[25301]: E1019 05:42:24.972002   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:24 k8s-master kubelet[25301]: W1019 05:42:24.907118   25301 status_manager.go:431] Failed to get status for pod "kube-controller-manager-k8s-master_kube-system(79f7c686bb6d415a1d71ac57fc06f7aa)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:24 k8s-master kubelet[25301]: W1019 05:42:24.906495   25301 pod_container_deletor.go:77] Container "8b79a916d4941180864a53969248c9432a503985dec66f1966aeedb01b368539" not found in pod's containers
Oct 19 05:42:24 k8s-master kubelet[25301]: W1019 05:42:24.884219   25301 status_manager.go:431] Failed to get status for pod "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/etcd-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:24 k8s-master kubelet[25301]: W1019 05:42:24.861336   25301 status_manager.go:431] Failed to get status for pod "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:24 k8s-master kubelet[25301]: W1019 05:42:24.834051   25301 status_manager.go:431] Failed to get status for pod "kube-apiserver-k8s-master_kube-system(d857b5bcdfd340d1ff3546b68f7f27c0)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:24 k8s-master kubelet[25301]: W1019 05:42:24.833396   25301 pod_container_deletor.go:77] Container "0824dfbba72822465c054c6befea85d9b1c4ddb6644f8a7765525ab8153a852e" not found in pod's containers
Oct 19 05:42:24 k8s-master kubelet[25301]: E1019 05:42:24.804063   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:24 k8s-master kubelet[25301]: E1019 05:42:24.803994   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:24 k8s-master kubelet[25301]: E1019 05:42:24.803941   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:24 k8s-master kubelet[25301]: E1019 05:42:24.803844   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:24 k8s-master kubelet[25301]: E1019 05:42:24.767932   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:24 k8s-master kubelet[25301]: W1019 05:42:24.767803   25301 container.go:354] Failed to create summary reader for "/libcontainer_24154_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:24 k8s-master kubelet[25301]: W1019 05:42:24.767631   25301 container.go:354] Failed to create summary reader for "/libcontainer_24150_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:24 k8s-master kubelet[25301]: W1019 05:42:24.767433   25301 container.go:354] Failed to create summary reader for "/libcontainer_24145_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:24 k8s-master kubelet[25301]: W1019 05:42:24.767252   25301 container.go:354] Failed to create summary reader for "/libcontainer_24134_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:24 k8s-master kubelet[25301]: W1019 05:42:24.767105   25301 container.go:354] Failed to create summary reader for "/libcontainer_24124_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:24 k8s-master kubelet[25301]: E1019 05:42:24.738253   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:24 k8s-master kubelet[25301]: E1019 05:42:24.736891   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:24 k8s-master kubelet[25301]: E1019 05:42:24.736891   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:24 k8s-master kubelet[25301]: W1019 05:42:24.728651   25301 container.go:354] Failed to create summary reader for "/libcontainer_24103_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:24 k8s-master kubelet[25301]: W1019 05:42:24.728517   25301 container.go:354] Failed to create summary reader for "/libcontainer_24088_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:24 k8s-master kubelet[25301]: E1019 05:42:24.694914   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:24 k8s-master kubelet[25301]: E1019 05:42:24.694814   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:24 k8s-master kubelet[25301]: W1019 05:42:24.665435   25301 container.go:354] Failed to create summary reader for "/libcontainer_24073_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:24 k8s-master kubelet[25301]: W1019 05:42:24.665283   25301 container.go:354] Failed to create summary reader for "/libcontainer_24059_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:24 k8s-master kubelet[25301]: E1019 05:42:24.636335   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:24 k8s-master kubelet[25301]: E1019 05:42:24.636275   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:24 k8s-master kubelet[25301]: W1019 05:42:24.635575   25301 container.go:354] Failed to create summary reader for "/libcontainer_24039_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:24 k8s-master kubelet[25301]: W1019 05:42:24.635399   25301 container.go:354] Failed to create summary reader for "/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-podb6af75e4_b41a_11e7_8c24_08002782873b.slice/docker-2003d06ac2757b7427d7ffbc81591ceabb453442abc431d354ab0f0ec6300b51.scope": none of the resources are being tracked.
Oct 19 05:42:24 k8s-master kubelet[25301]: E1019 05:42:24.591837   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:24 k8s-master kubelet[25301]: E1019 05:42:24.475914   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:24 k8s-master kubelet[25301]: E1019 05:42:24.403075   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:24 k8s-master kubelet[25301]: E1019 05:42:24.381280   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:24 k8s-master kubelet[25301]: W1019 05:42:24.181929   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24124_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24124_systemd_test_default.slice: no such file or directory
Oct 19 05:42:24 k8s-master kubelet[25301]: E1019 05:42:24.025505   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:23 k8s-master kubelet[25301]: W1019 05:42:23.984124   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24103_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24103_systemd_test_default.slice: no such file or directory
Oct 19 05:42:23 k8s-master kubelet[25301]: E1019 05:42:23.934158   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:23 k8s-master kubelet[25301]: W1019 05:42:23.807261   25301 kuberuntime_container.go:191] Non-root verification doesn't support non-numeric user (nobody)
Oct 19 05:42:23 k8s-master kubelet[25301]: I1019 05:42:23.792088   25301 kuberuntime_manager.go:738] checking backoff for container "sidecar" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 19 05:42:23 k8s-master kubelet[25301]: E1019 05:42:23.709514   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:23 k8s-master kubelet[25301]: E1019 05:42:23.622402   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:23 k8s-master kubelet[25301]: E1019 05:42:23.589035   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:23 k8s-master kubelet[25301]: E1019 05:42:23.574544   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:23 k8s-master kubelet[25301]: E1019 05:42:23.371801   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:23 k8s-master kubelet[25301]: W1019 05:42:23.349833   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24073_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24073_systemd_test_default.slice: no such file or directory
Oct 19 05:42:23 k8s-master kubelet[25301]: E1019 05:42:23.148927   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:23 k8s-master kubelet[25301]: I1019 05:42:23.108512   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 19 05:42:23 k8s-master kubelet[25301]: W1019 05:42:23.023913   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24059_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24059_systemd_test_default.slice: no such file or directory
Oct 19 05:42:22 k8s-master kubelet[25301]: E1019 05:42:22.984566   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:22 k8s-master kubelet[25301]: E1019 05:42:22.939969   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:22 k8s-master kubelet[25301]: E1019 05:42:22.737459   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:22 k8s-master kubelet[25301]: E1019 05:42:22.582793   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:22 k8s-master kubelet[25301]: E1019 05:42:22.411909   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:22 k8s-master kubelet[25301]: W1019 05:42:22.411634   25301 pod_container_deletor.go:77] Container "f55b5fc2afb86d039fc584a02be1c48a28bde80803f575c8dfb92a2de59ea16d" not found in pod's containers
Oct 19 05:42:22 k8s-master kubelet[25301]: E1019 05:42:22.367341   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:22 k8s-master kubelet[25301]: E1019 05:42:22.336132   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:22 k8s-master kubelet[25301]: I1019 05:42:22.160626   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 19 05:42:22 k8s-master kubelet[25301]: E1019 05:42:22.160339   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:22 k8s-master kubelet[25301]: E1019 05:42:22.158851   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:22 k8s-master kubelet[25301]: W1019 05:42:22.134905   25301 container.go:354] Failed to create summary reader for "/libcontainer_23902_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:22 k8s-master kubelet[25301]: E1019 05:42:22.112096   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:21 k8s-master kubelet[25301]: E1019 05:42:21.981959   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:21 k8s-master kubelet[25301]: E1019 05:42:21.946516   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:21 k8s-master kubelet[25301]: E1019 05:42:21.849135   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:21 k8s-master kubelet[25301]: I1019 05:42:21.664558   25301 kuberuntime_manager.go:738] checking backoff for container "install-cni" in pod "kube-flannel-ds-n747n_kube-system(b6af75e4-b41a-11e7-8c24-08002782873b)"
Oct 19 05:42:21 k8s-master kubelet[25301]: E1019 05:42:21.590761   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:21 k8s-master kubelet[25301]: E1019 05:42:21.573150   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:21 k8s-master kubelet[25301]: E1019 05:42:21.558352   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:21 k8s-master kubelet[25301]: E1019 05:42:21.362132   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:21 k8s-master kubelet[25301]: E1019 05:42:21.050058   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:20 k8s-master kubelet[25301]: E1019 05:42:20.990696   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:20 k8s-master kubelet[25301]: E1019 05:42:20.971977   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:20 k8s-master kubelet[25301]: I1019 05:42:20.949804   25301 kuberuntime_manager.go:738] checking backoff for container "kube-scheduler" in pod "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)"
Oct 19 05:42:20 k8s-master kubelet[25301]: W1019 05:42:20.762830   25301 status_manager.go:431] Failed to get status for pod "kube-proxy-rrmr6_kube-system(b2564e2e-b419-11e7-8c24-08002782873b)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-proxy-rrmr6: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:20 k8s-master kubelet[25301]: E1019 05:42:20.762440   25301 pod_workers.go:182] Error syncing pod 40eb0889c614345e2a2714d4ee7d1cc0 ("etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)"), skipping: failed to "StartContainer" for "etcd" with CrashLoopBackOff: "Back-off 10s restarting failed container=etcd pod=etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)"
Oct 19 05:42:20 k8s-master kubelet[25301]: I1019 05:42:20.762299   25301 kuberuntime_manager.go:748] Back-off 10s restarting failed container=etcd pod=etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)
Oct 19 05:42:20 k8s-master kubelet[25301]: I1019 05:42:20.761907   25301 kuberuntime_manager.go:738] checking backoff for container "etcd" in pod "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)"
Oct 19 05:42:20 k8s-master kubelet[25301]: E1019 05:42:20.755452   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:20 k8s-master kubelet[25301]: E1019 05:42:20.739381   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:20 k8s-master kubelet[25301]: I1019 05:42:20.692346   25301 kuberuntime_manager.go:738] checking backoff for container "kube-apiserver" in pod "kube-apiserver-k8s-master_kube-system(d857b5bcdfd340d1ff3546b68f7f27c0)"
Oct 19 05:42:20 k8s-master kubelet[25301]: E1019 05:42:20.676780   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:20 k8s-master kubelet[25301]: E1019 05:42:20.676691   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:20 k8s-master kubelet[25301]: E1019 05:42:20.567428   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:20 k8s-master kubelet[25301]: E1019 05:42:20.476365   25301 pod_workers.go:182] Error syncing pod 79f7c686bb6d415a1d71ac57fc06f7aa ("kube-controller-manager-k8s-master_kube-system(79f7c686bb6d415a1d71ac57fc06f7aa)"), skipping: failed to "StartContainer" for "kube-controller-manager" with CrashLoopBackOff: "Back-off 10s restarting failed container=kube-controller-manager pod=kube-controller-manager-k8s-master_kube-system(79f7c686bb6d415a1d71ac57fc06f7aa)"
Oct 19 05:42:20 k8s-master kubelet[25301]: I1019 05:42:20.476314   25301 kuberuntime_manager.go:748] Back-off 10s restarting failed container=kube-controller-manager pod=kube-controller-manager-k8s-master_kube-system(79f7c686bb6d415a1d71ac57fc06f7aa)
Oct 19 05:42:20 k8s-master kubelet[25301]: I1019 05:42:20.476135   25301 kuberuntime_manager.go:738] checking backoff for container "kube-controller-manager" in pod "kube-controller-manager-k8s-master_kube-system(79f7c686bb6d415a1d71ac57fc06f7aa)"
Oct 19 05:42:20 k8s-master kubelet[25301]: E1019 05:42:20.358080   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:20 k8s-master kubelet[25301]: W1019 05:42:20.257107   25301 pod_container_deletor.go:77] Container "5ef75e7d64880799e0c240e299336fad9e4d06071f89ac6dbd636b396db805c6" not found in pod's containers
Oct 19 05:42:20 k8s-master kubelet[25301]: I1019 05:42:20.225053   25301 kuberuntime_manager.go:738] checking backoff for container "kube-proxy" in pod "kube-proxy-rrmr6_kube-system(b2564e2e-b419-11e7-8c24-08002782873b)"
Oct 19 05:42:20 k8s-master kubelet[25301]: W1019 05:42:20.219664   25301 status_manager.go:431] Failed to get status for pod "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:20 k8s-master kubelet[25301]: E1019 05:42:20.085935   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:20 k8s-master kubelet[25301]: E1019 05:42:20.008561   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:19 k8s-master kubelet[25301]: E1019 05:42:19.958860   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:19 k8s-master kubelet[25301]: E1019 05:42:19.858439   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:19 k8s-master kubelet[25301]: W1019 05:42:19.766329   25301 status_manager.go:431] Failed to get status for pod "kube-controller-manager-k8s-master_kube-system(79f7c686bb6d415a1d71ac57fc06f7aa)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:19 k8s-master kubelet[25301]: E1019 05:42:19.721279   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:19 k8s-master kubelet[25301]: W1019 05:42:19.580870   25301 status_manager.go:431] Failed to get status for pod "kube-apiserver-k8s-master_kube-system(d857b5bcdfd340d1ff3546b68f7f27c0)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:19 k8s-master kubelet[25301]: E1019 05:42:19.580566   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:19 k8s-master kubelet[25301]: W1019 05:42:19.357276   25301 status_manager.go:431] Failed to get status for pod "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/etcd-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:19 k8s-master kubelet[25301]: W1019 05:42:19.202872   25301 status_manager.go:431] Failed to get status for pod "kube-flannel-ds-n747n_kube-system(b6af75e4-b41a-11e7-8c24-08002782873b)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-flannel-ds-n747n: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:19 k8s-master kubelet[25301]: W1019 05:42:19.202359   25301 status_manager.go:431] Failed to get status for pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-dns-545bc4bfd4-l2xsw: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:18 k8s-master kubelet[25301]: E1019 05:42:18.984652   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:18 k8s-master kubelet[25301]: E1019 05:42:18.795475   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:18 k8s-master kubelet[25301]: E1019 05:42:18.664020   25301 kubelet_node_status.go:378] Unable to update node status: update node status exceeds retry count
Oct 19 05:42:18 k8s-master kubelet[25301]: E1019 05:42:18.664010   25301 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:18 k8s-master kubelet[25301]: E1019 05:42:18.663825   25301 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:18 k8s-master kubelet[25301]: E1019 05:42:18.663366   25301 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:18 k8s-master kubelet[25301]: E1019 05:42:18.662431   25301 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:18 k8s-master kubelet[25301]: E1019 05:42:18.656981   25301 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:18 k8s-master kubelet[25301]: E1019 05:42:18.649881   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:18 k8s-master kubelet[25301]: E1019 05:42:18.634396   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:18 k8s-master kubelet[25301]: E1019 05:42:18.502990   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:18 k8s-master kubelet[25301]: W1019 05:42:18.329338   25301 status_manager.go:431] Failed to get status for pod "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/etcd-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:18 k8s-master kubelet[25301]: W1019 05:42:18.327166   25301 pod_container_deletor.go:77] Container "7a23c426cfe17e8905996fa3422d545de550a7d9721abd90ef2a73eb84192569" not found in pod's containers
Oct 19 05:42:18 k8s-master kubelet[25301]: E1019 05:42:18.303633   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:18 k8s-master kubelet[25301]: W1019 05:42:18.261064   25301 status_manager.go:431] Failed to get status for pod "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:18 k8s-master kubelet[25301]: W1019 05:42:18.260361   25301 pod_container_deletor.go:77] Container "d3a074dda4d57ef9ef26c6864debc7b88b7a821bcde403287f41ff9f452229f5" not found in pod's containers
Oct 19 05:42:18 k8s-master kubelet[25301]: E1019 05:42:18.241713   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:18 k8s-master kubelet[25301]: W1019 05:42:18.234917   25301 pod_container_deletor.go:77] Container "bfffdedfc9c2566eb58bcfbd52daed7bb9185a2eb35712c09568e9c780d01a63" not found in pod's containers
Oct 19 05:42:18 k8s-master kubelet[25301]: W1019 05:42:18.220643   25301 status_manager.go:431] Failed to get status for pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-dns-545bc4bfd4-l2xsw: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:18 k8s-master kubelet[25301]: W1019 05:42:18.219939   25301 pod_container_deletor.go:77] Container "388c6bedb853f1bec6d348c03da674fc4182aafbba972869473e16b0f524f772" not found in pod's containers
Oct 19 05:42:18 k8s-master kubelet[25301]: W1019 05:42:18.135228   25301 status_manager.go:431] Failed to get status for pod "kube-controller-manager-k8s-master_kube-system(79f7c686bb6d415a1d71ac57fc06f7aa)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:18 k8s-master kubelet[25301]: W1019 05:42:18.134591   25301 pod_container_deletor.go:77] Container "e3e7eaf2f6ae026074b50ec52de9ca4fe2918d33f90c46e1bfe03780afe6f34f" not found in pod's containers
Oct 19 05:42:17 k8s-master kubelet[25301]: E1019 05:42:17.976107   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:17 k8s-master kubelet[25301]: E1019 05:42:17.793169   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:17 k8s-master kubelet[25301]: E1019 05:42:17.672883   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:17 k8s-master kubelet[25301]: E1019 05:42:17.575627   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:17 k8s-master kubelet[25301]: E1019 05:42:17.477785   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:17 k8s-master kubelet[25301]: E1019 05:42:17.368839   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:16 k8s-master kubelet[25301]: E1019 05:42:16.969939   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:16 k8s-master kubelet[25301]: E1019 05:42:16.902136   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:16 k8s-master kubelet[25301]: E1019 05:42:16.858124   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:16 k8s-master kubelet[25301]: E1019 05:42:16.792174   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:16 k8s-master kubelet[25301]: E1019 05:42:16.742848   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:16 k8s-master kubelet[25301]: E1019 05:42:16.725887   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:16 k8s-master kubelet[25301]: E1019 05:42:16.664725   25301 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 19 05:42:16 k8s-master kubelet[25301]: E1019 05:42:16.601593   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:16 k8s-master kubelet[25301]: E1019 05:42:16.568613   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:16 k8s-master kubelet[25301]: E1019 05:42:16.450942   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:16 k8s-master kubelet[25301]: E1019 05:42:16.300679   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:16 k8s-master kubelet[25301]: W1019 05:42:16.300137   25301 container.go:354] Failed to create summary reader for "/libcontainer_23522_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:16 k8s-master kubelet[25301]: W1019 05:42:16.291824   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23522_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23522_systemd_test_default.slice: no such file or directory
Oct 19 05:42:16 k8s-master kubelet[25301]: E1019 05:42:16.212342   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:16 k8s-master kubelet[25301]: E1019 05:42:16.079129   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:15 k8s-master kubelet[25301]: E1019 05:42:15.966675   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:15 k8s-master kubelet[25301]: E1019 05:42:15.864549   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:15 k8s-master kubelet[25301]: E1019 05:42:15.790561   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:15 k8s-master kubelet[25301]: E1019 05:42:15.776361   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:15 k8s-master kubelet[25301]: E1019 05:42:15.659250   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:15 k8s-master kubelet[25301]: E1019 05:42:15.565433   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:14 k8s-master kubelet[25301]: E1019 05:42:14.965835   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:14 k8s-master kubelet[25301]: E1019 05:42:14.776673   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:14 k8s-master kubelet[25301]: E1019 05:42:14.748141   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:14 k8s-master kubelet[25301]: W1019 05:42:14.748010   25301 container.go:354] Failed to create summary reader for "/libcontainer_23429_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:14 k8s-master kubelet[25301]: W1019 05:42:14.735310   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23429_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23429_systemd_test_default.slice: no such file or directory
Oct 19 05:42:14 k8s-master kubelet[25301]: E1019 05:42:14.561790   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:14 k8s-master kubelet[25301]: E1019 05:42:14.480383   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:14 k8s-master kubelet[25301]: E1019 05:42:14.376320   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:13 k8s-master kubelet[25301]: E1019 05:42:13.964601   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:13 k8s-master kubelet[25301]: E1019 05:42:13.876586   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:13 k8s-master kubelet[25301]: E1019 05:42:13.762373   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:13 k8s-master kubelet[25301]: E1019 05:42:13.560932   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:13 k8s-master kubelet[25301]: E1019 05:42:13.382749   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:13 k8s-master kubelet[25301]: E1019 05:42:13.344088   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:13 k8s-master kubelet[25301]: E1019 05:42:13.129037   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:12 k8s-master kubelet[25301]: E1019 05:42:12.963799   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:12 k8s-master kubelet[25301]: E1019 05:42:12.861484   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:12 k8s-master kubelet[25301]: E1019 05:42:12.760935   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:12 k8s-master kubelet[25301]: E1019 05:42:12.604919   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:12 k8s-master kubelet[25301]: E1019 05:42:12.557505   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:12 k8s-master kubelet[25301]: E1019 05:42:12.462951   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:12 k8s-master kubelet[25301]: W1019 05:42:12.363413   25301 status_manager.go:431] Failed to get status for pod "kube-controller-manager-k8s-master_kube-system(79f7c686bb6d415a1d71ac57fc06f7aa)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:12 k8s-master kubelet[25301]: W1019 05:42:12.157350   25301 status_manager.go:431] Failed to get status for pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-dns-545bc4bfd4-l2xsw: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:11 k8s-master kubelet[25301]: E1019 05:42:11.958231   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:11 k8s-master kubelet[25301]: E1019 05:42:11.834571   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:11 k8s-master kubelet[25301]: E1019 05:42:11.758307   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:11 k8s-master kubelet[25301]: W1019 05:42:11.558087   25301 status_manager.go:431] Failed to get status for pod "kube-flannel-ds-n747n_kube-system(b6af75e4-b41a-11e7-8c24-08002782873b)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-flannel-ds-n747n: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:11 k8s-master kubelet[25301]: E1019 05:42:11.357599   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:11 k8s-master kubelet[25301]: W1019 05:42:11.158863   25301 status_manager.go:431] Failed to get status for pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-dns-545bc4bfd4-l2xsw: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:10 k8s-master kubelet[25301]: E1019 05:42:10.797834   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "CreatePodSandbox" for "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)" with CreatePodSandboxError: "CreatePodSandbox for pod \"kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)\" failed: rpc error: code = Unknown desc = failed to start sandbox container for pod \"kube-dns-545bc4bfd4-l2xsw\": Error response from daemon: failed to update store for object type *libnetwork.endpoint: open : no such file or directory"
Oct 19 05:42:10 k8s-master kubelet[25301]: E1019 05:42:10.797776   25301 kuberuntime_manager.go:632] createPodSandbox for pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)" failed: rpc error: code = Unknown desc = failed to start sandbox container for pod "kube-dns-545bc4bfd4-l2xsw": Error response from daemon: failed to update store for object type *libnetwork.endpoint: open : no such file or directory
Oct 19 05:42:10 k8s-master kubelet[25301]: E1019 05:42:10.797757   25301 kuberuntime_sandbox.go:54] CreatePodSandbox for pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)" failed: rpc error: code = Unknown desc = failed to start sandbox container for pod "kube-dns-545bc4bfd4-l2xsw": Error response from daemon: failed to update store for object type *libnetwork.endpoint: open : no such file or directory
Oct 19 05:42:10 k8s-master kubelet[25301]: E1019 05:42:10.797699   25301 remote_runtime.go:92] RunPodSandbox from runtime service failed: rpc error: code = Unknown desc = failed to start sandbox container for pod "kube-dns-545bc4bfd4-l2xsw": Error response from daemon: failed to update store for object type *libnetwork.endpoint: open : no such file or directory
Oct 19 05:42:10 k8s-master kubelet[25301]: W1019 05:42:10.565132   25301 status_manager.go:431] Failed to get status for pod "kube-proxy-rrmr6_kube-system(b2564e2e-b419-11e7-8c24-08002782873b)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-proxy-rrmr6: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:10 k8s-master kubelet[25301]: E1019 05:42:10.516471   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:10 k8s-master kubelet[25301]: E1019 05:42:10.358084   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:10 k8s-master kubelet[25301]: E1019 05:42:10.163085   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:09 k8s-master kubelet[25301]: W1019 05:42:09.959808   25301 status_manager.go:431] Failed to get status for pod "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:09 k8s-master kubelet[25301]: E1019 05:42:09.841183   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "KillContainer" for "kubedns" with KillContainerError: "rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"
Oct 19 05:42:09 k8s-master kubelet[25301]: E1019 05:42:09.841137   25301 kuberuntime_manager.go:580] killPodWithSyncResult failed: failed to "KillContainer" for "kubedns" with KillContainerError: "rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"
Oct 19 05:42:09 k8s-master kubelet[25301]: E1019 05:42:09.759645   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:09 k8s-master kubelet[25301]: E1019 05:42:09.675494   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:09 k8s-master kubelet[25301]: W1019 05:42:09.674639   25301 container.go:354] Failed to create summary reader for "/libcontainer_23380_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:09 k8s-master kubelet[25301]: W1019 05:42:09.559679   25301 status_manager.go:431] Failed to get status for pod "kube-controller-manager-k8s-master_kube-system(79f7c686bb6d415a1d71ac57fc06f7aa)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:09 k8s-master kubelet[25301]: W1019 05:42:09.357359   25301 status_manager.go:431] Failed to get status for pod "kube-apiserver-k8s-master_kube-system(d857b5bcdfd340d1ff3546b68f7f27c0)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:09 k8s-master kubelet[25301]: E1019 05:42:09.209082   25301 kubelet.go:1915] Failed cleaning pods: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:09 k8s-master kubelet[25301]: E1019 05:42:09.209064   25301 kubelet_pods.go:964] Error listing containers: &status.statusError{Code:2, Message:"Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?", Details:[]*any.Any(nil)}
Oct 19 05:42:09 k8s-master kubelet[25301]: E1019 05:42:09.209052   25301 kuberuntime_sandbox.go:198] ListPodSandbox failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:09 k8s-master kubelet[25301]: E1019 05:42:09.209017   25301 remote_runtime.go:169] ListPodSandbox with filter &PodSandboxFilter{Id:,State:&PodSandboxStateValue{State:SANDBOX_READY,},LabelSelector:map[string]string{},} from runtime service failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:09 k8s-master kubelet[25301]: W1019 05:42:09.208899   25301 status_manager.go:431] Failed to get status for pod "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/etcd-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:09 k8s-master kubelet[25301]: W1019 05:42:09.010112   25301 prober.go:98] No ref for container "docker://33cbdd23b07873d1a5ff7a027681d97b11544463152e86330c964dea240fabfb" (kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b):kubedns)
Oct 19 05:42:08 k8s-master kubelet[25301]: E1019 05:42:08.968428   25301 generic.go:196] GenericPLEG: Unable to retrieve pods: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:08 k8s-master kubelet[25301]: E1019 05:42:08.968419   25301 kuberuntime_sandbox.go:198] ListPodSandbox failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:08 k8s-master kubelet[25301]: E1019 05:42:08.968386   25301 remote_runtime.go:169] ListPodSandbox with filter nil from runtime service failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:08 k8s-master kubelet[25301]: E1019 05:42:08.957355   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:08 k8s-master kubelet[25301]: I1019 05:42:08.859395   25301 kuberuntime_manager.go:738] checking backoff for container "kube-controller-manager" in pod "kube-controller-manager-k8s-master_kube-system(79f7c686bb6d415a1d71ac57fc06f7aa)"
Oct 19 05:42:08 k8s-master kubelet[25301]: E1019 05:42:08.838222   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:08 k8s-master kubelet[25301]: W1019 05:42:08.837789   25301 container.go:354] Failed to create summary reader for "/libcontainer_23323_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:08 k8s-master kubelet[25301]: W1019 05:42:08.784874   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23323_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23323_systemd_test_default.slice: no such file or directory
Oct 19 05:42:08 k8s-master kubelet[25301]: E1019 05:42:08.762378   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:08 k8s-master kubelet[25301]: E1019 05:42:08.638847   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:08 k8s-master kubelet[25301]: W1019 05:42:08.638708   25301 container.go:354] Failed to create summary reader for "/libcontainer_23309_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:08 k8s-master kubelet[25301]: W1019 05:42:08.634582   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23309_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23309_systemd_test_default.slice: no such file or directory
Oct 19 05:42:08 k8s-master kubelet[25301]: E1019 05:42:08.594571   25301 kubelet_node_status.go:378] Unable to update node status: update node status exceeds retry count
Oct 19 05:42:08 k8s-master kubelet[25301]: E1019 05:42:08.594550   25301 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:08 k8s-master kubelet[25301]: E1019 05:42:08.593938   25301 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:08 k8s-master kubelet[25301]: E1019 05:42:08.593376   25301 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:08 k8s-master kubelet[25301]: E1019 05:42:08.593122   25301 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:08 k8s-master kubelet[25301]: E1019 05:42:08.592977   25301 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:08 k8s-master kubelet[25301]: E1019 05:42:08.559209   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:08 k8s-master kubelet[25301]: E1019 05:42:08.549259   25301 eviction_manager.go:238] eviction manager: unexpected err: failed to get imageFs stats: failed to get image stats: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:08 k8s-master kubelet[25301]: E1019 05:42:08.549180   25301 kuberuntime_image.go:140] ListImages failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:08 k8s-master kubelet[25301]: E1019 05:42:08.545410   25301 remote_image.go:67] ListImages with filter nil from image service failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:08 k8s-master kubelet[25301]: E1019 05:42:08.491649   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:08 k8s-master kubelet[25301]: E1019 05:42:08.427026   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:08 k8s-master kubelet[25301]: E1019 05:42:08.268905   25301 pod_workers.go:182] Error syncing pod b6af75e4-b41a-11e7-8c24-08002782873b ("kube-flannel-ds-n747n_kube-system(b6af75e4-b41a-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kube-flannel" with CrashLoopBackOff: "Back-off 10s restarting failed container=kube-flannel pod=kube-flannel-ds-n747n_kube-system(b6af75e4-b41a-11e7-8c24-08002782873b)"
Oct 19 05:42:08 k8s-master kubelet[25301]: I1019 05:42:08.268871   25301 kuberuntime_manager.go:748] Back-off 10s restarting failed container=kube-flannel pod=kube-flannel-ds-n747n_kube-system(b6af75e4-b41a-11e7-8c24-08002782873b)
Oct 19 05:42:08 k8s-master kubelet[25301]: I1019 05:42:08.268738   25301 kuberuntime_manager.go:738] checking backoff for container "kube-flannel" in pod "kube-flannel-ds-n747n_kube-system(b6af75e4-b41a-11e7-8c24-08002782873b)"
Oct 19 05:42:08 k8s-master kubelet[25301]: I1019 05:42:08.268593   25301 kuberuntime_manager.go:499] Container {Name:kube-flannel Image:quay.io/coreos/flannel:v0.9.0-amd64 Command:[/opt/bin/flanneld --ip-masq --kube-subnet-mgr] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[{Name:POD_NAME Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {Name:POD_NAMESPACE Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}}] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:run ReadOnly:false MountPath:/run SubPath: MountPropagation:<nil>} {Name:flannel-cfg ReadOnly:false MountPath:/etc/kube-flannel/ SubPath: MountPropagation:<nil>} {Name:flannel-token-2sfpp ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 19 05:42:07 k8s-master kubelet[25301]: W1019 05:42:07.968028   25301 status_manager.go:431] Failed to get status for pod "kube-flannel-ds-n747n_kube-system(b6af75e4-b41a-11e7-8c24-08002782873b)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-flannel-ds-n747n: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:07 k8s-master kubelet[25301]: E1019 05:42:07.757484   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:07 k8s-master kubelet[25301]: E1019 05:42:07.557667   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:07 k8s-master kubelet[25301]: E1019 05:42:07.507361   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:07 k8s-master kubelet[25301]: E1019 05:42:07.358541   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:07 k8s-master kubelet[25301]: E1019 05:42:07.235692   25301 pod_workers.go:182] Error syncing pod b6af75e4-b41a-11e7-8c24-08002782873b ("kube-flannel-ds-n747n_kube-system(b6af75e4-b41a-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kube-flannel" with CrashLoopBackOff: "Back-off 10s restarting failed container=kube-flannel pod=kube-flannel-ds-n747n_kube-system(b6af75e4-b41a-11e7-8c24-08002782873b)"
Oct 19 05:42:07 k8s-master kubelet[25301]: I1019 05:42:07.235652   25301 kuberuntime_manager.go:748] Back-off 10s restarting failed container=kube-flannel pod=kube-flannel-ds-n747n_kube-system(b6af75e4-b41a-11e7-8c24-08002782873b)
Oct 19 05:42:07 k8s-master kubelet[25301]: I1019 05:42:07.235494   25301 kuberuntime_manager.go:738] checking backoff for container "kube-flannel" in pod "kube-flannel-ds-n747n_kube-system(b6af75e4-b41a-11e7-8c24-08002782873b)"
Oct 19 05:42:07 k8s-master kubelet[25301]: I1019 05:42:07.235306   25301 kuberuntime_manager.go:499] Container {Name:kube-flannel Image:quay.io/coreos/flannel:v0.9.0-amd64 Command:[/opt/bin/flanneld --ip-masq --kube-subnet-mgr] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[{Name:POD_NAME Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {Name:POD_NAMESPACE Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}}] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:run ReadOnly:false MountPath:/run SubPath: MountPropagation:<nil>} {Name:flannel-cfg ReadOnly:false MountPath:/etc/kube-flannel/ SubPath: MountPropagation:<nil>} {Name:flannel-token-2sfpp ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 19 05:42:06 k8s-master kubelet[25301]: W1019 05:42:06.935098   25301 status_manager.go:431] Failed to get status for pod "kube-flannel-ds-n747n_kube-system(b6af75e4-b41a-11e7-8c24-08002782873b)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-flannel-ds-n747n: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:06 k8s-master kubelet[25301]: E1019 05:42:06.661938   25301 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 19 05:42:06 k8s-master kubelet[25301]: E1019 05:42:06.558804   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:06 k8s-master kubelet[25301]: E1019 05:42:06.421917   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:06 k8s-master kubelet[25301]: W1019 05:42:06.421710   25301 container.go:354] Failed to create summary reader for "/libcontainer_23300_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:06 k8s-master kubelet[25301]: W1019 05:42:06.403398   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23300_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23300_systemd_test_default.slice: no such file or directory
Oct 19 05:42:06 k8s-master kubelet[25301]: E1019 05:42:06.378986   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:06 k8s-master kubelet[25301]: E1019 05:42:06.364501   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:06 k8s-master kubelet[25301]: W1019 05:42:06.332948   25301 prober.go:98] No ref for container "docker://33cbdd23b07873d1a5ff7a027681d97b11544463152e86330c964dea240fabfb" (kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b):kubedns)
Oct 19 05:42:06 k8s-master kubelet[25301]: E1019 05:42:06.308492   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:06 k8s-master kubelet[25301]: W1019 05:42:06.308333   25301 container.go:354] Failed to create summary reader for "/libcontainer_23293_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:06 k8s-master kubelet[25301]: W1019 05:42:06.294241   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23293_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23293_systemd_test_default.slice: no such file or directory
Oct 19 05:42:06 k8s-master kubelet[25301]: E1019 05:42:06.159171   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:06 k8s-master kubelet[25301]: E1019 05:42:06.071593   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:06 k8s-master kubelet[25301]: W1019 05:42:06.071379   25301 container.go:354] Failed to create summary reader for "/libcontainer_23269_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:05 k8s-master kubelet[25301]: W1019 05:42:05.957339   25301 status_manager.go:431] Failed to get status for pod "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/etcd-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:05 k8s-master kubelet[25301]: W1019 05:42:05.937779   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23269_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23269_systemd_test_default.slice: no such file or directory
Oct 19 05:42:05 k8s-master kubelet[25301]: E1019 05:42:05.901903   25301 generic.go:196] GenericPLEG: Unable to retrieve pods: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:05 k8s-master kubelet[25301]: E1019 05:42:05.901890   25301 kuberuntime_sandbox.go:198] ListPodSandbox failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:05 k8s-master kubelet[25301]: E1019 05:42:05.901858   25301 remote_runtime.go:169] ListPodSandbox with filter nil from runtime service failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:05 k8s-master kubelet[25301]: E1019 05:42:05.843063   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:05 k8s-master kubelet[25301]: W1019 05:42:05.842939   25301 container.go:354] Failed to create summary reader for "/libcontainer_23256_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:05 k8s-master kubelet[25301]: W1019 05:42:05.826192   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23256_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23256_systemd_test_default.slice: no such file or directory
Oct 19 05:42:05 k8s-master kubelet[25301]: W1019 05:42:05.459107   25301 status_manager.go:431] Failed to get status for pod "kube-flannel-ds-n747n_kube-system(b6af75e4-b41a-11e7-8c24-08002782873b)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-flannel-ds-n747n: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:05 k8s-master kubelet[25301]: E1019 05:42:05.457764   25301 kubelet.go:1915] Failed cleaning pods: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:05 k8s-master kubelet[25301]: E1019 05:42:05.457732   25301 kubelet_pods.go:964] Error listing containers: &status.statusError{Code:2, Message:"Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?", Details:[]*any.Any(nil)}
Oct 19 05:42:05 k8s-master kubelet[25301]: E1019 05:42:05.457707   25301 kuberuntime_sandbox.go:198] ListPodSandbox failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:05 k8s-master kubelet[25301]: E1019 05:42:05.206006   25301 remote_runtime.go:169] ListPodSandbox with filter &PodSandboxFilter{Id:,State:&PodSandboxStateValue{State:SANDBOX_READY,},LabelSelector:map[string]string{},} from runtime service failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:05 k8s-master kubelet[25301]: I1019 05:42:05.182792   25301 kuberuntime_manager.go:738] checking backoff for container "kube-flannel" in pod "kube-flannel-ds-n747n_kube-system(b6af75e4-b41a-11e7-8c24-08002782873b)"
Oct 19 05:42:05 k8s-master kubelet[25301]: I1019 05:42:05.182400   25301 kuberuntime_manager.go:499] Container {Name:kube-flannel Image:quay.io/coreos/flannel:v0.9.0-amd64 Command:[/opt/bin/flanneld --ip-masq --kube-subnet-mgr] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[{Name:POD_NAME Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {Name:POD_NAMESPACE Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}}] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:run ReadOnly:false MountPath:/run SubPath: MountPropagation:<nil>} {Name:flannel-cfg ReadOnly:false MountPath:/etc/kube-flannel/ SubPath: MountPropagation:<nil>} {Name:flannel-token-2sfpp ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 19 05:42:05 k8s-master kubelet[25301]: E1019 05:42:05.166987   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.958695   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.820659   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:04 k8s-master kubelet[25301]: W1019 05:42:04.820659   25301 container.go:354] Failed to create summary reader for "/libcontainer_23236_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:04 k8s-master kubelet[25301]: W1019 05:42:04.790154   25301 prober.go:98] No ref for container "docker://68914d3301b5970546e88236d3d55781f864d112f379cdf6baaaad3a6b631afb" (kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b):dnsmasq)
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.757908   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.739355   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:04 k8s-master kubelet[25301]: W1019 05:42:04.731469   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23236_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23236_systemd_test_default.slice: no such file or directory
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.652760   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:04 k8s-master kubelet[25301]: W1019 05:42:04.652625   25301 container.go:354] Failed to create summary reader for "/libcontainer_23222_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:04 k8s-master kubelet[25301]: W1019 05:42:04.651076   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23222_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23222_systemd_test_default.slice: no such file or directory
Oct 19 05:42:04 k8s-master kubelet[25301]: I1019 05:42:04.542129   25301 kuberuntime_manager.go:738] checking backoff for container "etcd" in pod "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)"
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.523988   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:04 k8s-master kubelet[25301]: W1019 05:42:04.523873   25301 container.go:354] Failed to create summary reader for "/libcontainer_23210_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.521021   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.504636   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.418052   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:04 k8s-master kubelet[25301]: W1019 05:42:04.417880   25301 container.go:354] Failed to create summary reader for "/libcontainer_23197_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:04 k8s-master kubelet[25301]: W1019 05:42:04.413373   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23197_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23197_systemd_test_default.slice: no such file or directory
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.123135   25301 pod_workers.go:182] Error syncing pod b2564e2e-b419-11e7-8c24-08002782873b ("kube-proxy-rrmr6_kube-system(b2564e2e-b419-11e7-8c24-08002782873b)"), skipping: failed to "KillPodSandbox" for "b2564e2e-b419-11e7-8c24-08002782873b" with KillPodSandboxError: "rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.123118   25301 kuberuntime_manager.go:580] killPodWithSyncResult failed: failed to "KillPodSandbox" for "b2564e2e-b419-11e7-8c24-08002782873b" with KillPodSandboxError: "rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.123086   25301 kuberuntime_manager.go:780] Failed to stop sandbox {"docker" "0aa57a2b259f35efb69f184207a304c3083fa863be74defafb752caba73c44dd"}
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.123064   25301 remote_runtime.go:115] StopPodSandbox "0aa57a2b259f35efb69f184207a304c3083fa863be74defafb752caba73c44dd" from runtime service failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.122969   25301 docker_sandbox.go:240] Failed to stop sandbox "0aa57a2b259f35efb69f184207a304c3083fa863be74defafb752caba73c44dd": Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.115786   25301 pod_workers.go:182] Error syncing pod d857b5bcdfd340d1ff3546b68f7f27c0 ("kube-apiserver-k8s-master_kube-system(d857b5bcdfd340d1ff3546b68f7f27c0)"), skipping: failed to "KillPodSandbox" for "d857b5bcdfd340d1ff3546b68f7f27c0" with KillPodSandboxError: "rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.115774   25301 kuberuntime_manager.go:580] killPodWithSyncResult failed: failed to "KillPodSandbox" for "d857b5bcdfd340d1ff3546b68f7f27c0" with KillPodSandboxError: "rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.115748   25301 kuberuntime_manager.go:780] Failed to stop sandbox {"docker" "3126fa7f4c1433d55c8ae9e611633a4e0ea4357758ff221747442e886df61a51"}
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.115734   25301 remote_runtime.go:115] StopPodSandbox "3126fa7f4c1433d55c8ae9e611633a4e0ea4357758ff221747442e886df61a51" from runtime service failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.115699   25301 kuberuntime_container.go:592] Container "docker://33cbdd23b07873d1a5ff7a027681d97b11544463152e86330c964dea240fabfb" termination failed with gracePeriod 30: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.115683   25301 remote_runtime.go:229] StopContainer "33cbdd23b07873d1a5ff7a027681d97b11544463152e86330c964dea240fabfb" from runtime service failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.115635   25301 pod_workers.go:182] Error syncing pod 79f7c686bb6d415a1d71ac57fc06f7aa ("kube-controller-manager-k8s-master_kube-system(79f7c686bb6d415a1d71ac57fc06f7aa)"), skipping: failed to "KillPodSandbox" for "79f7c686bb6d415a1d71ac57fc06f7aa" with KillPodSandboxError: "rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.115616   25301 kuberuntime_manager.go:580] killPodWithSyncResult failed: failed to "KillPodSandbox" for "79f7c686bb6d415a1d71ac57fc06f7aa" with KillPodSandboxError: "rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.115580   25301 kuberuntime_manager.go:780] Failed to stop sandbox {"docker" "dba5b9aacbc397838201742778227ccabda915ed75976c2975e9ab0a619e354a"}
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.115554   25301 remote_runtime.go:115] StopPodSandbox "dba5b9aacbc397838201742778227ccabda915ed75976c2975e9ab0a619e354a" from runtime service failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.115472   25301 docker_sandbox.go:240] Failed to stop sandbox "dba5b9aacbc397838201742778227ccabda915ed75976c2975e9ab0a619e354a": Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:04 k8s-master kubelet[25301]: E1019 05:42:04.115158   25301 docker_sandbox.go:240] Failed to stop sandbox "3126fa7f4c1433d55c8ae9e611633a4e0ea4357758ff221747442e886df61a51": Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:03 k8s-master kubelet[25301]: W1019 05:42:03.813925   25301 status_manager.go:431] Failed to get status for pod "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:03 k8s-master kubelet[25301]: E1019 05:42:03.775921   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:03 k8s-master kubelet[25301]: E1019 05:42:03.775813   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:03 k8s-master kubelet[25301]: E1019 05:42:03.766643   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:03 k8s-master kubelet[25301]: W1019 05:42:03.765829   25301 container.go:354] Failed to create summary reader for "/libcontainer_23179_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:03 k8s-master kubelet[25301]: W1019 05:42:03.765713   25301 container.go:354] Failed to create summary reader for "/libcontainer_23174_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:03 k8s-master kubelet[25301]: W1019 05:42:03.715467   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23179_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23179_systemd_test_default.slice: no such file or directory
Oct 19 05:42:03 k8s-master kubelet[25301]: W1019 05:42:03.596444   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23174_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23174_systemd_test_default.slice: no such file or directory
Oct 19 05:42:03 k8s-master kubelet[25301]: E1019 05:42:03.570603   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:03 k8s-master kubelet[25301]: E1019 05:42:03.469036   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:03 k8s-master kubelet[25301]: W1019 05:42:03.468941   25301 container.go:354] Failed to create summary reader for "/libcontainer_23158_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:03 k8s-master kubelet[25301]: W1019 05:42:03.466123   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23158_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23158_systemd_test_default.slice: no such file or directory
Oct 19 05:42:03 k8s-master kubelet[25301]: E1019 05:42:03.357439   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:03 k8s-master kubelet[25301]: I1019 05:42:03.328150   25301 kuberuntime_manager.go:738] checking backoff for container "install-cni" in pod "kube-flannel-ds-n747n_kube-system(b6af75e4-b41a-11e7-8c24-08002782873b)"
Oct 19 05:42:03 k8s-master kubelet[25301]: E1019 05:42:03.306955   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:03 k8s-master kubelet[25301]: W1019 05:42:03.306802   25301 container.go:354] Failed to create summary reader for "/libcontainer_23139_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:03 k8s-master kubelet[25301]: E1019 05:42:03.161801   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:02 k8s-master kubelet[25301]: W1019 05:42:02.906745   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23111_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23111_systemd_test_default.slice: no such file or directory
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.906704   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:02 k8s-master kubelet[25301]: W1019 05:42:02.906558   25301 container.go:354] Failed to create summary reader for "/libcontainer_23111_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.860192   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:02 k8s-master kubelet[25301]: W1019 05:42:02.860077   25301 container.go:354] Failed to create summary reader for "/libcontainer_23107_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.854483   25301 pod_workers.go:182] Error syncing pod b2564e2e-b419-11e7-8c24-08002782873b ("kube-proxy-rrmr6_kube-system(b2564e2e-b419-11e7-8c24-08002782873b)"), skipping: failed to "KillPodSandbox" for "b2564e2e-b419-11e7-8c24-08002782873b" with KillPodSandboxError: "rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.854435   25301 kuberuntime_manager.go:580] killPodWithSyncResult failed: failed to "KillPodSandbox" for "b2564e2e-b419-11e7-8c24-08002782873b" with KillPodSandboxError: "rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.854393   25301 kuberuntime_manager.go:780] Failed to stop sandbox {"docker" "0aa57a2b259f35efb69f184207a304c3083fa863be74defafb752caba73c44dd"}
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.854336   25301 remote_runtime.go:115] StopPodSandbox "0aa57a2b259f35efb69f184207a304c3083fa863be74defafb752caba73c44dd" from runtime service failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.854009   25301 docker_sandbox.go:240] Failed to stop sandbox "0aa57a2b259f35efb69f184207a304c3083fa863be74defafb752caba73c44dd": Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.822896   25301 pod_workers.go:182] Error syncing pod 40eb0889c614345e2a2714d4ee7d1cc0 ("etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)"), skipping: failed to "KillPodSandbox" for "40eb0889c614345e2a2714d4ee7d1cc0" with KillPodSandboxError: "rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.822896   25301 kuberuntime_manager.go:580] killPodWithSyncResult failed: failed to "KillPodSandbox" for "40eb0889c614345e2a2714d4ee7d1cc0" with KillPodSandboxError: "rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.822896   25301 kuberuntime_manager.go:780] Failed to stop sandbox {"docker" "7f00e39255e6a05b814ad7abd7b258fea62328acc0e64e91f97fc743e8693e29"}
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.822896   25301 remote_runtime.go:115] StopPodSandbox "7f00e39255e6a05b814ad7abd7b258fea62328acc0e64e91f97fc743e8693e29" from runtime service failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.822816   25301 docker_sandbox.go:240] Failed to stop sandbox "7f00e39255e6a05b814ad7abd7b258fea62328acc0e64e91f97fc743e8693e29": Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.788355   25301 pod_workers.go:182] Error syncing pod 79f7c686bb6d415a1d71ac57fc06f7aa ("kube-controller-manager-k8s-master_kube-system(79f7c686bb6d415a1d71ac57fc06f7aa)"), skipping: failed to "KillPodSandbox" for "79f7c686bb6d415a1d71ac57fc06f7aa" with KillPodSandboxError: "rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.788336   25301 kuberuntime_manager.go:580] killPodWithSyncResult failed: failed to "KillPodSandbox" for "79f7c686bb6d415a1d71ac57fc06f7aa" with KillPodSandboxError: "rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.788301   25301 kuberuntime_manager.go:780] Failed to stop sandbox {"docker" "dba5b9aacbc397838201742778227ccabda915ed75976c2975e9ab0a619e354a"}
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.788267   25301 remote_runtime.go:115] StopPodSandbox "dba5b9aacbc397838201742778227ccabda915ed75976c2975e9ab0a619e354a" from runtime service failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.788171   25301 docker_sandbox.go:240] Failed to stop sandbox "dba5b9aacbc397838201742778227ccabda915ed75976c2975e9ab0a619e354a": Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.636609   25301 pod_workers.go:182] Error syncing pod d857b5bcdfd340d1ff3546b68f7f27c0 ("kube-apiserver-k8s-master_kube-system(d857b5bcdfd340d1ff3546b68f7f27c0)"), skipping: failed to "KillPodSandbox" for "d857b5bcdfd340d1ff3546b68f7f27c0" with KillPodSandboxError: "rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.636583   25301 kuberuntime_manager.go:580] killPodWithSyncResult failed: failed to "KillPodSandbox" for "d857b5bcdfd340d1ff3546b68f7f27c0" with KillPodSandboxError: "rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.636541   25301 kuberuntime_manager.go:780] Failed to stop sandbox {"docker" "3126fa7f4c1433d55c8ae9e611633a4e0ea4357758ff221747442e886df61a51"}
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.636513   25301 remote_runtime.go:115] StopPodSandbox "3126fa7f4c1433d55c8ae9e611633a4e0ea4357758ff221747442e886df61a51" from runtime service failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.636348   25301 docker_sandbox.go:240] Failed to stop sandbox "3126fa7f4c1433d55c8ae9e611633a4e0ea4357758ff221747442e886df61a51": Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.614520   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:02 k8s-master kubelet[25301]: W1019 05:42:02.614376   25301 container.go:354] Failed to create summary reader for "/libcontainer_23085_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:02 k8s-master kubelet[25301]: W1019 05:42:02.600948   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23085_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23085_systemd_test_default.slice: no such file or directory
Oct 19 05:42:02 k8s-master kubelet[25301]: W1019 05:42:02.564036   25301 status_manager.go:431] Failed to get status for pod "kube-proxy-rrmr6_kube-system(b2564e2e-b419-11e7-8c24-08002782873b)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-proxy-rrmr6: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:02 k8s-master kubelet[25301]: W1019 05:42:02.538353   25301 pod_container_deletor.go:77] Container "0aa57a2b259f35efb69f184207a304c3083fa863be74defafb752caba73c44dd" not found in pod's containers
Oct 19 05:42:02 k8s-master kubelet[25301]: ]
Oct 19 05:42:02 k8s-master kubelet[25301]: , failed to "KillPodSandbox" for "b2593c3c-b419-11e7-8c24-08002782873b" with KillPodSandboxError: "rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"
Oct 19 05:42:02 k8s-master kubelet[25301]: , failed to "KillContainer" for "dnsmasq" with KillContainerError: "rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.531608   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "KillContainer" for "kubedns" with KillContainerError: "rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"
Oct 19 05:42:02 k8s-master kubelet[25301]: ]
Oct 19 05:42:02 k8s-master kubelet[25301]: , failed to "KillPodSandbox" for "b2593c3c-b419-11e7-8c24-08002782873b" with KillPodSandboxError: "rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"
Oct 19 05:42:02 k8s-master kubelet[25301]: , failed to "KillContainer" for "dnsmasq" with KillContainerError: "rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.531553   25301 kuberuntime_manager.go:580] killPodWithSyncResult failed: [failed to "KillContainer" for "kubedns" with KillContainerError: "rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?"
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.531464   25301 kuberuntime_manager.go:780] Failed to stop sandbox {"docker" "b3d4007b7a1975f340eee2f9610ad3e0f556787989e3ed7d69c4a3e212682d6c"}
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.531392   25301 remote_runtime.go:115] StopPodSandbox "b3d4007b7a1975f340eee2f9610ad3e0f556787989e3ed7d69c4a3e212682d6c" from runtime service failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.529533   25301 docker_sandbox.go:240] Failed to stop sandbox "b3d4007b7a1975f340eee2f9610ad3e0f556787989e3ed7d69c4a3e212682d6c": Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.527433   25301 kuberuntime_container.go:592] Container "docker://68914d3301b5970546e88236d3d55781f864d112f379cdf6baaaad3a6b631afb" termination failed with gracePeriod 30: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.527413   25301 remote_runtime.go:229] StopContainer "68914d3301b5970546e88236d3d55781f864d112f379cdf6baaaad3a6b631afb" from runtime service failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.527364   25301 kuberuntime_container.go:592] Container "docker://33cbdd23b07873d1a5ff7a027681d97b11544463152e86330c964dea240fabfb" termination failed with gracePeriod 30: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.527315   25301 remote_runtime.go:229] StopContainer "33cbdd23b07873d1a5ff7a027681d97b11544463152e86330c964dea240fabfb" from runtime service failed: rpc error: code = Unknown desc = Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
Oct 19 05:42:02 k8s-master kubelet[25301]: W1019 05:42:02.489606   25301 status_manager.go:431] Failed to get status for pod "kube-controller-manager-k8s-master_kube-system(79f7c686bb6d415a1d71ac57fc06f7aa)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:02 k8s-master kubelet[25301]: W1019 05:42:02.482599   25301 pod_container_deletor.go:77] Container "dba5b9aacbc397838201742778227ccabda915ed75976c2975e9ab0a619e354a" not found in pod's containers
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.474504   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:02 k8s-master kubelet[25301]: W1019 05:42:02.470238   25301 status_manager.go:431] Failed to get status for pod "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/etcd-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:02 k8s-master kubelet[25301]: W1019 05:42:02.469523   25301 pod_container_deletor.go:77] Container "7f00e39255e6a05b814ad7abd7b258fea62328acc0e64e91f97fc743e8693e29" not found in pod's containers
Oct 19 05:42:02 k8s-master kubelet[25301]: W1019 05:42:02.352961   25301 pod_container_deletor.go:77] Container "d3a074dda4d57ef9ef26c6864debc7b88b7a821bcde403287f41ff9f452229f5" not found in pod's containers
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.328252   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:02 k8s-master kubelet[25301]: W1019 05:42:02.328145   25301 container.go:354] Failed to create summary reader for "/libcontainer_23069_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:02 k8s-master kubelet[25301]: W1019 05:42:02.315004   25301 status_manager.go:431] Failed to get status for pod "kube-apiserver-k8s-master_kube-system(d857b5bcdfd340d1ff3546b68f7f27c0)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:02 k8s-master kubelet[25301]: W1019 05:42:02.313082   25301 pod_container_deletor.go:77] Container "3126fa7f4c1433d55c8ae9e611633a4e0ea4357758ff221747442e886df61a51" not found in pod's containers
Oct 19 05:42:02 k8s-master kubelet[25301]: W1019 05:42:02.277140   25301 status_manager.go:431] Failed to get status for pod "kube-flannel-ds-n747n_kube-system(b6af75e4-b41a-11e7-8c24-08002782873b)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-flannel-ds-n747n: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:02 k8s-master kubelet[25301]: W1019 05:42:02.276464   25301 pod_container_deletor.go:77] Container "9703db6dae57b68b8b73c5e0ff73e19483e9cc847e9b7949dec866dfa28baa9e" not found in pod's containers
Oct 19 05:42:02 k8s-master kubelet[25301]: W1019 05:42:02.224412   25301 status_manager.go:431] Failed to get status for pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-dns-545bc4bfd4-l2xsw: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:02 k8s-master kubelet[25301]: W1019 05:42:02.223428   25301 pod_container_deletor.go:77] Container "b3d4007b7a1975f340eee2f9610ad3e0f556787989e3ed7d69c4a3e212682d6c" not found in pod's containers
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.170624   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.170514   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.161084   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:02 k8s-master kubelet[25301]: W1019 05:42:02.160953   25301 container.go:354] Failed to create summary reader for "/libcontainer_23054_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:02 k8s-master kubelet[25301]: W1019 05:42:02.157191   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23054_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23054_systemd_test_default.slice: no such file or directory
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.151346   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:02 k8s-master kubelet[25301]: E1019 05:42:02.138012   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:02 k8s-master kubelet[25301]: W1019 05:42:02.109003   25301 docker_sandbox.go:343] failed to read pod IP from plugin/docker: Couldn't find network status for kube-system/kube-dns-545bc4bfd4-l2xsw through plugin: invalid network status for
Oct 19 05:42:01 k8s-master kubelet[25301]: E1019 05:42:01.704800   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:01 k8s-master kubelet[25301]: W1019 05:42:01.704800   25301 container.go:354] Failed to create summary reader for "/libcontainer_23044_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:01 k8s-master kubelet[25301]: W1019 05:42:01.700807   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23044_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23044_systemd_test_default.slice: no such file or directory
Oct 19 05:42:01 k8s-master kubelet[25301]: E1019 05:42:01.650300   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:01 k8s-master kubelet[25301]: W1019 05:42:01.650174   25301 container.go:354] Failed to create summary reader for "/libcontainer_23038_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:01 k8s-master kubelet[25301]: W1019 05:42:01.649870   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23038_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23038_systemd_test_default.slice: no such file or directory
Oct 19 05:42:01 k8s-master kubelet[25301]: E1019 05:42:01.627627   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:01 k8s-master kubelet[25301]: E1019 05:42:01.436743   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:01 k8s-master kubelet[25301]: W1019 05:42:01.436605   25301 container.go:354] Failed to create summary reader for "/libcontainer_23026_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:01 k8s-master kubelet[25301]: W1019 05:42:01.431698   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23026_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23026_systemd_test_default.slice: no such file or directory
Oct 19 05:42:01 k8s-master kubelet[25301]: E1019 05:42:01.370813   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:01 k8s-master kubelet[25301]: W1019 05:42:01.370640   25301 container.go:354] Failed to create summary reader for "/libcontainer_23019_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:01 k8s-master kubelet[25301]: W1019 05:42:01.354448   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23019_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23019_systemd_test_default.slice: no such file or directory
Oct 19 05:42:01 k8s-master kubelet[25301]: E1019 05:42:01.273435   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:01 k8s-master kubelet[25301]: W1019 05:42:01.273241   25301 container.go:354] Failed to create summary reader for "/libcontainer_23009_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:01 k8s-master kubelet[25301]: W1019 05:42:01.262108   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23009_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23009_systemd_test_default.slice: no such file or directory
Oct 19 05:42:01 k8s-master kubelet[25301]: E1019 05:42:01.165185   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:01 k8s-master kubelet[25301]: E1019 05:42:01.165130   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:01 k8s-master kubelet[25301]: E1019 05:42:01.164187   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 05:42:01 k8s-master kubelet[25301]: W1019 05:42:01.163951   25301 container.go:354] Failed to create summary reader for "/libcontainer_23006_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:01 k8s-master kubelet[25301]: W1019 05:42:01.160680   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23006_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23006_systemd_test_default.slice: no such file or directory
Oct 19 05:42:01 k8s-master kubelet[25301]: E1019 05:42:01.149450   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:01 k8s-master kubelet[25301]: W1019 05:42:01.084969   25301 status_manager.go:431] Failed to get status for pod "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:01 k8s-master kubelet[25301]: W1019 05:42:01.084208   25301 pod_container_deletor.go:77] Container "7d46e12c310c183ebe9686063d535c756e8f2adaba6cf4b300800a1dfa74d365" not found in pod's containers
Oct 19 05:42:00 k8s-master kubelet[25301]: W1019 05:42:00.851005   25301 container.go:354] Failed to create summary reader for "/libcontainer_22984_systemd_test_default.slice": none of the resources are being tracked.
Oct 19 05:42:00 k8s-master kubelet[25301]: W1019 05:42:00.846809   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_22984_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_22984_systemd_test_default.slice: no such file or directory
Oct 19 05:42:00 k8s-master kubelet[25301]: E1019 05:42:00.296611   25301 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 19 05:42:00 k8s-master kubelet[25301]: E1019 05:42:00.154697   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:00 k8s-master kubelet[25301]: E1019 05:42:00.152933   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:42:00 k8s-master kubelet[25301]: E1019 05:42:00.148557   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:41:59 k8s-master kubelet[25301]: W1019 05:41:59.217748   25301 status_manager.go:431] Failed to get status for pod "kube-controller-manager-k8s-master_kube-system(79f7c686bb6d415a1d71ac57fc06f7aa)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:41:59 k8s-master kubelet[25301]: E1019 05:41:59.146411   25301 reflector.go:315] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to watch *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=26704&timeoutSeconds=431&watch=true: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:41:59 k8s-master kubelet[25301]: E1019 05:41:59.146361   25301 reflector.go:315] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to watch *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=2332&timeoutSeconds=330&watch=true: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 05:41:59 k8s-master kubelet[25301]: E1019 05:41:59.145421   25301 reflector.go:315] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to watch *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=26721&timeoutSeconds=458&watch=true: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 19 02:40:16 k8s-master kubelet[25301]: W1019 02:40:16.040353   25301 conversion.go:110] Could not get instant cpu stats: cumulative stats decrease
Oct 19 02:38:39 k8s-master kubelet[25301]: E1019 02:38:39.077738   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 01:15:46 k8s-master kubelet[25301]: E1019 01:15:46.962610   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 01:15:40 k8s-master kubelet[25301]: E1019 01:15:40.025149   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 01:15:32 k8s-master kubelet[25301]: E1019 01:15:32.268030   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 01:15:28 k8s-master kubelet[25301]: E1019 01:15:28.028251   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 01:15:13 k8s-master kubelet[25301]: E1019 01:15:13.727918   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 01:15:06 k8s-master kubelet[25301]: E1019 01:15:06.018760   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 01:14:56 k8s-master kubelet[25301]: E1019 01:14:56.597082   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 01:14:54 k8s-master kubelet[25301]: E1019 01:14:54.691103   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 01:14:46 k8s-master kubelet[25301]: E1019 01:14:46.599489   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 01:10:32 k8s-master kubelet[25301]: E1019 01:10:32.214310   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 01:10:17 k8s-master kubelet[25301]: E1019 01:10:17.234191   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 01:01:33 k8s-master kubelet[25301]: E1019 01:01:33.663929   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 01:01:20 k8s-master kubelet[25301]: E1019 01:01:20.083858   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 01:00:35 k8s-master kubelet[25301]: E1019 01:00:35.104071   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 01:00:18 k8s-master kubelet[25301]: E1019 01:00:18.868444   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 00:50:33 k8s-master kubelet[25301]: E1019 00:50:33.892903   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 00:50:21 k8s-master kubelet[25301]: E1019 00:50:21.229575   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 00:40:33 k8s-master kubelet[25301]: E1019 00:40:33.983574   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 00:40:17 k8s-master kubelet[25301]: E1019 00:40:17.900523   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 00:30:29 k8s-master kubelet[25301]: E1019 00:30:29.350864   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 00:30:16 k8s-master kubelet[25301]: E1019 00:30:16.795029   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 00:20:27 k8s-master kubelet[25301]: E1019 00:20:27.445438   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 00:20:12 k8s-master kubelet[25301]: E1019 00:20:12.648246   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 00:10:46 k8s-master kubelet[25301]: E1019 00:10:46.115862   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 00:10:28 k8s-master kubelet[25301]: E1019 00:10:28.965567   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 00:10:17 k8s-master kubelet[25301]: E1019 00:10:17.806542   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 00:01:48 k8s-master kubelet[25301]: E1019 00:01:48.933035   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 00:01:27 k8s-master kubelet[25301]: E1019 00:01:27.350669   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 00:01:11 k8s-master kubelet[25301]: E1019 00:01:11.605010   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 00:00:45 k8s-master kubelet[25301]: E1019 00:00:45.448856   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 00:00:27 k8s-master kubelet[25301]: E1019 00:00:27.597920   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 19 00:00:12 k8s-master kubelet[25301]: E1019 00:00:12.532661   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:58:37 k8s-master kubelet[25301]: E1018 23:58:37.833720   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:58:37 k8s-master kubelet[25301]: W1018 23:58:37.833623   25301 container.go:354] Failed to create summary reader for "/libcontainer_30633_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:58:37 k8s-master kubelet[25301]: I1018 23:58:37.503120   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:58:37 k8s-master kubelet[25301]: I1018 23:58:37.502504   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:58:24 k8s-master kubelet[25301]: E1018 23:58:24.498951   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:58:24 k8s-master kubelet[25301]: I1018 23:58:24.498927   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:58:24 k8s-master kubelet[25301]: I1018 23:58:24.498846   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:58:24 k8s-master kubelet[25301]: I1018 23:58:24.498692   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:58:12 k8s-master kubelet[25301]: E1018 23:58:12.499177   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:58:12 k8s-master kubelet[25301]: I1018 23:58:12.499144   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:58:12 k8s-master kubelet[25301]: I1018 23:58:12.499040   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:58:12 k8s-master kubelet[25301]: I1018 23:58:12.498839   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:57:57 k8s-master kubelet[25301]: E1018 23:57:57.503603   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:57:57 k8s-master kubelet[25301]: I1018 23:57:57.503574   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:57:57 k8s-master kubelet[25301]: I1018 23:57:57.503490   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:57:57 k8s-master kubelet[25301]: I1018 23:57:57.503324   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:57:46 k8s-master kubelet[25301]: E1018 23:57:46.349134   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:57:45 k8s-master kubelet[25301]: E1018 23:57:45.069776   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:57:42 k8s-master kubelet[25301]: E1018 23:57:42.500114   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:57:42 k8s-master kubelet[25301]: I1018 23:57:42.500086   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:57:42 k8s-master kubelet[25301]: I1018 23:57:42.499996   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:57:42 k8s-master kubelet[25301]: I1018 23:57:42.499797   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:57:31 k8s-master kubelet[25301]: E1018 23:57:31.212417   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:57:31 k8s-master kubelet[25301]: I1018 23:57:31.212391   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:57:31 k8s-master kubelet[25301]: I1018 23:57:31.212313   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:57:31 k8s-master kubelet[25301]: I1018 23:57:31.212132   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:57:30 k8s-master kubelet[25301]: E1018 23:57:30.787544   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:57:30 k8s-master kubelet[25301]: W1018 23:57:30.787450   25301 container.go:354] Failed to create summary reader for "/libcontainer_30426_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:57:30 k8s-master kubelet[25301]: E1018 23:57:30.785234   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:57:30 k8s-master kubelet[25301]: I1018 23:57:30.785210   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:57:30 k8s-master kubelet[25301]: I1018 23:57:30.785095   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:57:30 k8s-master kubelet[25301]: W1018 23:57:30.717041   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_30426_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_30426_systemd_test_default.slice: no such file or directory
Oct 18 23:57:30 k8s-master kubelet[25301]: W1018 23:57:30.634614   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_30412_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_30412_systemd_test_default.slice: no such file or directory
Oct 18 23:57:30 k8s-master kubelet[25301]: E1018 23:57:30.634583   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:57:30 k8s-master kubelet[25301]: W1018 23:57:30.634399   25301 container.go:354] Failed to create summary reader for "/libcontainer_30412_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:57:30 k8s-master kubelet[25301]: I1018 23:57:30.498924   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:57:30 k8s-master kubelet[25301]: I1018 23:57:30.498751   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:57:30 k8s-master kubelet[25301]: I1018 23:57:30.498681   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:57:18 k8s-master kubelet[25301]: ]
Oct 18 23:57:18 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:57:18 k8s-master kubelet[25301]: E1018 23:57:18.499299   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:57:18 k8s-master kubelet[25301]: I1018 23:57:18.499260   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:57:18 k8s-master kubelet[25301]: I1018 23:57:18.499195   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:57:18 k8s-master kubelet[25301]: I1018 23:57:18.499189   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:57:18 k8s-master kubelet[25301]: I1018 23:57:18.499092   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:57:18 k8s-master kubelet[25301]: I1018 23:57:18.498935   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:57:18 k8s-master kubelet[25301]: I1018 23:57:18.498861   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:57:05 k8s-master kubelet[25301]: ]
Oct 18 23:57:05 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:57:05 k8s-master kubelet[25301]: E1018 23:57:05.502389   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:57:05 k8s-master kubelet[25301]: I1018 23:57:05.502358   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:57:05 k8s-master kubelet[25301]: I1018 23:57:05.502291   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:57:05 k8s-master kubelet[25301]: I1018 23:57:05.502284   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:57:05 k8s-master kubelet[25301]: I1018 23:57:05.502175   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:57:05 k8s-master kubelet[25301]: I1018 23:57:05.501998   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:57:05 k8s-master kubelet[25301]: I1018 23:57:05.501930   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:56:52 k8s-master kubelet[25301]: ]
Oct 18 23:56:52 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:56:52 k8s-master kubelet[25301]: E1018 23:56:52.500250   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:56:52 k8s-master kubelet[25301]: I1018 23:56:52.500216   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:56:52 k8s-master kubelet[25301]: I1018 23:56:52.500147   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:56:52 k8s-master kubelet[25301]: I1018 23:56:52.500140   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:56:52 k8s-master kubelet[25301]: I1018 23:56:52.500024   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:56:52 k8s-master kubelet[25301]: I1018 23:56:52.499531   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:56:52 k8s-master kubelet[25301]: I1018 23:56:52.499294   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:56:40 k8s-master kubelet[25301]: ]
Oct 18 23:56:40 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:56:40 k8s-master kubelet[25301]: E1018 23:56:40.499180   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:56:40 k8s-master kubelet[25301]: I1018 23:56:40.499147   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:56:40 k8s-master kubelet[25301]: I1018 23:56:40.499091   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:56:40 k8s-master kubelet[25301]: I1018 23:56:40.499084   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:56:40 k8s-master kubelet[25301]: I1018 23:56:40.498979   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:56:40 k8s-master kubelet[25301]: I1018 23:56:40.498802   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:56:40 k8s-master kubelet[25301]: I1018 23:56:40.498731   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:56:29 k8s-master kubelet[25301]: ]
Oct 18 23:56:29 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:56:29 k8s-master kubelet[25301]: E1018 23:56:29.501801   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:56:29 k8s-master kubelet[25301]: I1018 23:56:29.501773   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:56:29 k8s-master kubelet[25301]: I1018 23:56:29.501720   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:56:29 k8s-master kubelet[25301]: I1018 23:56:29.501714   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:56:29 k8s-master kubelet[25301]: I1018 23:56:29.501619   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:56:29 k8s-master kubelet[25301]: I1018 23:56:29.501461   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:56:29 k8s-master kubelet[25301]: I1018 23:56:29.501401   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:56:15 k8s-master kubelet[25301]: ]
Oct 18 23:56:15 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:56:15 k8s-master kubelet[25301]: E1018 23:56:15.504614   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:56:15 k8s-master kubelet[25301]: I1018 23:56:15.504580   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:56:15 k8s-master kubelet[25301]: I1018 23:56:15.504521   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:56:15 k8s-master kubelet[25301]: I1018 23:56:15.504514   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:56:15 k8s-master kubelet[25301]: I1018 23:56:15.504406   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:56:15 k8s-master kubelet[25301]: I1018 23:56:15.504210   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:56:15 k8s-master kubelet[25301]: I1018 23:56:15.504146   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:56:04 k8s-master kubelet[25301]: ]
Oct 18 23:56:04 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:56:04 k8s-master kubelet[25301]: E1018 23:56:04.499264   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:56:04 k8s-master kubelet[25301]: I1018 23:56:04.499229   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:56:04 k8s-master kubelet[25301]: I1018 23:56:04.499165   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:56:04 k8s-master kubelet[25301]: I1018 23:56:04.499158   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:56:04 k8s-master kubelet[25301]: I1018 23:56:04.499051   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:56:04 k8s-master kubelet[25301]: I1018 23:56:04.498851   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:56:04 k8s-master kubelet[25301]: I1018 23:56:04.498799   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:55:52 k8s-master kubelet[25301]: ]
Oct 18 23:55:52 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:55:52 k8s-master kubelet[25301]: E1018 23:55:52.509610   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:55:52 k8s-master kubelet[25301]: I1018 23:55:52.509603   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:55:52 k8s-master kubelet[25301]: I1018 23:55:52.509603   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:55:52 k8s-master kubelet[25301]: I1018 23:55:52.509603   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:55:52 k8s-master kubelet[25301]: I1018 23:55:52.509603   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:55:52 k8s-master kubelet[25301]: I1018 23:55:52.509426   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:55:52 k8s-master kubelet[25301]: I1018 23:55:52.509318   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:55:45 k8s-master kubelet[25301]: E1018 23:55:45.058890   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:55:37 k8s-master kubelet[25301]: ]
Oct 18 23:55:37 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:55:37 k8s-master kubelet[25301]: E1018 23:55:37.502169   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:55:37 k8s-master kubelet[25301]: I1018 23:55:37.502138   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:55:37 k8s-master kubelet[25301]: I1018 23:55:37.502079   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:55:37 k8s-master kubelet[25301]: I1018 23:55:37.502072   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:55:37 k8s-master kubelet[25301]: I1018 23:55:37.501959   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:55:37 k8s-master kubelet[25301]: I1018 23:55:37.501779   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:55:37 k8s-master kubelet[25301]: I1018 23:55:37.501714   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:55:31 k8s-master kubelet[25301]: E1018 23:55:31.210520   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:55:25 k8s-master kubelet[25301]: ]
Oct 18 23:55:25 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:55:25 k8s-master kubelet[25301]: E1018 23:55:25.503062   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:55:25 k8s-master kubelet[25301]: I1018 23:55:25.503032   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:55:25 k8s-master kubelet[25301]: I1018 23:55:25.502972   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:55:25 k8s-master kubelet[25301]: I1018 23:55:25.502965   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:55:25 k8s-master kubelet[25301]: I1018 23:55:25.502855   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:55:25 k8s-master kubelet[25301]: I1018 23:55:25.502682   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:55:25 k8s-master kubelet[25301]: I1018 23:55:25.502619   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:55:11 k8s-master kubelet[25301]: ]
Oct 18 23:55:11 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:55:11 k8s-master kubelet[25301]: E1018 23:55:11.510940   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:55:11 k8s-master kubelet[25301]: I1018 23:55:11.510850   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:55:11 k8s-master kubelet[25301]: I1018 23:55:11.505804   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:55:11 k8s-master kubelet[25301]: I1018 23:55:11.505788   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:55:11 k8s-master kubelet[25301]: I1018 23:55:11.505610   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:55:11 k8s-master kubelet[25301]: I1018 23:55:11.505343   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:55:11 k8s-master kubelet[25301]: I1018 23:55:11.504238   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:55:10 k8s-master kubelet[25301]: E1018 23:55:10.837088   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:54:58 k8s-master kubelet[25301]: ]
Oct 18 23:54:58 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:54:58 k8s-master kubelet[25301]: E1018 23:54:58.499295   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:54:58 k8s-master kubelet[25301]: I1018 23:54:58.499260   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:54:58 k8s-master kubelet[25301]: I1018 23:54:58.499202   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:54:58 k8s-master kubelet[25301]: I1018 23:54:58.499195   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:54:58 k8s-master kubelet[25301]: I1018 23:54:58.499067   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:54:58 k8s-master kubelet[25301]: I1018 23:54:58.498900   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:54:58 k8s-master kubelet[25301]: I1018 23:54:58.498843   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:54:54 k8s-master kubelet[25301]: E1018 23:54:54.651772   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:54:46 k8s-master kubelet[25301]: ]
Oct 18 23:54:46 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:54:46 k8s-master kubelet[25301]: E1018 23:54:46.499245   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:54:46 k8s-master kubelet[25301]: I1018 23:54:46.499211   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:54:46 k8s-master kubelet[25301]: I1018 23:54:46.499145   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:54:46 k8s-master kubelet[25301]: I1018 23:54:46.499137   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:54:46 k8s-master kubelet[25301]: I1018 23:54:46.499029   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:54:46 k8s-master kubelet[25301]: I1018 23:54:46.498868   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:54:46 k8s-master kubelet[25301]: I1018 23:54:46.498814   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:54:34 k8s-master kubelet[25301]: ]
Oct 18 23:54:34 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:54:34 k8s-master kubelet[25301]: E1018 23:54:34.502691   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:54:34 k8s-master kubelet[25301]: I1018 23:54:34.502658   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:54:34 k8s-master kubelet[25301]: I1018 23:54:34.502601   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:54:34 k8s-master kubelet[25301]: I1018 23:54:34.502595   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:54:34 k8s-master kubelet[25301]: I1018 23:54:34.502489   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:54:34 k8s-master kubelet[25301]: I1018 23:54:34.502321   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:54:34 k8s-master kubelet[25301]: I1018 23:54:34.502266   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:54:23 k8s-master kubelet[25301]: ]
Oct 18 23:54:23 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:54:23 k8s-master kubelet[25301]: E1018 23:54:23.502510   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:54:23 k8s-master kubelet[25301]: I1018 23:54:23.502483   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:54:23 k8s-master kubelet[25301]: I1018 23:54:23.502428   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:54:23 k8s-master kubelet[25301]: I1018 23:54:23.502421   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:54:23 k8s-master kubelet[25301]: I1018 23:54:23.502324   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:54:23 k8s-master kubelet[25301]: I1018 23:54:23.502174   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:54:23 k8s-master kubelet[25301]: I1018 23:54:23.502073   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:54:08 k8s-master kubelet[25301]: ]
Oct 18 23:54:08 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:54:08 k8s-master kubelet[25301]: E1018 23:54:08.499089   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:54:08 k8s-master kubelet[25301]: I1018 23:54:08.499051   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:54:08 k8s-master kubelet[25301]: I1018 23:54:08.498999   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:54:08 k8s-master kubelet[25301]: I1018 23:54:08.498993   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:54:08 k8s-master kubelet[25301]: I1018 23:54:08.498898   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:54:08 k8s-master kubelet[25301]: I1018 23:54:08.498730   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:54:08 k8s-master kubelet[25301]: I1018 23:54:08.498675   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:53:53 k8s-master kubelet[25301]: ]
Oct 18 23:53:53 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:53:53 k8s-master kubelet[25301]: E1018 23:53:53.503292   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:53:53 k8s-master kubelet[25301]: I1018 23:53:53.503259   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:53:53 k8s-master kubelet[25301]: I1018 23:53:53.503204   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:53:53 k8s-master kubelet[25301]: I1018 23:53:53.503198   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:53:53 k8s-master kubelet[25301]: I1018 23:53:53.503099   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:53:53 k8s-master kubelet[25301]: I1018 23:53:53.502875   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:53:53 k8s-master kubelet[25301]: I1018 23:53:53.502769   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:53:45 k8s-master kubelet[25301]: E1018 23:53:45.515732   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:53:44 k8s-master kubelet[25301]: E1018 23:53:44.862874   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:53:44 k8s-master kubelet[25301]: E1018 23:53:44.502199   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:53:37 k8s-master kubelet[25301]: ]
Oct 18 23:53:37 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:53:37 k8s-master kubelet[25301]: E1018 23:53:37.408806   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:53:37 k8s-master kubelet[25301]: I1018 23:53:37.408777   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:53:37 k8s-master kubelet[25301]: I1018 23:53:37.408717   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:53:37 k8s-master kubelet[25301]: I1018 23:53:37.408710   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:53:37 k8s-master kubelet[25301]: I1018 23:53:37.408559   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:53:37 k8s-master kubelet[25301]: I1018 23:53:37.408402   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:53:37 k8s-master kubelet[25301]: I1018 23:53:37.408311   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:53:36 k8s-master kubelet[25301]: ]
Oct 18 23:53:36 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:53:36 k8s-master kubelet[25301]: E1018 23:53:36.395716   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:53:36 k8s-master kubelet[25301]: I1018 23:53:36.395591   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:53:36 k8s-master kubelet[25301]: I1018 23:53:36.395425   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:53:36 k8s-master kubelet[25301]: I1018 23:53:36.395399   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:53:36 k8s-master kubelet[25301]: I1018 23:53:36.395146   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:53:36 k8s-master kubelet[25301]: I1018 23:53:36.394885   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:53:36 k8s-master kubelet[25301]: I1018 23:53:36.394783   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:53:35 k8s-master kubelet[25301]: ]
Oct 18 23:53:35 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:53:35 k8s-master kubelet[25301]: E1018 23:53:35.340351   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:53:35 k8s-master kubelet[25301]: I1018 23:53:35.340268   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:53:35 k8s-master kubelet[25301]: I1018 23:53:35.340154   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:53:35 k8s-master kubelet[25301]: I1018 23:53:35.340134   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:53:35 k8s-master kubelet[25301]: I1018 23:53:35.339785   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:53:35 k8s-master kubelet[25301]: E1018 23:53:35.322589   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:53:35 k8s-master kubelet[25301]: W1018 23:53:35.322447   25301 container.go:354] Failed to create summary reader for "/libcontainer_29424_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:53:35 k8s-master kubelet[25301]: W1018 23:53:35.300562   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_29424_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_29424_systemd_test_default.slice: no such file or directory
Oct 18 23:53:35 k8s-master kubelet[25301]: E1018 23:53:35.207217   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:53:35 k8s-master kubelet[25301]: W1018 23:53:35.207086   25301 container.go:354] Failed to create summary reader for "/libcontainer_29421_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:53:35 k8s-master kubelet[25301]: W1018 23:53:35.194820   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_29421_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_29421_systemd_test_default.slice: no such file or directory
Oct 18 23:53:35 k8s-master kubelet[25301]: W1018 23:53:35.157722   25301 container.go:354] Failed to create summary reader for "/libcontainer_29418_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:53:35 k8s-master kubelet[25301]: W1018 23:53:35.154995   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_29418_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_29418_systemd_test_default.slice: no such file or directory
Oct 18 23:53:24 k8s-master kubelet[25301]: E1018 23:53:24.886857   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:53:15 k8s-master kubelet[25301]: E1018 23:53:15.016011   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:53:13 k8s-master kubelet[25301]: E1018 23:53:13.856700   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:53:05 k8s-master kubelet[25301]: I1018 23:53:05.090661   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:52:56 k8s-master kubelet[25301]: E1018 23:52:56.038676   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:52:55 k8s-master kubelet[25301]: E1018 23:52:55.502900   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:52:55 k8s-master kubelet[25301]: I1018 23:52:55.502868   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:52:55 k8s-master kubelet[25301]: I1018 23:52:55.502765   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:52:55 k8s-master kubelet[25301]: I1018 23:52:55.502307   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:52:44 k8s-master kubelet[25301]: E1018 23:52:44.627568   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:52:43 k8s-master kubelet[25301]: E1018 23:52:43.623521   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:52:43 k8s-master kubelet[25301]: E1018 23:52:43.503422   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:52:43 k8s-master kubelet[25301]: I1018 23:52:43.503398   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:52:43 k8s-master kubelet[25301]: I1018 23:52:43.503286   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:52:43 k8s-master kubelet[25301]: I1018 23:52:43.503022   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:52:41 k8s-master kubelet[25301]: E1018 23:52:41.917316   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:52:38 k8s-master kubelet[25301]: E1018 23:52:38.176180   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:52:31 k8s-master kubelet[25301]: E1018 23:52:31.632329   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:52:31 k8s-master kubelet[25301]: I1018 23:52:31.632304   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:52:31 k8s-master kubelet[25301]: I1018 23:52:31.632210   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:52:31 k8s-master kubelet[25301]: I1018 23:52:31.631916   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:52:27 k8s-master kubelet[25301]: E1018 23:52:27.136424   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:52:25 k8s-master kubelet[25301]: E1018 23:52:25.873132   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:52:25 k8s-master kubelet[25301]: I1018 23:52:25.873100   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:52:25 k8s-master kubelet[25301]: I1018 23:52:25.872984   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:52:25 k8s-master kubelet[25301]: I1018 23:52:25.872691   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:52:25 k8s-master kubelet[25301]: E1018 23:52:25.309177   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:52:25 k8s-master kubelet[25301]: W1018 23:52:25.309056   25301 container.go:354] Failed to create summary reader for "/libcontainer_29088_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:52:25 k8s-master kubelet[25301]: W1018 23:52:25.283504   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_29088_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_29088_systemd_test_default.slice: no such file or directory
Oct 18 23:52:25 k8s-master kubelet[25301]: E1018 23:52:25.238999   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:52:25 k8s-master kubelet[25301]: W1018 23:52:25.238886   25301 container.go:354] Failed to create summary reader for "/libcontainer_29085_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:52:25 k8s-master kubelet[25301]: W1018 23:52:25.233337   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_29085_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_29085_systemd_test_default.slice: no such file or directory
Oct 18 23:52:25 k8s-master kubelet[25301]: E1018 23:52:25.169014   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:52:25 k8s-master kubelet[25301]: W1018 23:52:25.168730   25301 container.go:354] Failed to create summary reader for "/libcontainer_29081_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:52:25 k8s-master kubelet[25301]: W1018 23:52:25.156004   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_29081_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_29081_systemd_test_default.slice: no such file or directory
Oct 18 23:51:40 k8s-master kubelet[25301]: E1018 23:51:40.188779   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:51:37 k8s-master kubelet[25301]: E1018 23:51:37.485729   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:51:37 k8s-master kubelet[25301]: E1018 23:51:37.295457   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:51:26 k8s-master kubelet[25301]: W1018 23:51:26.615395   25301 container.go:354] Failed to create summary reader for "/libcontainer_28864_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:51:26 k8s-master kubelet[25301]: W1018 23:51:26.582725   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_28864_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_28864_systemd_test_default.slice: no such file or directory
Oct 18 23:51:26 k8s-master kubelet[25301]: E1018 23:51:26.510051   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:51:26 k8s-master kubelet[25301]: W1018 23:51:26.509857   25301 container.go:354] Failed to create summary reader for "/libcontainer_28850_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:51:26 k8s-master kubelet[25301]: W1018 23:51:26.507638   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_28850_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_28850_systemd_test_default.slice: no such file or directory
Oct 18 23:51:26 k8s-master kubelet[25301]: I1018 23:51:26.393790   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:51:26 k8s-master kubelet[25301]: I1018 23:51:26.393625   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:51:25 k8s-master kubelet[25301]: E1018 23:51:25.006981   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:51:25 k8s-master kubelet[25301]: I1018 23:51:25.006957   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:51:25 k8s-master kubelet[25301]: I1018 23:51:25.006855   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:51:25 k8s-master kubelet[25301]: E1018 23:51:25.006640   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:51:25 k8s-master kubelet[25301]: W1018 23:51:25.006534   25301 container.go:354] Failed to create summary reader for "/libcontainer_28833_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:51:24 k8s-master kubelet[25301]: W1018 23:51:24.922329   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_28833_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_28833_systemd_test_default.slice: no such file or directory
Oct 18 23:51:24 k8s-master kubelet[25301]: E1018 23:51:24.830050   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:51:24 k8s-master kubelet[25301]: W1018 23:51:24.829874   25301 container.go:354] Failed to create summary reader for "/libcontainer_28819_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:51:24 k8s-master kubelet[25301]: W1018 23:51:24.827026   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_28819_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_28819_systemd_test_default.slice: no such file or directory
Oct 18 23:51:24 k8s-master kubelet[25301]: I1018 23:51:24.499138   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:51:24 k8s-master kubelet[25301]: I1018 23:51:24.498959   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:51:24 k8s-master kubelet[25301]: I1018 23:51:24.498904   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:51:11 k8s-master kubelet[25301]: ]
Oct 18 23:51:11 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:51:11 k8s-master kubelet[25301]: E1018 23:51:11.501689   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:51:11 k8s-master kubelet[25301]: I1018 23:51:11.501627   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:51:11 k8s-master kubelet[25301]: I1018 23:51:11.501515   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:51:11 k8s-master kubelet[25301]: I1018 23:51:11.501505   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:51:11 k8s-master kubelet[25301]: I1018 23:51:11.501398   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:51:11 k8s-master kubelet[25301]: I1018 23:51:11.501213   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:51:11 k8s-master kubelet[25301]: I1018 23:51:11.501158   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:50:59 k8s-master kubelet[25301]: ]
Oct 18 23:50:59 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:50:59 k8s-master kubelet[25301]: E1018 23:50:59.502458   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:50:59 k8s-master kubelet[25301]: I1018 23:50:59.502428   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:50:59 k8s-master kubelet[25301]: I1018 23:50:59.502370   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:50:59 k8s-master kubelet[25301]: I1018 23:50:59.502363   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:50:59 k8s-master kubelet[25301]: I1018 23:50:59.502265   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:50:59 k8s-master kubelet[25301]: I1018 23:50:59.502109   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:50:59 k8s-master kubelet[25301]: I1018 23:50:59.502051   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:50:48 k8s-master kubelet[25301]: ]
Oct 18 23:50:48 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:50:48 k8s-master kubelet[25301]: E1018 23:50:48.505100   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:50:48 k8s-master kubelet[25301]: I1018 23:50:48.505048   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:50:48 k8s-master kubelet[25301]: I1018 23:50:48.502545   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:50:48 k8s-master kubelet[25301]: I1018 23:50:48.502535   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:50:48 k8s-master kubelet[25301]: I1018 23:50:48.502426   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:50:48 k8s-master kubelet[25301]: I1018 23:50:48.502263   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:50:48 k8s-master kubelet[25301]: I1018 23:50:48.502209   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:50:36 k8s-master kubelet[25301]: ]
Oct 18 23:50:36 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:50:36 k8s-master kubelet[25301]: E1018 23:50:36.500304   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:50:36 k8s-master kubelet[25301]: I1018 23:50:36.500262   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:50:36 k8s-master kubelet[25301]: I1018 23:50:36.500150   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:50:36 k8s-master kubelet[25301]: I1018 23:50:36.500141   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:50:36 k8s-master kubelet[25301]: I1018 23:50:36.500025   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:50:36 k8s-master kubelet[25301]: I1018 23:50:36.499858   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:50:36 k8s-master kubelet[25301]: I1018 23:50:36.499795   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:50:31 k8s-master kubelet[25301]: E1018 23:50:31.109788   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:50:24 k8s-master kubelet[25301]: ]
Oct 18 23:50:24 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:50:24 k8s-master kubelet[25301]: E1018 23:50:24.499196   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:50:24 k8s-master kubelet[25301]: I1018 23:50:24.499157   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:50:24 k8s-master kubelet[25301]: I1018 23:50:24.499104   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:50:24 k8s-master kubelet[25301]: I1018 23:50:24.499097   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:50:24 k8s-master kubelet[25301]: I1018 23:50:24.498991   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:50:24 k8s-master kubelet[25301]: I1018 23:50:24.498818   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:50:24 k8s-master kubelet[25301]: I1018 23:50:24.498763   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:50:19 k8s-master kubelet[25301]: E1018 23:50:19.472536   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:50:10 k8s-master kubelet[25301]: ]
Oct 18 23:50:10 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:50:10 k8s-master kubelet[25301]: E1018 23:50:10.499177   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:50:10 k8s-master kubelet[25301]: I1018 23:50:10.499147   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:50:10 k8s-master kubelet[25301]: I1018 23:50:10.499093   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:50:10 k8s-master kubelet[25301]: I1018 23:50:10.499086   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:50:10 k8s-master kubelet[25301]: I1018 23:50:10.498986   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:50:10 k8s-master kubelet[25301]: I1018 23:50:10.498824   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:50:10 k8s-master kubelet[25301]: I1018 23:50:10.498762   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:49:57 k8s-master kubelet[25301]: ]
Oct 18 23:49:57 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:49:57 k8s-master kubelet[25301]: E1018 23:49:57.503091   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:49:57 k8s-master kubelet[25301]: I1018 23:49:57.503058   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:49:57 k8s-master kubelet[25301]: I1018 23:49:57.502998   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:49:57 k8s-master kubelet[25301]: I1018 23:49:57.502991   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:49:57 k8s-master kubelet[25301]: I1018 23:49:57.502883   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:49:57 k8s-master kubelet[25301]: I1018 23:49:57.502706   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:49:57 k8s-master kubelet[25301]: I1018 23:49:57.502646   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:49:43 k8s-master kubelet[25301]: ]
Oct 18 23:49:43 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:49:43 k8s-master kubelet[25301]: E1018 23:49:43.503717   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:49:43 k8s-master kubelet[25301]: I1018 23:49:43.503684   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:49:43 k8s-master kubelet[25301]: I1018 23:49:43.503629   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:49:43 k8s-master kubelet[25301]: I1018 23:49:43.503621   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:49:43 k8s-master kubelet[25301]: I1018 23:49:43.503510   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:49:43 k8s-master kubelet[25301]: I1018 23:49:43.503329   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:49:43 k8s-master kubelet[25301]: I1018 23:49:43.503247   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:49:28 k8s-master kubelet[25301]: ]
Oct 18 23:49:28 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:49:28 k8s-master kubelet[25301]: E1018 23:49:28.500183   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:49:28 k8s-master kubelet[25301]: I1018 23:49:28.500140   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:49:28 k8s-master kubelet[25301]: I1018 23:49:28.500040   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:49:28 k8s-master kubelet[25301]: I1018 23:49:28.500034   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:49:28 k8s-master kubelet[25301]: I1018 23:49:28.499923   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:49:28 k8s-master kubelet[25301]: I1018 23:49:28.499767   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:49:28 k8s-master kubelet[25301]: I1018 23:49:28.499707   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:49:13 k8s-master kubelet[25301]: ]
Oct 18 23:49:13 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:49:13 k8s-master kubelet[25301]: E1018 23:49:13.501692   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:49:13 k8s-master kubelet[25301]: I1018 23:49:13.501647   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:49:13 k8s-master kubelet[25301]: I1018 23:49:13.501555   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:49:13 k8s-master kubelet[25301]: I1018 23:49:13.501513   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:49:13 k8s-master kubelet[25301]: I1018 23:49:13.500994   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:49:13 k8s-master kubelet[25301]: I1018 23:49:13.500994   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:49:13 k8s-master kubelet[25301]: I1018 23:49:13.500994   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:49:01 k8s-master kubelet[25301]: ]
Oct 18 23:49:01 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:49:01 k8s-master kubelet[25301]: E1018 23:49:01.501962   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:49:01 k8s-master kubelet[25301]: I1018 23:49:01.501929   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:49:01 k8s-master kubelet[25301]: I1018 23:49:01.501875   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:49:01 k8s-master kubelet[25301]: I1018 23:49:01.501869   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:49:01 k8s-master kubelet[25301]: I1018 23:49:01.501772   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:49:01 k8s-master kubelet[25301]: I1018 23:49:01.501613   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:49:01 k8s-master kubelet[25301]: I1018 23:49:01.501556   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:48:47 k8s-master kubelet[25301]: ]
Oct 18 23:48:47 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:48:47 k8s-master kubelet[25301]: E1018 23:48:47.056142   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:48:47 k8s-master kubelet[25301]: I1018 23:48:47.056113   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:48:47 k8s-master kubelet[25301]: I1018 23:48:47.056061   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:48:47 k8s-master kubelet[25301]: I1018 23:48:47.056054   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:48:47 k8s-master kubelet[25301]: I1018 23:48:47.055960   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:48:47 k8s-master kubelet[25301]: I1018 23:48:47.055788   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:48:47 k8s-master kubelet[25301]: I1018 23:48:47.055729   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:48:46 k8s-master kubelet[25301]: ]
Oct 18 23:48:46 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:48:46 k8s-master kubelet[25301]: E1018 23:48:46.041018   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:48:46 k8s-master kubelet[25301]: I1018 23:48:46.040988   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:48:46 k8s-master kubelet[25301]: I1018 23:48:46.040927   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:48:46 k8s-master kubelet[25301]: I1018 23:48:46.040919   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:48:46 k8s-master kubelet[25301]: I1018 23:48:46.040798   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:48:46 k8s-master kubelet[25301]: I1018 23:48:46.040625   25301 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:48:46 k8s-master kubelet[25301]: I1018 23:48:46.040563   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:48:45 k8s-master kubelet[25301]: ]
Oct 18 23:48:45 k8s-master kubelet[25301]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:48:45 k8s-master kubelet[25301]: E1018 23:48:45.273921   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:48:45 k8s-master kubelet[25301]: I1018 23:48:45.273889   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:48:45 k8s-master kubelet[25301]: I1018 23:48:45.273833   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:48:45 k8s-master kubelet[25301]: I1018 23:48:45.273826   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:48:45 k8s-master kubelet[25301]: I1018 23:48:45.273652   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:48:45 k8s-master kubelet[25301]: E1018 23:48:45.210300   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:48:45 k8s-master kubelet[25301]: W1018 23:48:45.210177   25301 container.go:354] Failed to create summary reader for "/libcontainer_28315_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:48:45 k8s-master kubelet[25301]: W1018 23:48:45.206108   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_28315_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_28315_systemd_test_default.slice: no such file or directory
Oct 18 23:48:45 k8s-master kubelet[25301]: W1018 23:48:45.142071   25301 container.go:354] Failed to create summary reader for "/libcontainer_28309_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:48:45 k8s-master kubelet[25301]: W1018 23:48:45.135473   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_28309_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_28309_systemd_test_default.slice: no such file or directory
Oct 18 23:48:15 k8s-master kubelet[25301]: I1018 23:48:15.089943   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:48:11 k8s-master kubelet[25301]: E1018 23:48:11.502797   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:48:11 k8s-master kubelet[25301]: I1018 23:48:11.502774   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:48:11 k8s-master kubelet[25301]: I1018 23:48:11.502676   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:48:11 k8s-master kubelet[25301]: I1018 23:48:11.502423   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:47:59 k8s-master kubelet[25301]: E1018 23:47:59.502884   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:47:59 k8s-master kubelet[25301]: I1018 23:47:59.502839   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:47:59 k8s-master kubelet[25301]: I1018 23:47:59.502748   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:47:59 k8s-master kubelet[25301]: I1018 23:47:59.502572   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:47:48 k8s-master kubelet[25301]: E1018 23:47:48.499884   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:47:48 k8s-master kubelet[25301]: I1018 23:47:48.499854   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:47:48 k8s-master kubelet[25301]: I1018 23:47:48.499756   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:47:48 k8s-master kubelet[25301]: I1018 23:47:48.498705   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:47:35 k8s-master kubelet[25301]: E1018 23:47:35.513473   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:47:35 k8s-master kubelet[25301]: I1018 23:47:35.513406   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:47:35 k8s-master kubelet[25301]: I1018 23:47:35.513194   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:47:35 k8s-master kubelet[25301]: I1018 23:47:35.512556   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:47:22 k8s-master kubelet[25301]: E1018 23:47:22.499924   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:47:22 k8s-master kubelet[25301]: I1018 23:47:22.499894   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:47:22 k8s-master kubelet[25301]: I1018 23:47:22.499786   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:47:22 k8s-master kubelet[25301]: I1018 23:47:22.499561   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:47:07 k8s-master kubelet[25301]: E1018 23:47:07.501903   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:47:07 k8s-master kubelet[25301]: I1018 23:47:07.501870   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:47:07 k8s-master kubelet[25301]: I1018 23:47:07.501769   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:47:07 k8s-master kubelet[25301]: I1018 23:47:07.501468   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:46:54 k8s-master kubelet[25301]: E1018 23:46:54.500464   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:46:54 k8s-master kubelet[25301]: I1018 23:46:54.500436   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:46:54 k8s-master kubelet[25301]: I1018 23:46:54.500342   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:46:54 k8s-master kubelet[25301]: I1018 23:46:54.500105   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:46:48 k8s-master kubelet[25301]: E1018 23:46:48.413700   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:46:47 k8s-master kubelet[25301]: E1018 23:46:47.727609   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:46:45 k8s-master kubelet[25301]: E1018 23:46:45.630168   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:46:45 k8s-master kubelet[25301]: E1018 23:46:45.483220   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:46:41 k8s-master kubelet[25301]: E1018 23:46:41.627721   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:46:41 k8s-master kubelet[25301]: I1018 23:46:41.627597   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:46:41 k8s-master kubelet[25301]: I1018 23:46:41.627498   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:46:41 k8s-master kubelet[25301]: I1018 23:46:41.627008   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:46:40 k8s-master kubelet[25301]: E1018 23:46:40.520916   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:46:39 k8s-master kubelet[25301]: E1018 23:46:39.153577   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:46:38 k8s-master kubelet[25301]: E1018 23:46:38.325780   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:46:37 k8s-master kubelet[25301]: E1018 23:46:37.117264   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:46:37 k8s-master kubelet[25301]: I1018 23:46:37.117241   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:46:37 k8s-master kubelet[25301]: I1018 23:46:37.117144   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:46:37 k8s-master kubelet[25301]: I1018 23:46:37.116872   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:46:36 k8s-master kubelet[25301]: E1018 23:46:36.097771   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:46:36 k8s-master kubelet[25301]: I1018 23:46:36.097730   25301 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:46:36 k8s-master kubelet[25301]: I1018 23:46:36.097616   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:46:36 k8s-master kubelet[25301]: I1018 23:46:36.097313   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:46:35 k8s-master kubelet[25301]: E1018 23:46:35.724918   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:46:35 k8s-master kubelet[25301]: W1018 23:46:35.710424   25301 container.go:354] Failed to create summary reader for "/libcontainer_28017_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:46:35 k8s-master kubelet[25301]: W1018 23:46:35.616754   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_28017_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_28017_systemd_test_default.slice: no such file or directory
Oct 18 23:46:35 k8s-master kubelet[25301]: E1018 23:46:35.541535   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:46:35 k8s-master kubelet[25301]: W1018 23:46:35.541386   25301 container.go:354] Failed to create summary reader for "/libcontainer_28003_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:46:35 k8s-master kubelet[25301]: W1018 23:46:35.533895   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_28003_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_28003_systemd_test_default.slice: no such file or directory
Oct 18 23:46:35 k8s-master kubelet[25301]: I1018 23:46:35.347237   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:46:35 k8s-master kubelet[25301]: E1018 23:46:35.301799   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:46:35 k8s-master kubelet[25301]: W1018 23:46:35.301677   25301 container.go:354] Failed to create summary reader for "/libcontainer_27994_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:46:35 k8s-master kubelet[25301]: W1018 23:46:35.285449   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_27994_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_27994_systemd_test_default.slice: no such file or directory
Oct 18 23:46:35 k8s-master kubelet[25301]: E1018 23:46:35.259797   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:46:35 k8s-master kubelet[25301]: W1018 23:46:35.259682   25301 container.go:354] Failed to create summary reader for "/libcontainer_27991_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:46:35 k8s-master kubelet[25301]: W1018 23:46:35.244551   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_27991_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_27991_systemd_test_default.slice: no such file or directory
Oct 18 23:46:35 k8s-master kubelet[25301]: E1018 23:46:35.169397   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:46:35 k8s-master kubelet[25301]: W1018 23:46:35.169127   25301 container.go:354] Failed to create summary reader for "/libcontainer_27982_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:46:35 k8s-master kubelet[25301]: W1018 23:46:35.135767   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_27982_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_27982_systemd_test_default.slice: no such file or directory
Oct 18 23:46:30 k8s-master kubelet[25301]: E1018 23:46:30.477804   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:46:25 k8s-master kubelet[25301]: E1018 23:46:25.747002   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:46:20 k8s-master kubelet[25301]: E1018 23:46:20.214357   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:46:20 k8s-master kubelet[25301]: W1018 23:46:20.214175   25301 container.go:354] Failed to create summary reader for "/libcontainer_27937_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:46:20 k8s-master kubelet[25301]: W1018 23:46:20.181617   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_27937_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_27937_systemd_test_default.slice: no such file or directory
Oct 18 23:45:37 k8s-master kubelet[25301]: E1018 23:45:37.915163   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:45:36 k8s-master kubelet[25301]: E1018 23:45:36.696537   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:45:20 k8s-master kubelet[25301]: E1018 23:45:20.010279   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:45:19 k8s-master kubelet[25301]: W1018 23:45:19.976230   25301 container.go:354] Failed to create summary reader for "/libcontainer_27779_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:45:19 k8s-master kubelet[25301]: W1018 23:45:19.894580   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_27779_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_27779_systemd_test_default.slice: no such file or directory
Oct 18 23:45:19 k8s-master kubelet[25301]: E1018 23:45:19.830861   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:45:19 k8s-master kubelet[25301]: W1018 23:45:19.830658   25301 container.go:354] Failed to create summary reader for "/libcontainer_27766_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:45:19 k8s-master kubelet[25301]: W1018 23:45:19.815042   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_27766_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_27766_systemd_test_default.slice: no such file or directory
Oct 18 23:45:19 k8s-master kubelet[25301]: I1018 23:45:19.501849   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:45:19 k8s-master kubelet[25301]: I1018 23:45:19.501623   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:45:04 k8s-master kubelet[25301]: E1018 23:45:04.499101   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:45:04 k8s-master kubelet[25301]: I1018 23:45:04.499073   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:45:04 k8s-master kubelet[25301]: I1018 23:45:04.498979   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:45:04 k8s-master kubelet[25301]: I1018 23:45:04.498762   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:44:52 k8s-master kubelet[25301]: E1018 23:44:52.499358   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:44:52 k8s-master kubelet[25301]: I1018 23:44:52.499325   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:44:52 k8s-master kubelet[25301]: I1018 23:44:52.499228   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:44:52 k8s-master kubelet[25301]: I1018 23:44:52.498947   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:44:44 k8s-master kubelet[25301]: E1018 23:44:44.187950   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:44:43 k8s-master kubelet[25301]: E1018 23:44:43.150257   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:44:42 k8s-master kubelet[25301]: E1018 23:44:42.808648   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:44:42 k8s-master kubelet[25301]: E1018 23:44:42.095096   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:44:40 k8s-master kubelet[25301]: E1018 23:44:40.500621   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:44:40 k8s-master kubelet[25301]: I1018 23:44:40.500592   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:44:40 k8s-master kubelet[25301]: I1018 23:44:40.500487   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:44:40 k8s-master kubelet[25301]: I1018 23:44:40.500229   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:44:38 k8s-master kubelet[25301]: E1018 23:44:38.211162   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:44:35 k8s-master kubelet[25301]: E1018 23:44:35.951622   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:44:27 k8s-master kubelet[25301]: E1018 23:44:27.014894   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:44:27 k8s-master kubelet[25301]: I1018 23:44:27.014870   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:44:27 k8s-master kubelet[25301]: I1018 23:44:27.014776   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:44:27 k8s-master kubelet[25301]: I1018 23:44:27.014525   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:44:25 k8s-master kubelet[25301]: E1018 23:44:25.997217   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:44:25 k8s-master kubelet[25301]: I1018 23:44:25.997193   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:44:25 k8s-master kubelet[25301]: I1018 23:44:25.997093   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:44:25 k8s-master kubelet[25301]: I1018 23:44:25.996842   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:44:25 k8s-master kubelet[25301]: E1018 23:44:25.458328   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:44:25 k8s-master kubelet[25301]: W1018 23:44:25.458111   25301 container.go:354] Failed to create summary reader for "/libcontainer_27620_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:44:25 k8s-master kubelet[25301]: E1018 23:44:25.455926   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:44:25 k8s-master kubelet[25301]: E1018 23:44:25.361136   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:44:25 k8s-master kubelet[25301]: W1018 23:44:25.361012   25301 container.go:354] Failed to create summary reader for "/libcontainer_27607_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:44:25 k8s-master kubelet[25301]: W1018 23:44:25.346580   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_27607_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_27607_systemd_test_default.slice: no such file or directory
Oct 18 23:44:25 k8s-master kubelet[25301]: I1018 23:44:25.252389   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:44:25 k8s-master kubelet[25301]: I1018 23:44:25.252382   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:44:25 k8s-master kubelet[25301]: I1018 23:44:25.252250   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:44:25 k8s-master kubelet[25301]: E1018 23:44:25.199785   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:44:25 k8s-master kubelet[25301]: W1018 23:44:25.199644   25301 container.go:354] Failed to create summary reader for "/libcontainer_27595_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:44:25 k8s-master kubelet[25301]: W1018 23:44:25.174865   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_27595_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_27595_systemd_test_default.slice: no such file or directory
Oct 18 23:43:55 k8s-master kubelet[25301]: I1018 23:43:55.090121   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:43:45 k8s-master kubelet[25301]: E1018 23:43:45.503723   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:43:45 k8s-master kubelet[25301]: I1018 23:43:45.503694   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:43:45 k8s-master kubelet[25301]: I1018 23:43:45.503597   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:43:45 k8s-master kubelet[25301]: I1018 23:43:45.503372   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:43:30 k8s-master kubelet[25301]: E1018 23:43:30.499522   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:43:30 k8s-master kubelet[25301]: I1018 23:43:30.499496   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:43:30 k8s-master kubelet[25301]: I1018 23:43:30.499392   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:43:30 k8s-master kubelet[25301]: I1018 23:43:30.499171   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:43:18 k8s-master kubelet[25301]: E1018 23:43:18.500192   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:43:18 k8s-master kubelet[25301]: I1018 23:43:18.500161   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:43:18 k8s-master kubelet[25301]: I1018 23:43:18.500053   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:43:18 k8s-master kubelet[25301]: I1018 23:43:18.499829   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:43:07 k8s-master kubelet[25301]: E1018 23:43:07.508977   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:43:07 k8s-master kubelet[25301]: I1018 23:43:07.508933   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:43:07 k8s-master kubelet[25301]: I1018 23:43:07.508742   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:43:07 k8s-master kubelet[25301]: I1018 23:43:07.506495   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:42:53 k8s-master kubelet[25301]: E1018 23:42:53.503271   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:42:53 k8s-master kubelet[25301]: I1018 23:42:53.503242   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:42:53 k8s-master kubelet[25301]: I1018 23:42:53.503145   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:42:53 k8s-master kubelet[25301]: I1018 23:42:53.502849   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:42:49 k8s-master kubelet[25301]: E1018 23:42:49.323636   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:42:45 k8s-master kubelet[25301]: E1018 23:42:45.625823   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:42:45 k8s-master kubelet[25301]: E1018 23:42:45.186927   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:42:43 k8s-master kubelet[25301]: E1018 23:42:43.196430   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:42:43 k8s-master kubelet[25301]: E1018 23:42:43.135436   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:42:41 k8s-master kubelet[25301]: E1018 23:42:41.627616   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:42:41 k8s-master kubelet[25301]: I1018 23:42:41.627582   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:42:41 k8s-master kubelet[25301]: I1018 23:42:41.627467   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:42:41 k8s-master kubelet[25301]: I1018 23:42:41.626963   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:42:35 k8s-master kubelet[25301]: E1018 23:42:35.125921   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:42:35 k8s-master kubelet[25301]: I1018 23:42:35.125894   25301 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:42:35 k8s-master kubelet[25301]: I1018 23:42:35.125799   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:42:35 k8s-master kubelet[25301]: I1018 23:42:35.125584   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:42:33 k8s-master kubelet[25301]: E1018 23:42:33.985476   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:42:33 k8s-master kubelet[25301]: W1018 23:42:33.984772   25301 container.go:354] Failed to create summary reader for "/libcontainer_27336_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:42:33 k8s-master kubelet[25301]: W1018 23:42:33.964793   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_27336_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_27336_systemd_test_default.slice: no such file or directory
Oct 18 23:42:33 k8s-master kubelet[25301]: E1018 23:42:33.931179   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:42:33 k8s-master kubelet[25301]: W1018 23:42:33.931055   25301 container.go:354] Failed to create summary reader for "/libcontainer_27330_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:42:33 k8s-master kubelet[25301]: W1018 23:42:33.928477   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_27330_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_27330_systemd_test_default.slice: no such file or directory
Oct 18 23:42:33 k8s-master kubelet[25301]: E1018 23:42:33.125194   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:42:29 k8s-master kubelet[25301]: E1018 23:42:29.820424   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:42:28 k8s-master kubelet[25301]: E1018 23:42:28.238439   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:42:27 k8s-master kubelet[25301]: E1018 23:42:27.724831   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:42:27 k8s-master kubelet[25301]: E1018 23:42:27.031432   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:42:26 k8s-master kubelet[25301]: E1018 23:42:26.358518   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:42:15 k8s-master kubelet[25301]: E1018 23:42:15.523030   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:42:15 k8s-master kubelet[25301]: W1018 23:42:15.522885   25301 container.go:354] Failed to create summary reader for "/libcontainer_27281_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:42:15 k8s-master kubelet[25301]: W1018 23:42:15.469307   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_27281_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_27281_systemd_test_default.slice: no such file or directory
Oct 18 23:42:15 k8s-master kubelet[25301]: I1018 23:42:15.245851   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:42:15 k8s-master kubelet[25301]: E1018 23:42:15.198706   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:42:15 k8s-master kubelet[25301]: W1018 23:42:15.198591   25301 container.go:354] Failed to create summary reader for "/libcontainer_27258_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:42:15 k8s-master kubelet[25301]: W1018 23:42:15.196522   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_27258_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_27258_systemd_test_default.slice: no such file or directory
Oct 18 23:42:15 k8s-master kubelet[25301]: E1018 23:42:15.159052   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:42:15 k8s-master kubelet[25301]: W1018 23:42:15.159052   25301 container.go:354] Failed to create summary reader for "/libcontainer_27255_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:42:15 k8s-master kubelet[25301]: W1018 23:42:15.157124   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_27255_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_27255_systemd_test_default.slice: no such file or directory
Oct 18 23:42:15 k8s-master kubelet[25301]: E1018 23:42:15.139796   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:42:15 k8s-master kubelet[25301]: W1018 23:42:15.139664   25301 container.go:354] Failed to create summary reader for "/libcontainer_27252_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:42:15 k8s-master kubelet[25301]: W1018 23:42:15.124125   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_27252_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_27252_systemd_test_default.slice: no such file or directory
Oct 18 23:41:49 k8s-master kubelet[25301]: E1018 23:41:49.318420   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:41:49 k8s-master kubelet[25301]: E1018 23:41:49.169110   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:41:46 k8s-master kubelet[25301]: E1018 23:41:46.088592   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:41:44 k8s-master kubelet[25301]: E1018 23:41:44.722720   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:41:40 k8s-master kubelet[25301]: E1018 23:41:40.241758   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:41:38 k8s-master kubelet[25301]: E1018 23:41:38.892644   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:41:36 k8s-master kubelet[25301]: E1018 23:41:36.987233   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:41:33 k8s-master kubelet[25301]: E1018 23:41:33.777046   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:41:33 k8s-master kubelet[25301]: W1018 23:41:33.776932   25301 container.go:354] Failed to create summary reader for "/libcontainer_27131_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:41:33 k8s-master kubelet[25301]: E1018 23:41:33.630914   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:41:33 k8s-master kubelet[25301]: W1018 23:41:33.630908   25301 container.go:354] Failed to create summary reader for "/libcontainer_27117_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:41:33 k8s-master kubelet[25301]: W1018 23:41:33.617551   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_27117_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_27117_systemd_test_default.slice: no such file or directory
Oct 18 23:41:33 k8s-master kubelet[25301]: I1018 23:41:33.503208   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:41:33 k8s-master kubelet[25301]: I1018 23:41:33.502956   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:41:32 k8s-master kubelet[25301]: E1018 23:41:32.174127   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:41:32 k8s-master kubelet[25301]: E1018 23:41:32.043604   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:41:31 k8s-master kubelet[25301]: E1018 23:41:31.855382   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:41:24 k8s-master kubelet[25301]: E1018 23:41:24.745210   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:41:21 k8s-master kubelet[25301]: E1018 23:41:21.519331   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:41:20 k8s-master kubelet[25301]: E1018 23:41:20.499232   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:41:20 k8s-master kubelet[25301]: I1018 23:41:20.499204   25301 kuberuntime_manager.go:748] Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:41:20 k8s-master kubelet[25301]: I1018 23:41:20.499098   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:41:20 k8s-master kubelet[25301]: I1018 23:41:20.498842   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:41:20 k8s-master kubelet[25301]: E1018 23:41:20.492219   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:41:16 k8s-master kubelet[25301]: E1018 23:41:16.305012   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:41:14 k8s-master kubelet[25301]: E1018 23:41:14.078863   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:41:06 k8s-master kubelet[25301]: E1018 23:41:06.499245   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:41:06 k8s-master kubelet[25301]: I1018 23:41:06.499245   25301 kuberuntime_manager.go:748] Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:41:06 k8s-master kubelet[25301]: I1018 23:41:06.499155   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:41:06 k8s-master kubelet[25301]: I1018 23:41:06.498895   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:41:06 k8s-master kubelet[25301]: E1018 23:41:06.047433   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:41:05 k8s-master kubelet[25301]: E1018 23:41:05.899920   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:41:05 k8s-master kubelet[25301]: E1018 23:41:05.488768   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:41:02 k8s-master kubelet[25301]: E1018 23:41:02.539021   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:41:02 k8s-master kubelet[25301]: E1018 23:41:02.117633   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:41:00 k8s-master kubelet[25301]: E1018 23:41:00.602746   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:41:00 k8s-master kubelet[25301]: E1018 23:41:00.077450   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:52 k8s-master kubelet[25301]: E1018 23:40:52.499836   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:40:52 k8s-master kubelet[25301]: I1018 23:40:52.499805   25301 kuberuntime_manager.go:748] Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:40:52 k8s-master kubelet[25301]: I1018 23:40:52.499682   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:40:52 k8s-master kubelet[25301]: I1018 23:40:52.498934   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:40:51 k8s-master kubelet[25301]: E1018 23:40:51.445321   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:51 k8s-master kubelet[25301]: W1018 23:40:51.445219   25301 container.go:354] Failed to create summary reader for "/libcontainer_27009_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:40:51 k8s-master kubelet[25301]: W1018 23:40:51.383267   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_27009_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_27009_systemd_test_default.slice: no such file or directory
Oct 18 23:40:51 k8s-master kubelet[25301]: I1018 23:40:51.075249   25301 kuberuntime_manager.go:499] Container {Name:kube-flannel Image:quay.io/coreos/flannel:v0.9.0-amd64 Command:[/opt/bin/flanneld --ip-masq --kube-subnet-mgr] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[{Name:POD_NAME Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {Name:POD_NAMESPACE Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}}] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:run ReadOnly:false MountPath:/run SubPath: MountPropagation:<nil>} {Name:flannel-cfg ReadOnly:false MountPath:/etc/kube-flannel/ SubPath: MountPropagation:<nil>} {Name:flannel-token-2sfpp ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:40:49 k8s-master kubelet[25301]: E1018 23:40:49.852675   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:49 k8s-master kubelet[25301]: W1018 23:40:49.852539   25301 container.go:354] Failed to create summary reader for "/libcontainer_26986_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:40:49 k8s-master kubelet[25301]: W1018 23:40:49.851030   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_26986_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_26986_systemd_test_default.slice: no such file or directory
Oct 18 23:40:49 k8s-master kubelet[25301]: E1018 23:40:49.820091   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:49 k8s-master kubelet[25301]: W1018 23:40:49.819962   25301 container.go:354] Failed to create summary reader for "/libcontainer_26983_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:40:49 k8s-master kubelet[25301]: W1018 23:40:49.807275   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_26983_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_26983_systemd_test_default.slice: no such file or directory
Oct 18 23:40:49 k8s-master kubelet[25301]: E1018 23:40:49.721733   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:49 k8s-master kubelet[25301]: W1018 23:40:49.721479   25301 container.go:354] Failed to create summary reader for "/libcontainer_26960_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:40:49 k8s-master kubelet[25301]: E1018 23:40:49.656097   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:49 k8s-master kubelet[25301]: W1018 23:40:49.500428   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_26960_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_26960_systemd_test_default.slice: no such file or directory
Oct 18 23:40:49 k8s-master kubelet[25301]: I1018 23:40:49.407557   25301 container.go:471] Failed to update stats for container "/libcontainer_26947_systemd_test_default.slice": failed to parse memory.usage_in_bytes - read /sys/fs/cgroup/memory/libcontainer_26947_systemd_test_default.slice/memory.usage_in_bytes: no such device, continuing to push stats
Oct 18 23:40:49 k8s-master kubelet[25301]: E1018 23:40:49.208685   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:49 k8s-master kubelet[25301]: W1018 23:40:49.208576   25301 container.go:354] Failed to create summary reader for "/libcontainer_26932_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:40:49 k8s-master kubelet[25301]: W1018 23:40:49.177830   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_26932_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_26932_systemd_test_default.slice: no such file or directory
Oct 18 23:40:49 k8s-master kubelet[25301]: E1018 23:40:49.111204   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:49 k8s-master kubelet[25301]: W1018 23:40:49.110995   25301 container.go:354] Failed to create summary reader for "/libcontainer_26920_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:40:49 k8s-master kubelet[25301]: W1018 23:40:49.107547   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_26920_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_26920_systemd_test_default.slice: no such file or directory
Oct 18 23:40:48 k8s-master kubelet[25301]: E1018 23:40:48.862900   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:48 k8s-master kubelet[25301]: W1018 23:40:48.862492   25301 container.go:354] Failed to create summary reader for "/system.slice/run-26912.scope": none of the resources are being tracked.
Oct 18 23:40:48 k8s-master kubelet[25301]: I1018 23:40:48.721149   25301 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "flannel-token-2sfpp" (UniqueName: "kubernetes.io/secret/b6af75e4-b41a-11e7-8c24-08002782873b-flannel-token-2sfpp") pod "kube-flannel-ds-n747n" (UID: "b6af75e4-b41a-11e7-8c24-08002782873b")
Oct 18 23:40:48 k8s-master kubelet[25301]: I1018 23:40:48.721112   25301 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "run" (UniqueName: "kubernetes.io/host-path/b6af75e4-b41a-11e7-8c24-08002782873b-run") pod "kube-flannel-ds-n747n" (UID: "b6af75e4-b41a-11e7-8c24-08002782873b")
Oct 18 23:40:48 k8s-master kubelet[25301]: I1018 23:40:48.721086   25301 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "flannel-cfg" (UniqueName: "kubernetes.io/configmap/b6af75e4-b41a-11e7-8c24-08002782873b-flannel-cfg") pod "kube-flannel-ds-n747n" (UID: "b6af75e4-b41a-11e7-8c24-08002782873b")
Oct 18 23:40:48 k8s-master kubelet[25301]: I1018 23:40:48.719703   25301 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "cni" (UniqueName: "kubernetes.io/host-path/b6af75e4-b41a-11e7-8c24-08002782873b-cni") pod "kube-flannel-ds-n747n" (UID: "b6af75e4-b41a-11e7-8c24-08002782873b")
Oct 18 23:40:41 k8s-master kubelet[25301]: E1018 23:40:41.496439   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:40 k8s-master kubelet[25301]: E1018 23:40:40.388584   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:37 k8s-master kubelet[25301]: E1018 23:40:37.686631   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:37 k8s-master kubelet[25301]: E1018 23:40:37.505664   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:40:37 k8s-master kubelet[25301]: I1018 23:40:37.505636   25301 kuberuntime_manager.go:748] Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:40:37 k8s-master kubelet[25301]: I1018 23:40:37.505538   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:40:37 k8s-master kubelet[25301]: I1018 23:40:37.505310   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:40:32 k8s-master kubelet[25301]: E1018 23:40:32.948912   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:31 k8s-master kubelet[25301]: E1018 23:40:31.211976   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:30 k8s-master kubelet[25301]: E1018 23:40:30.020331   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:27 k8s-master kubelet[25301]: E1018 23:40:27.546442   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:25 k8s-master kubelet[25301]: E1018 23:40:25.917211   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:24 k8s-master kubelet[25301]: E1018 23:40:24.499135   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:40:24 k8s-master kubelet[25301]: I1018 23:40:24.499110   25301 kuberuntime_manager.go:748] Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:40:24 k8s-master kubelet[25301]: I1018 23:40:24.499017   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:40:24 k8s-master kubelet[25301]: I1018 23:40:24.498783   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:40:24 k8s-master kubelet[25301]: E1018 23:40:24.354145   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:23 k8s-master kubelet[25301]: E1018 23:40:23.766986   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:21 k8s-master kubelet[25301]: E1018 23:40:21.238348   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:19 k8s-master kubelet[25301]: E1018 23:40:19.637144   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:18 k8s-master kubelet[25301]: E1018 23:40:18.901091   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:17 k8s-master kubelet[25301]: E1018 23:40:17.297050   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:16 k8s-master kubelet[25301]: E1018 23:40:16.103098   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:15 k8s-master kubelet[25301]: E1018 23:40:15.699474   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:15 k8s-master kubelet[25301]: E1018 23:40:15.407099   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:11 k8s-master kubelet[25301]: E1018 23:40:11.627395   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:40:11 k8s-master kubelet[25301]: I1018 23:40:11.627371   25301 kuberuntime_manager.go:748] Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:40:11 k8s-master kubelet[25301]: I1018 23:40:11.627278   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:40:11 k8s-master kubelet[25301]: I1018 23:40:11.627048   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:40:08 k8s-master kubelet[25301]: E1018 23:40:08.508387   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:40:08 k8s-master kubelet[25301]: I1018 23:40:08.508358   25301 kuberuntime_manager.go:748] Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:40:08 k8s-master kubelet[25301]: I1018 23:40:08.508261   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:40:08 k8s-master kubelet[25301]: I1018 23:40:08.508023   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:40:08 k8s-master kubelet[25301]: E1018 23:40:08.019729   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:08 k8s-master kubelet[25301]: W1018 23:40:08.019011   25301 container.go:354] Failed to create summary reader for "/libcontainer_26843_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:40:08 k8s-master kubelet[25301]: W1018 23:40:08.016751   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_26843_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_26843_systemd_test_default.slice: no such file or directory
Oct 18 23:40:07 k8s-master kubelet[25301]: W1018 23:40:07.989293   25301 container.go:354] Failed to create summary reader for "/libcontainer_26839_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:40:07 k8s-master kubelet[25301]: W1018 23:40:07.978907   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_26839_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_26839_systemd_test_default.slice: no such file or directory
Oct 18 23:40:05 k8s-master kubelet[25301]: E1018 23:40:05.706633   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:40:05 k8s-master kubelet[25301]: W1018 23:40:05.706519   25301 container.go:354] Failed to create summary reader for "/libcontainer_26821_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:40:05 k8s-master kubelet[25301]: W1018 23:40:05.627833   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_26821_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_26821_systemd_test_default.slice: no such file or directory
Oct 18 23:40:05 k8s-master kubelet[25301]: W1018 23:40:05.506121   25301 container.go:354] Failed to create summary reader for "/libcontainer_26807_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:40:05 k8s-master kubelet[25301]: W1018 23:40:05.484051   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_26807_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_26807_systemd_test_default.slice: no such file or directory
Oct 18 23:40:05 k8s-master kubelet[25301]: I1018 23:40:05.368531   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:39:47 k8s-master kubelet[25301]: E1018 23:39:47.968196   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:39:41 k8s-master kubelet[25301]: E1018 23:39:41.652887   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:39:37 k8s-master kubelet[25301]: E1018 23:39:37.785505   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:39:25 k8s-master kubelet[25301]: E1018 23:39:25.843843   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:39:18 k8s-master kubelet[25301]: E1018 23:39:18.022970   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:39:07 k8s-master kubelet[25301]: E1018 23:39:07.900513   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:39:07 k8s-master kubelet[25301]: W1018 23:39:07.900411   25301 container.go:354] Failed to create summary reader for "/libcontainer_26553_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:39:07 k8s-master kubelet[25301]: W1018 23:39:07.812144   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_26553_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_26553_systemd_test_default.slice: no such file or directory
Oct 18 23:39:07 k8s-master kubelet[25301]: E1018 23:39:07.759673   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:39:07 k8s-master kubelet[25301]: W1018 23:39:07.759470   25301 container.go:354] Failed to create summary reader for "/libcontainer_26539_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:39:07 k8s-master kubelet[25301]: W1018 23:39:07.736806   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_26539_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_26539_systemd_test_default.slice: no such file or directory
Oct 18 23:39:07 k8s-master kubelet[25301]: I1018 23:39:07.503149   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:39:07 k8s-master kubelet[25301]: I1018 23:39:07.502919   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:38:52 k8s-master kubelet[25301]: E1018 23:38:52.499429   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:38:52 k8s-master kubelet[25301]: I1018 23:38:52.499429   25301 kuberuntime_manager.go:748] Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:38:52 k8s-master kubelet[25301]: I1018 23:38:52.499317   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:38:52 k8s-master kubelet[25301]: I1018 23:38:52.499076   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:38:48 k8s-master kubelet[25301]: E1018 23:38:48.275357   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:38:45 k8s-master kubelet[25301]: E1018 23:38:45.576037   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:38:43 k8s-master kubelet[25301]: E1018 23:38:43.597345   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:38:41 k8s-master kubelet[25301]: E1018 23:38:41.745419   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:38:40 k8s-master kubelet[25301]: E1018 23:38:40.220759   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:38:36 k8s-master kubelet[25301]: E1018 23:38:36.791878   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:38:36 k8s-master kubelet[25301]: E1018 23:38:36.500466   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:38:36 k8s-master kubelet[25301]: I1018 23:38:36.500439   25301 kuberuntime_manager.go:748] Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:38:36 k8s-master kubelet[25301]: I1018 23:38:36.500342   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:38:36 k8s-master kubelet[25301]: I1018 23:38:36.500089   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:38:35 k8s-master kubelet[25301]: E1018 23:38:35.628087   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:38:34 k8s-master kubelet[25301]: E1018 23:38:34.718851   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:38:34 k8s-master kubelet[25301]: E1018 23:38:34.669933   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:38:31 k8s-master kubelet[25301]: E1018 23:38:31.050759   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:38:30 k8s-master kubelet[25301]: E1018 23:38:30.093615   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:38:29 k8s-master kubelet[25301]: E1018 23:38:29.208827   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:38:27 k8s-master kubelet[25301]: E1018 23:38:27.844097   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:38:27 k8s-master kubelet[25301]: E1018 23:38:27.122979   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:38:24 k8s-master kubelet[25301]: E1018 23:38:24.169638   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:38:21 k8s-master kubelet[25301]: E1018 23:38:21.627177   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:38:21 k8s-master kubelet[25301]: I1018 23:38:21.627151   25301 kuberuntime_manager.go:748] Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:38:21 k8s-master kubelet[25301]: I1018 23:38:21.627054   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:38:21 k8s-master kubelet[25301]: I1018 23:38:21.626809   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:38:17 k8s-master kubelet[25301]: E1018 23:38:17.704745   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:38:17 k8s-master kubelet[25301]: E1018 23:38:17.613699   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:38:17 k8s-master kubelet[25301]: I1018 23:38:17.613672   25301 kuberuntime_manager.go:748] Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:38:17 k8s-master kubelet[25301]: I1018 23:38:17.613574   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:38:17 k8s-master kubelet[25301]: I1018 23:38:17.613352   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:38:16 k8s-master kubelet[25301]: E1018 23:38:16.980225   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:38:16 k8s-master kubelet[25301]: W1018 23:38:16.979532   25301 container.go:354] Failed to create summary reader for "/libcontainer_26447_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:38:16 k8s-master kubelet[25301]: W1018 23:38:16.977243   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_26447_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_26447_systemd_test_default.slice: no such file or directory
Oct 18 23:38:13 k8s-master kubelet[25301]: E1018 23:38:13.480568   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:38:11 k8s-master kubelet[25301]: E1018 23:38:11.976625   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:38:10 k8s-master kubelet[25301]: E1018 23:38:10.101889   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:38:07 k8s-master kubelet[25301]: E1018 23:38:07.879825   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:38:05 k8s-master kubelet[25301]: E1018 23:38:05.909683   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:37:59 k8s-master kubelet[25301]: E1018 23:37:59.218339   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:37:55 k8s-master kubelet[25301]: E1018 23:37:55.572563   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:37:55 k8s-master kubelet[25301]: W1018 23:37:55.572448   25301 container.go:354] Failed to create summary reader for "/libcontainer_26399_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:37:55 k8s-master kubelet[25301]: W1018 23:37:55.496758   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_26399_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_26399_systemd_test_default.slice: no such file or directory
Oct 18 23:37:55 k8s-master kubelet[25301]: E1018 23:37:55.439658   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:37:55 k8s-master kubelet[25301]: W1018 23:37:55.439515   25301 container.go:354] Failed to create summary reader for "/libcontainer_26385_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:37:55 k8s-master kubelet[25301]: W1018 23:37:55.423500   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_26385_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_26385_systemd_test_default.slice: no such file or directory
Oct 18 23:37:55 k8s-master kubelet[25301]: I1018 23:37:55.285016   25301 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:37:55 k8s-master kubelet[25301]: E1018 23:37:55.252398   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:37:55 k8s-master kubelet[25301]: W1018 23:37:55.252232   25301 container.go:354] Failed to create summary reader for "/libcontainer_26366_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:37:55 k8s-master kubelet[25301]: W1018 23:37:55.246898   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_26366_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_26366_systemd_test_default.slice: no such file or directory
Oct 18 23:37:55 k8s-master kubelet[25301]: E1018 23:37:55.131073   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:37:55 k8s-master kubelet[25301]: W1018 23:37:55.130594   25301 container.go:354] Failed to create summary reader for "/libcontainer_26360_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:37:55 k8s-master kubelet[25301]: W1018 23:37:55.128166   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_26360_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_26360_systemd_test_default.slice: no such file or directory
Oct 18 23:37:44 k8s-master kubelet[25301]: E1018 23:37:44.774254   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:37:32 k8s-master kubelet[25301]: E1018 23:37:32.492437   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:37:28 k8s-master kubelet[25301]: E1018 23:37:28.909727   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:37:16 k8s-master kubelet[25301]: E1018 23:37:16.809461   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:37:16 k8s-master kubelet[25301]: W1018 23:37:16.809365   25301 container.go:354] Failed to create summary reader for "/libcontainer_26302_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:37:16 k8s-master kubelet[25301]: I1018 23:37:16.499359   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:37:16 k8s-master kubelet[25301]: I1018 23:37:16.499091   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:37:03 k8s-master kubelet[25301]: E1018 23:37:03.503406   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:37:03 k8s-master kubelet[25301]: I1018 23:37:03.503381   25301 kuberuntime_manager.go:748] Back-off 20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:37:03 k8s-master kubelet[25301]: I1018 23:37:03.503274   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:37:03 k8s-master kubelet[25301]: I1018 23:37:03.503029   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:36:51 k8s-master kubelet[25301]: E1018 23:36:51.627349   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:36:51 k8s-master kubelet[25301]: I1018 23:36:51.627324   25301 kuberuntime_manager.go:748] Back-off 20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:36:51 k8s-master kubelet[25301]: I1018 23:36:51.627201   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:36:51 k8s-master kubelet[25301]: I1018 23:36:51.626961   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:36:47 k8s-master kubelet[25301]: E1018 23:36:47.716560   25301 pod_workers.go:182] Error syncing pod b2593c3c-b419-11e7-8c24-08002782873b ("kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:36:47 k8s-master kubelet[25301]: I1018 23:36:47.716473   25301 kuberuntime_manager.go:748] Back-off 20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)
Oct 18 23:36:47 k8s-master kubelet[25301]: I1018 23:36:47.716316   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:36:47 k8s-master kubelet[25301]: I1018 23:36:47.715490   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:36:46 k8s-master kubelet[25301]: E1018 23:36:46.740290   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:36:46 k8s-master kubelet[25301]: W1018 23:36:46.740175   25301 container.go:354] Failed to create summary reader for "/libcontainer_26230_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:36:46 k8s-master kubelet[25301]: W1018 23:36:46.726124   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_26230_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_26230_systemd_test_default.slice: no such file or directory
Oct 18 23:36:46 k8s-master kubelet[25301]: E1018 23:36:46.690996   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:36:46 k8s-master kubelet[25301]: W1018 23:36:46.690855   25301 container.go:354] Failed to create summary reader for "/libcontainer_26227_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:36:46 k8s-master kubelet[25301]: W1018 23:36:46.686912   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_26227_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_26227_systemd_test_default.slice: no such file or directory
Oct 18 23:35:49 k8s-master kubelet[25301]: E1018 23:35:49.159426   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:35:46 k8s-master kubelet[25301]: E1018 23:35:46.594609   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:35:46 k8s-master kubelet[25301]: E1018 23:35:46.548890   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:35:46 k8s-master kubelet[25301]: W1018 23:35:46.548794   25301 container.go:354] Failed to create summary reader for "/libcontainer_26129_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:35:46 k8s-master kubelet[25301]: I1018 23:35:46.330683   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:35:46 k8s-master kubelet[25301]: I1018 23:35:46.330432   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:35:45 k8s-master kubelet[25301]: E1018 23:35:45.659216   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:35:45 k8s-master kubelet[25301]: E1018 23:35:45.572919   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:35:45 k8s-master kubelet[25301]: W1018 23:35:45.572828   25301 container.go:354] Failed to create summary reader for "/libcontainer_26097_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:35:45 k8s-master kubelet[25301]: E1018 23:35:45.215424   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:35:45 k8s-master kubelet[25301]: W1018 23:35:45.215311   25301 container.go:354] Failed to create summary reader for "/libcontainer_26076_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:35:45 k8s-master kubelet[25301]: W1018 23:35:45.197589   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_26076_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_26076_systemd_test_default.slice: no such file or directory
Oct 18 23:35:45 k8s-master kubelet[25301]: E1018 23:35:45.133882   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:35:45 k8s-master kubelet[25301]: W1018 23:35:45.133732   25301 container.go:354] Failed to create summary reader for "/libcontainer_26070_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:35:45 k8s-master kubelet[25301]: W1018 23:35:45.127246   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_26070_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_26070_systemd_test_default.slice: no such file or directory
Oct 18 23:35:37 k8s-master kubelet[25301]: E1018 23:35:37.902064   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:35:35 k8s-master kubelet[25301]: E1018 23:35:35.595176   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:35:35 k8s-master kubelet[25301]: W1018 23:35:35.594960   25301 container.go:354] Failed to create summary reader for "/libcontainer_26066_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:35:35 k8s-master kubelet[25301]: W1018 23:35:35.589787   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_26066_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_26066_systemd_test_default.slice: no such file or directory
Oct 18 23:35:35 k8s-master kubelet[25301]: E1018 23:35:35.551759   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:35:35 k8s-master kubelet[25301]: W1018 23:35:35.551599   25301 container.go:354] Failed to create summary reader for "/libcontainer_26063_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:35:35 k8s-master kubelet[25301]: W1018 23:35:35.548414   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_26063_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_26063_systemd_test_default.slice: no such file or directory
Oct 18 23:34:47 k8s-master kubelet[25301]: E1018 23:34:47.788635   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:34:44 k8s-master kubelet[25301]: E1018 23:34:44.200163   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:34:39 k8s-master kubelet[25301]: E1018 23:34:39.923874   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:34:35 k8s-master kubelet[25301]: E1018 23:34:35.385161   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:34:35 k8s-master kubelet[25301]: W1018 23:34:35.385053   25301 container.go:354] Failed to create summary reader for "/libcontainer_25963_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:34:35 k8s-master kubelet[25301]: W1018 23:34:35.348330   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_25963_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_25963_systemd_test_default.slice: no such file or directory
Oct 18 23:34:35 k8s-master kubelet[25301]: E1018 23:34:35.291869   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:34:35 k8s-master kubelet[25301]: W1018 23:34:35.291693   25301 container.go:354] Failed to create summary reader for "/libcontainer_25949_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:34:35 k8s-master kubelet[25301]: W1018 23:34:35.273629   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_25949_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_25949_systemd_test_default.slice: no such file or directory
Oct 18 23:34:34 k8s-master kubelet[25301]: I1018 23:34:34.850213   25301 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-l2xsw_kube-system(b2593c3c-b419-11e7-8c24-08002782873b)"
Oct 18 23:34:34 k8s-master kubelet[25301]: I1018 23:34:34.849942   25301 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-xdjjs ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:34:33 k8s-master kubelet[25301]: E1018 23:34:33.692255   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:34:33 k8s-master kubelet[25301]: E1018 23:34:33.692191   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:34:33 k8s-master kubelet[25301]: W1018 23:34:33.692095   25301 container.go:354] Failed to create summary reader for "/libcontainer_25917_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:34:33 k8s-master kubelet[25301]: W1018 23:34:33.691938   25301 container.go:354] Failed to create summary reader for "/libcontainer_25914_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:34:33 k8s-master kubelet[25301]: W1018 23:34:33.690293   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_25917_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_25917_systemd_test_default.slice: no such file or directory
Oct 18 23:34:33 k8s-master kubelet[25301]: W1018 23:34:33.650788   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_25914_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_25914_systemd_test_default.slice: no such file or directory
Oct 18 23:33:49 k8s-master kubelet[25301]: E1018 23:33:49.169344   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:49 k8s-master kubelet[25301]: E1018 23:33:49.023345   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:48 k8s-master kubelet[25301]: E1018 23:33:48.409551   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:48 k8s-master kubelet[25301]: E1018 23:33:48.062132   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:47 k8s-master kubelet[25301]: E1018 23:33:47.795547   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:47 k8s-master kubelet[25301]: E1018 23:33:47.744309   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:46 k8s-master kubelet[25301]: E1018 23:33:46.712660   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:46 k8s-master kubelet[25301]: E1018 23:33:46.496574   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:45 k8s-master kubelet[25301]: E1018 23:33:45.754878   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:44 k8s-master kubelet[25301]: E1018 23:33:44.952386   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:42 k8s-master kubelet[25301]: E1018 23:33:42.025112   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:37 k8s-master kubelet[25301]: E1018 23:33:37.936601   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:37 k8s-master kubelet[25301]: E1018 23:33:37.706855   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:35 k8s-master kubelet[25301]: E1018 23:33:35.992489   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:35 k8s-master kubelet[25301]: E1018 23:33:35.698902   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:34 k8s-master kubelet[25301]: E1018 23:33:34.558843   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:34 k8s-master kubelet[25301]: W1018 23:33:34.541062   25301 container.go:354] Failed to create summary reader for "/libcontainer_25820_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:33:34 k8s-master kubelet[25301]: E1018 23:33:34.323647   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:34 k8s-master kubelet[25301]: W1018 23:33:34.323538   25301 container.go:354] Failed to create summary reader for "/libcontainer_25794_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:33:34 k8s-master kubelet[25301]: W1018 23:33:34.323406   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_25794_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_25794_systemd_test_default.slice: no such file or directory
Oct 18 23:33:34 k8s-master kubelet[25301]: E1018 23:33:34.157089   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:34 k8s-master kubelet[25301]: W1018 23:33:34.135763   25301 kuberuntime_container.go:191] Non-root verification doesn't support non-numeric user (nobody)
Oct 18 23:33:34 k8s-master kubelet[25301]: E1018 23:33:34.102683   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:34 k8s-master kubelet[25301]: W1018 23:33:34.102592   25301 container.go:354] Failed to create summary reader for "/libcontainer_25754_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:33:33 k8s-master kubelet[25301]: W1018 23:33:33.950838   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_25754_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_25754_systemd_test_default.slice: no such file or directory
Oct 18 23:33:33 k8s-master kubelet[25301]: E1018 23:33:33.665699   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:33 k8s-master kubelet[25301]: W1018 23:33:33.650118   25301 container.go:354] Failed to create summary reader for "/libcontainer_25692_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:33:33 k8s-master kubelet[25301]: E1018 23:33:33.368044   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:33 k8s-master kubelet[25301]: W1018 23:33:33.297625   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_25692_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_25692_systemd_test_default.slice: no such file or directory
Oct 18 23:33:33 k8s-master kubelet[25301]: E1018 23:33:33.053691   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:33 k8s-master kubelet[25301]: W1018 23:33:33.053556   25301 container.go:354] Failed to create summary reader for "/libcontainer_25662_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:33:33 k8s-master kubelet[25301]: W1018 23:33:33.045235   25301 pod_container_deletor.go:77] Container "b3d4007b7a1975f340eee2f9610ad3e0f556787989e3ed7d69c4a3e212682d6c" not found in pod's containers
Oct 18 23:33:33 k8s-master kubelet[25301]: E1018 23:33:33.044677   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:33 k8s-master kubelet[25301]: W1018 23:33:33.028317   25301 container.go:354] Failed to create summary reader for "/libcontainer_25646_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:33:32 k8s-master kubelet[25301]: W1018 23:33:32.937486   25301 pod_container_deletor.go:77] Container "0aa57a2b259f35efb69f184207a304c3083fa863be74defafb752caba73c44dd" not found in pod's containers
Oct 18 23:33:32 k8s-master kubelet[25301]: I1018 23:33:32.544522   25301 kubelet_network.go:276] Setting Pod CIDR:  -> 10.244.0.0/24
Oct 18 23:33:32 k8s-master kubelet[25301]: I1018 23:33:32.544400   25301 docker_service.go:306] docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}
Oct 18 23:33:32 k8s-master kubelet[25301]: I1018 23:33:32.544400   25301 kuberuntime_manager.go:898] updating runtime config through cri with podcidr 10.244.0.0/24
Oct 18 23:33:32 k8s-master kubelet[25301]: E1018 23:33:32.514746   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:32 k8s-master kubelet[25301]: W1018 23:33:32.514614   25301 container.go:354] Failed to create summary reader for "/libcontainer_25617_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:33:32 k8s-master kubelet[25301]: W1018 23:33:32.506609   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_25617_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_25617_systemd_test_default.slice: no such file or directory
Oct 18 23:33:32 k8s-master kubelet[25301]: E1018 23:33:32.217390   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:31 k8s-master kubelet[25301]: I1018 23:33:31.971632   25301 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-proxy-token-jx5p4" (UniqueName: "kubernetes.io/secret/b2564e2e-b419-11e7-8c24-08002782873b-kube-proxy-token-jx5p4") pod "kube-proxy-rrmr6" (UID: "b2564e2e-b419-11e7-8c24-08002782873b")
Oct 18 23:33:31 k8s-master kubelet[25301]: I1018 23:33:31.971537   25301 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-dns-config" (UniqueName: "kubernetes.io/configmap/b2593c3c-b419-11e7-8c24-08002782873b-kube-dns-config") pod "kube-dns-545bc4bfd4-l2xsw" (UID: "b2593c3c-b419-11e7-8c24-08002782873b")
Oct 18 23:33:31 k8s-master kubelet[25301]: I1018 23:33:31.970749   25301 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "xtables-lock" (UniqueName: "kubernetes.io/host-path/b2564e2e-b419-11e7-8c24-08002782873b-xtables-lock") pod "kube-proxy-rrmr6" (UID: "b2564e2e-b419-11e7-8c24-08002782873b")
Oct 18 23:33:31 k8s-master kubelet[25301]: I1018 23:33:31.970664   25301 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-proxy" (UniqueName: "kubernetes.io/configmap/b2564e2e-b419-11e7-8c24-08002782873b-kube-proxy") pod "kube-proxy-rrmr6" (UID: "b2564e2e-b419-11e7-8c24-08002782873b")
Oct 18 23:33:31 k8s-master kubelet[25301]: I1018 23:33:31.970503   25301 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-dns-token-xdjjs" (UniqueName: "kubernetes.io/secret/b2593c3c-b419-11e7-8c24-08002782873b-kube-dns-token-xdjjs") pod "kube-dns-545bc4bfd4-l2xsw" (UID: "b2593c3c-b419-11e7-8c24-08002782873b")
Oct 18 23:33:31 k8s-master kubelet[25301]: E1018 23:33:31.242644   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:31 k8s-master kubelet[25301]: E1018 23:33:31.097259   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:29 k8s-master kubelet[25301]: E1018 23:33:29.374520   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:27 k8s-master kubelet[25301]: E1018 23:33:27.283839   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:25 k8s-master kubelet[25301]: E1018 23:33:25.578352   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:25 k8s-master kubelet[25301]: E1018 23:33:25.375540   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:25 k8s-master kubelet[25301]: E1018 23:33:25.234636   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:20 k8s-master kubelet[25301]: E1018 23:33:20.834621   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:14 k8s-master kubelet[25301]: E1018 23:33:14.717063   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:14 k8s-master kubelet[25301]: E1018 23:33:14.691782   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:14 k8s-master kubelet[25301]: E1018 23:33:14.571396   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:13 k8s-master kubelet[25301]: E1018 23:33:13.622930   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:13 k8s-master kubelet[25301]: E1018 23:33:13.439693   25301 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-master.14eeb3f02437ef9c", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-master", UID:"k8s-master", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node k8s-master status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"k8s-master"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:196863388, loc:(*time.Location)(0x5314a60)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63643937570, nsec:872173783, loc:(*time.Location)(0x5314a60)}}, Count:5, Type:"Normal"}': 'namespaces "default" not found' (will not retry!)
Oct 18 23:33:13 k8s-master kubelet[25301]: E1018 23:33:13.136459   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:13 k8s-master kubelet[25301]: E1018 23:33:13.040823   25301 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-master.14eeb3f02437c097", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-master", UID:"k8s-master", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientDisk", Message:"Node k8s-master status is now: NodeHasSufficientDisk", Source:v1.EventSource{Component:"kubelet", Host:"k8s-master"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:196851351, loc:(*time.Location)(0x5314a60)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63643937570, nsec:872144180, loc:(*time.Location)(0x5314a60)}}, Count:5, Type:"Normal"}': 'namespaces "default" not found' (will not retry!)
Oct 18 23:33:12 k8s-master kubelet[25301]: E1018 23:33:12.640418   25301 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-master.14eeb3f02437fb8e", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-master", UID:"k8s-master", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node k8s-master status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"k8s-master"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:196866446, loc:(*time.Location)(0x5314a60)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63643937570, nsec:69587561, loc:(*time.Location)(0x5314a60)}}, Count:4, Type:"Normal"}': 'namespaces "default" not found' (will not retry!)
Oct 18 23:33:12 k8s-master kubelet[25301]: E1018 23:33:12.496327   25301 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": nodes "k8s-master" not found
Oct 18 23:33:12 k8s-master kubelet[25301]: I1018 23:33:12.493708   25301 kubelet_node_status.go:86] Successfully registered node k8s-master
Oct 18 23:33:12 k8s-master kubelet[25301]: E1018 23:33:12.240659   25301 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-master.14eeb3f02437ef9c", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-master", UID:"k8s-master", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node k8s-master status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"k8s-master"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:196863388, loc:(*time.Location)(0x5314a60)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63643937570, nsec:69584906, loc:(*time.Location)(0x5314a60)}}, Count:4, Type:"Normal"}': 'namespaces "default" not found' (will not retry!)
Oct 18 23:33:11 k8s-master kubelet[25301]: E1018 23:33:11.840171   25301 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-master.14eeb3f02437c097", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-master", UID:"k8s-master", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientDisk", Message:"Node k8s-master status is now: NodeHasSufficientDisk", Source:v1.EventSource{Component:"kubelet", Host:"k8s-master"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:196851351, loc:(*time.Location)(0x5314a60)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63643937570, nsec:69579969, loc:(*time.Location)(0x5314a60)}}, Count:4, Type:"Normal"}': 'namespaces "default" not found' (will not retry!)
Oct 18 23:33:11 k8s-master kubelet[25301]: E1018 23:33:11.531138   25301 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-master.14eeb3f02437fb8e", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-master", UID:"k8s-master", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node k8s-master status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"k8s-master"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:196866446, loc:(*time.Location)(0x5314a60)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:667000261, loc:(*time.Location)(0x5314a60)}}, Count:3, Type:"Normal"}': 'namespaces "default" not found' (will not retry!)
Oct 18 23:33:11 k8s-master kubelet[25301]: E1018 23:33:11.475784   25301 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-master.14eeb3f02437ef9c", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-master", UID:"k8s-master", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node k8s-master status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"k8s-master"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:196863388, loc:(*time.Location)(0x5314a60)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:666993385, loc:(*time.Location)(0x5314a60)}}, Count:3, Type:"Normal"}': 'namespaces "default" not found' (will not retry!)
Oct 18 23:33:11 k8s-master kubelet[25301]: E1018 23:33:11.420020   25301 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-master.14eeb3f02437c097", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-master", UID:"k8s-master", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientDisk", Message:"Node k8s-master status is now: NodeHasSufficientDisk", Source:v1.EventSource{Component:"kubelet", Host:"k8s-master"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:196851351, loc:(*time.Location)(0x5314a60)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:666988127, loc:(*time.Location)(0x5314a60)}}, Count:3, Type:"Normal"}': 'namespaces "default" not found' (will not retry!)
Oct 18 23:33:11 k8s-master kubelet[25301]: E1018 23:33:11.363640   25301 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-master.14eeb3f02437fb8e", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-master", UID:"k8s-master", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node k8s-master status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"k8s-master"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:196866446, loc:(*time.Location)(0x5314a60)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:463182688, loc:(*time.Location)(0x5314a60)}}, Count:2, Type:"Normal"}': 'namespaces "default" not found' (will not retry!)
Oct 18 23:33:11 k8s-master kubelet[25301]: E1018 23:33:11.307908   25301 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-master.14eeb3f02437ef9c", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-master", UID:"k8s-master", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node k8s-master status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"k8s-master"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:196863388, loc:(*time.Location)(0x5314a60)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:463177336, loc:(*time.Location)(0x5314a60)}}, Count:2, Type:"Normal"}': 'namespaces "default" not found' (will not retry!)
Oct 18 23:33:11 k8s-master kubelet[25301]: E1018 23:33:11.252157   25301 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-master.14eeb3f02437c097", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-master", UID:"k8s-master", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientDisk", Message:"Node k8s-master status is now: NodeHasSufficientDisk", Source:v1.EventSource{Component:"kubelet", Host:"k8s-master"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:196851351, loc:(*time.Location)(0x5314a60)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:463153779, loc:(*time.Location)(0x5314a60)}}, Count:2, Type:"Normal"}': 'namespaces "default" not found' (will not retry!)
Oct 18 23:33:09 k8s-master kubelet[25301]: E1018 23:33:09.598813   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:09 k8s-master kubelet[25301]: E1018 23:33:09.472086   25301 eviction_manager.go:238] eviction manager: unexpected err: failed to get node info: node 'k8s-master' not found
Oct 18 23:33:08 k8s-master kubelet[25301]: E1018 23:33:08.114938   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:33:07 k8s-master kubelet[25301]: E1018 23:33:07.176619   25301 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-master.14eeb3f0244b6b32", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-master", UID:"k8s-master", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeAllocatableEnforced", Message:"Updated Node Allocatable limit across pods", Source:v1.EventSource{Component:"kubelet", Host:"k8s-master"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:198140210, loc:(*time.Location)(0x5314a60)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:198140210, loc:(*time.Location)(0x5314a60)}}, Count:1, Type:"Normal"}': 'namespaces "default" not found' (will not retry!)
Oct 18 23:33:07 k8s-master kubelet[25301]: E1018 23:33:07.090311   25301 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-master.14eeb3f02437fb8e", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-master", UID:"k8s-master", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasNoDiskPressure", Message:"Node k8s-master status is now: NodeHasNoDiskPressure", Source:v1.EventSource{Component:"kubelet", Host:"k8s-master"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:196866446, loc:(*time.Location)(0x5314a60)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:196866446, loc:(*time.Location)(0x5314a60)}}, Count:1, Type:"Normal"}': 'namespaces "default" not found' (will not retry!)
Oct 18 23:33:07 k8s-master kubelet[25301]: E1018 23:33:07.034644   25301 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-master.14eeb3f02437ef9c", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-master", UID:"k8s-master", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientMemory", Message:"Node k8s-master status is now: NodeHasSufficientMemory", Source:v1.EventSource{Component:"kubelet", Host:"k8s-master"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:196863388, loc:(*time.Location)(0x5314a60)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:196863388, loc:(*time.Location)(0x5314a60)}}, Count:1, Type:"Normal"}': 'namespaces "default" not found' (will not retry!)
Oct 18 23:33:06 k8s-master kubelet[25301]: E1018 23:33:06.979678   25301 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-master.14eeb3f02437c097", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-master", UID:"k8s-master", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"NodeHasSufficientDisk", Message:"Node k8s-master status is now: NodeHasSufficientDisk", Source:v1.EventSource{Component:"kubelet", Host:"k8s-master"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:196851351, loc:(*time.Location)(0x5314a60)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:196851351, loc:(*time.Location)(0x5314a60)}}, Count:1, Type:"Normal"}': 'namespaces "default" not found' (will not retry!)
Oct 18 23:33:06 k8s-master kubelet[25301]: E1018 23:33:06.924572   25301 event.go:200] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"k8s-master.14eeb3f02322d6ab", GenerateName:"", Namespace:"default", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Node", Namespace:"", Name:"k8s-master", UID:"k8s-master", APIVersion:"", ResourceVersion:"", FieldPath:""}, Reason:"Starting", Message:"Starting kubelet.", Source:v1.EventSource{Component:"kubelet", Host:"k8s-master"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:178703531, loc:(*time.Location)(0x5314a60)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63643937569, nsec:178703531, loc:(*time.Location)(0x5314a60)}}, Count:1, Type:"Normal"}': 'namespaces "default" not found' (will not retry!)
Oct 18 23:33:02 k8s-master kubelet[25301]: I1018 23:33:02.085372   25301 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 23:33:02 k8s-master kubelet[25301]: I1018 23:33:02.080750   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:33:00 k8s-master kubelet[25301]: E1018 23:33:00.771294   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:33:00 k8s-master kubelet[25301]: E1018 23:33:00.592126   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:33:00 k8s-master kubelet[25301]: E1018 23:33:00.371768   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:59 k8s-master kubelet[25301]: E1018 23:32:59.759860   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:59 k8s-master kubelet[25301]: E1018 23:32:59.571295   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:59 k8s-master kubelet[25301]: E1018 23:32:59.471859   25301 eviction_manager.go:238] eviction manager: unexpected err: failed to get node info: node 'k8s-master' not found
Oct 18 23:32:59 k8s-master kubelet[25301]: E1018 23:32:59.359621   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:59 k8s-master kubelet[25301]: E1018 23:32:59.159901   25301 kubelet.go:1612] Failed creating a mirror pod for "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:58 k8s-master kubelet[25301]: E1018 23:32:58.959646   25301 kubelet.go:1612] Failed creating a mirror pod for "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:58 k8s-master kubelet[25301]: E1018 23:32:58.770240   25301 kubelet.go:1612] Failed creating a mirror pod for "kube-apiserver-k8s-master_kube-system(d857b5bcdfd340d1ff3546b68f7f27c0)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:58 k8s-master kubelet[25301]: E1018 23:32:58.570474   25301 kubelet.go:1612] Failed creating a mirror pod for "kube-controller-manager-k8s-master_kube-system(79f7c686bb6d415a1d71ac57fc06f7aa)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:58 k8s-master kubelet[25301]: I1018 23:32:58.427941   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:32:58 k8s-master kubelet[25301]: I1018 23:32:58.427664   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:32:58 k8s-master kubelet[25301]: I1018 23:32:58.427502   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:32:58 k8s-master kubelet[25301]: I1018 23:32:58.427229   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:32:58 k8s-master kubelet[25301]: E1018 23:32:58.359836   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:58 k8s-master kubelet[25301]: E1018 23:32:58.212007   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:58 k8s-master kubelet[25301]: E1018 23:32:58.159538   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:57 k8s-master kubelet[25301]: W1018 23:32:57.759846   25301 status_manager.go:431] Failed to get status for pod "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:57 k8s-master kubelet[25301]: E1018 23:32:57.559519   25301 kubelet.go:1612] Failed creating a mirror pod for "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:57 k8s-master kubelet[25301]: I1018 23:32:57.418461   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:32:57 k8s-master kubelet[25301]: E1018 23:32:57.414674   25301 kubelet.go:1612] Failed creating a mirror pod for "kube-apiserver-k8s-master_kube-system(d857b5bcdfd340d1ff3546b68f7f27c0)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:57 k8s-master kubelet[25301]: W1018 23:32:57.414624   25301 status_manager.go:431] Failed to get status for pod "kube-apiserver-k8s-master_kube-system(d857b5bcdfd340d1ff3546b68f7f27c0)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:57 k8s-master kubelet[25301]: I1018 23:32:57.397482   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:32:57 k8s-master kubelet[25301]: E1018 23:32:57.393691   25301 kubelet.go:1612] Failed creating a mirror pod for "kube-controller-manager-k8s-master_kube-system(79f7c686bb6d415a1d71ac57fc06f7aa)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:57 k8s-master kubelet[25301]: W1018 23:32:57.393634   25301 status_manager.go:431] Failed to get status for pod "kube-controller-manager-k8s-master_kube-system(79f7c686bb6d415a1d71ac57fc06f7aa)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:57 k8s-master kubelet[25301]: I1018 23:32:57.377113   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:32:57 k8s-master kubelet[25301]: E1018 23:32:57.373466   25301 kubelet.go:1612] Failed creating a mirror pod for "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:57 k8s-master kubelet[25301]: W1018 23:32:57.373406   25301 status_manager.go:431] Failed to get status for pod "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/etcd-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:57 k8s-master kubelet[25301]: I1018 23:32:57.366501   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:32:57 k8s-master kubelet[25301]: E1018 23:32:57.207121   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:57 k8s-master kubelet[25301]: E1018 23:32:57.192713   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:57 k8s-master kubelet[25301]: E1018 23:32:57.151868   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:56 k8s-master kubelet[25301]: W1018 23:32:56.372121   25301 pod_container_deletor.go:77] Container "dba5b9aacbc397838201742778227ccabda915ed75976c2975e9ab0a619e354a" not found in pod's containers
Oct 18 23:32:56 k8s-master kubelet[25301]: E1018 23:32:56.335435   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:32:56 k8s-master kubelet[25301]: W1018 23:32:56.335338   25301 container.go:354] Failed to create summary reader for "/libcontainer_25561_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:32:56 k8s-master kubelet[25301]: I1018 23:32:56.332801   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:32:56 k8s-master kubelet[25301]: E1018 23:32:56.249286   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:32:56 k8s-master kubelet[25301]: W1018 23:32:56.237227   25301 container.go:354] Failed to create summary reader for "/libcontainer_25539_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:32:56 k8s-master kubelet[25301]: W1018 23:32:56.167456   25301 docker_container.go:202] Deleted previously existing symlink file: "/var/log/pods/40eb0889c614345e2a2714d4ee7d1cc0/etcd_0.log"
Oct 18 23:32:56 k8s-master kubelet[25301]: E1018 23:32:56.145892   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:56 k8s-master kubelet[25301]: E1018 23:32:56.145846   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:56 k8s-master kubelet[25301]: E1018 23:32:56.145796   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:56 k8s-master kubelet[25301]: W1018 23:32:56.122076   25301 docker_container.go:202] Deleted previously existing symlink file: "/var/log/pods/f3e5e477637a31dd77e4d4e3534d2e23/kube-scheduler_0.log"
Oct 18 23:32:55 k8s-master kubelet[25301]: E1018 23:32:55.680686   25301 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:55 k8s-master kubelet[25301]: I1018 23:32:55.680259   25301 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 23:32:55 k8s-master kubelet[25301]: I1018 23:32:55.677264   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:32:55 k8s-master kubelet[25301]: W1018 23:32:55.412007   25301 pod_container_deletor.go:77] Container "3126fa7f4c1433d55c8ae9e611633a4e0ea4357758ff221747442e886df61a51" not found in pod's containers
Oct 18 23:32:55 k8s-master kubelet[25301]: I1018 23:32:55.342598   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:32:55 k8s-master kubelet[25301]: E1018 23:32:55.323110   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:32:55 k8s-master kubelet[25301]: W1018 23:32:55.322990   25301 container.go:354] Failed to create summary reader for "/libcontainer_25424_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:32:55 k8s-master kubelet[25301]: W1018 23:32:55.220127   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_25424_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_25424_systemd_test_default.slice: no such file or directory
Oct 18 23:32:55 k8s-master kubelet[25301]: E1018 23:32:55.154391   25301 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:32:55 k8s-master kubelet[25301]: W1018 23:32:55.146410   25301 container.go:354] Failed to create summary reader for "/libcontainer_25373_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:32:55 k8s-master kubelet[25301]: E1018 23:32:55.140059   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:55 k8s-master kubelet[25301]: E1018 23:32:55.125573   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:55 k8s-master kubelet[25301]: E1018 23:32:55.122354   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:54 k8s-master kubelet[25301]: W1018 23:32:54.928540   25301 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_25373_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_25373_systemd_test_default.slice: no such file or directory
Oct 18 23:32:54 k8s-master kubelet[25301]: I1018 23:32:54.298874   25301 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs-etc-pki" (UniqueName: "kubernetes.io/host-path/79f7c686bb6d415a1d71ac57fc06f7aa-ca-certs-etc-pki") pod "kube-controller-manager-k8s-master" (UID: "79f7c686bb6d415a1d71ac57fc06f7aa")
Oct 18 23:32:54 k8s-master kubelet[25301]: I1018 23:32:54.298854   25301 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "flexvolume-dir" (UniqueName: "kubernetes.io/host-path/79f7c686bb6d415a1d71ac57fc06f7aa-flexvolume-dir") pod "kube-controller-manager-k8s-master" (UID: "79f7c686bb6d415a1d71ac57fc06f7aa")
Oct 18 23:32:54 k8s-master kubelet[25301]: I1018 23:32:54.298830   25301 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/79f7c686bb6d415a1d71ac57fc06f7aa-kubeconfig") pod "kube-controller-manager-k8s-master" (UID: "79f7c686bb6d415a1d71ac57fc06f7aa")
Oct 18 23:32:54 k8s-master kubelet[25301]: I1018 23:32:54.298813   25301 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs-etc-pki" (UniqueName: "kubernetes.io/host-path/d857b5bcdfd340d1ff3546b68f7f27c0-ca-certs-etc-pki") pod "kube-apiserver-k8s-master" (UID: "d857b5bcdfd340d1ff3546b68f7f27c0")
Oct 18 23:32:54 k8s-master kubelet[25301]: I1018 23:32:54.298794   25301 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs" (UniqueName: "kubernetes.io/host-path/d857b5bcdfd340d1ff3546b68f7f27c0-ca-certs") pod "kube-apiserver-k8s-master" (UID: "d857b5bcdfd340d1ff3546b68f7f27c0")
Oct 18 23:32:54 k8s-master kubelet[25301]: I1018 23:32:54.298773   25301 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/f3e5e477637a31dd77e4d4e3534d2e23-kubeconfig") pod "kube-scheduler-k8s-master" (UID: "f3e5e477637a31dd77e4d4e3534d2e23")
Oct 18 23:32:54 k8s-master kubelet[25301]: I1018 23:32:54.298717   25301 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs" (UniqueName: "kubernetes.io/host-path/79f7c686bb6d415a1d71ac57fc06f7aa-ca-certs") pod "kube-controller-manager-k8s-master" (UID: "79f7c686bb6d415a1d71ac57fc06f7aa")
Oct 18 23:32:54 k8s-master kubelet[25301]: I1018 23:32:54.298695   25301 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "k8s-certs" (UniqueName: "kubernetes.io/host-path/79f7c686bb6d415a1d71ac57fc06f7aa-k8s-certs") pod "kube-controller-manager-k8s-master" (UID: "79f7c686bb6d415a1d71ac57fc06f7aa")
Oct 18 23:32:54 k8s-master kubelet[25301]: I1018 23:32:54.298668   25301 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "k8s-certs" (UniqueName: "kubernetes.io/host-path/d857b5bcdfd340d1ff3546b68f7f27c0-k8s-certs") pod "kube-apiserver-k8s-master" (UID: "d857b5bcdfd340d1ff3546b68f7f27c0")
Oct 18 23:32:54 k8s-master kubelet[25301]: I1018 23:32:54.298623   25301 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "etcd" (UniqueName: "kubernetes.io/host-path/40eb0889c614345e2a2714d4ee7d1cc0-etcd") pod "etcd-k8s-master" (UID: "40eb0889c614345e2a2714d4ee7d1cc0")
Oct 18 23:32:54 k8s-master kubelet[25301]: E1018 23:32:54.277532   25301 kubelet.go:1612] Failed creating a mirror pod for "kube-controller-manager-k8s-master_kube-system(79f7c686bb6d415a1d71ac57fc06f7aa)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:54 k8s-master kubelet[25301]: E1018 23:32:54.265844   25301 kubelet.go:1612] Failed creating a mirror pod for "kube-apiserver-k8s-master_kube-system(d857b5bcdfd340d1ff3546b68f7f27c0)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:54 k8s-master kubelet[25301]: E1018 23:32:54.223228   25301 kubelet.go:1612] Failed creating a mirror pod for "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:54 k8s-master kubelet[25301]: W1018 23:32:54.223163   25301 status_manager.go:431] Failed to get status for pod "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:54 k8s-master kubelet[25301]: W1018 23:32:54.221045   25301 status_manager.go:431] Failed to get status for pod "kube-controller-manager-k8s-master_kube-system(79f7c686bb6d415a1d71ac57fc06f7aa)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:54 k8s-master kubelet[25301]: I1018 23:32:54.215840   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:32:54 k8s-master kubelet[25301]: W1018 23:32:54.213112   25301 status_manager.go:431] Failed to get status for pod "kube-apiserver-k8s-master_kube-system(d857b5bcdfd340d1ff3546b68f7f27c0)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:54 k8s-master kubelet[25301]: I1018 23:32:54.212250   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:32:54 k8s-master kubelet[25301]: I1018 23:32:54.210458   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:32:54 k8s-master kubelet[25301]: E1018 23:32:54.207478   25301 kubelet.go:1612] Failed creating a mirror pod for "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:54 k8s-master kubelet[25301]: W1018 23:32:54.207427   25301 status_manager.go:431] Failed to get status for pod "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/etcd-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:54 k8s-master kubelet[25301]: I1018 23:32:54.206969   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:32:54 k8s-master kubelet[25301]: I1018 23:32:54.206092   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:32:54 k8s-master kubelet[25301]: I1018 23:32:54.201546   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:32:54 k8s-master kubelet[25301]: I1018 23:32:54.201152   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:32:54 k8s-master kubelet[25301]: I1018 23:32:54.198408   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:32:54 k8s-master kubelet[25301]: E1018 23:32:54.135054   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:54 k8s-master kubelet[25301]: E1018 23:32:54.122625   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:54 k8s-master kubelet[25301]: E1018 23:32:54.118012   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:53 k8s-master kubelet[25301]: E1018 23:32:53.134467   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:53 k8s-master kubelet[25301]: E1018 23:32:53.121969   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:53 k8s-master kubelet[25301]: E1018 23:32:53.117433   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:52 k8s-master kubelet[25301]: E1018 23:32:52.475131   25301 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:52 k8s-master kubelet[25301]: I1018 23:32:52.474777   25301 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 23:32:52 k8s-master kubelet[25301]: I1018 23:32:52.472735   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:32:52 k8s-master kubelet[25301]: E1018 23:32:52.133885   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:52 k8s-master kubelet[25301]: E1018 23:32:52.121357   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:52 k8s-master kubelet[25301]: E1018 23:32:52.116858   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:51 k8s-master kubelet[25301]: E1018 23:32:51.566519   25301 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/default/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 23:32:51 k8s-master kubelet[25301]: E1018 23:32:51.133335   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:51 k8s-master kubelet[25301]: E1018 23:32:51.120775   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:51 k8s-master kubelet[25301]: E1018 23:32:51.116161   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:50 k8s-master kubelet[25301]: E1018 23:32:50.872515   25301 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:50 k8s-master kubelet[25301]: I1018 23:32:50.872189   25301 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 23:32:50 k8s-master kubelet[25301]: I1018 23:32:50.870295   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:32:50 k8s-master kubelet[25301]: E1018 23:32:50.132682   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:50 k8s-master kubelet[25301]: E1018 23:32:50.119401   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:50 k8s-master kubelet[25301]: E1018 23:32:50.115507   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:50 k8s-master kubelet[25301]: E1018 23:32:50.069892   25301 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:50 k8s-master kubelet[25301]: I1018 23:32:50.069598   25301 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 23:32:50 k8s-master kubelet[25301]: I1018 23:32:50.067776   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:32:49 k8s-master kubelet[25301]: E1018 23:32:49.667359   25301 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.667014   25301 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.664178   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:32:49 k8s-master kubelet[25301]: E1018 23:32:49.463709   25301 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.463197   25301 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 23:32:49 k8s-master kubelet[25301]: E1018 23:32:49.461585   25301 eviction_manager.go:238] eviction manager: unexpected err: failed to get node info: node 'k8s-master' not found
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.328941   25301 manager.go:316] Recovery completed
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.303509   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.245997   25301 manager.go:311] Starting recovery of all containers
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.244943   25301 manager.go:1140] Started watching for new ooms in manager
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.244822   25301 factory.go:86] Registering Raw factory
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.244684   25301 factory.go:54] Registering systemd factory
Oct 18 23:32:49 k8s-master kubelet[25301]: W1018 23:32:49.244667   25301 manager.go:276] Registration of the crio container factory failed: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 23:32:49 k8s-master kubelet[25301]: W1018 23:32:49.244544   25301 manager.go:265] Registration of the rkt container factory failed: unable to communicate with Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.244519   25301 factory.go:355] Registering Docker factory
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.198310   25301 volume_manager.go:246] Starting Kubelet Volume Manager
Oct 18 23:32:49 k8s-master kubelet[25301]: E1018 23:32:49.198292   25301 container_manager_linux.go:603] [ContainerManager]: Fail to get rootfs information unable to find data for container /
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.197885   25301 kubelet.go:1779] skipping pod synchronization - [container runtime is down PLEG is not healthy: pleg was last seen active 2562047h47m16.854775807s ago; threshold is 3m0s]
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.197874   25301 kubelet.go:1768] Starting kubelet main sync loop.
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.197860   25301 status_manager.go:140] Starting to sync pod status with apiserver
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.197818   25301 fs_resource_analyzer.go:66] Starting FS ResourceAnalyzer
Oct 18 23:32:49 k8s-master kubelet[25301]: E1018 23:32:49.183344   25301 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/default/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.180854   25301 server.go:296] Adding debug handlers to kubelet server.
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.180062   25301 server.go:128] Starting to listen on 0.0.0.0:10250
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.179757   25301 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:32:49 k8s-master kubelet[25301]: E1018 23:32:49.179362   25301 kubelet.go:1234] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data for container /
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.178741   25301 server.go:718] Started kubelet v1.8.1
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.176362   25301 kuberuntime_manager.go:177] Container runtime docker initialized, version: 17.10.0-ce, apiVersion: 1.33.0
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.173675   25301 remote_runtime.go:43] Connecting to runtime service unix:///var/run/dockershim.sock
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.148425   25301 docker_service.go:224] Setting cgroupDriver to systemd
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.134604   25301 docker_service.go:207] Docker cri networking managed by kubernetes.io/no-op
Oct 18 23:32:49 k8s-master kubelet[25301]: W1018 23:32:49.125707   25301 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.120905   25301 kubelet.go:517] Hairpin mode set to "hairpin-veth"
Oct 18 23:32:49 k8s-master kubelet[25301]: W1018 23:32:49.120815   25301 kubelet_network.go:69] Hairpin mode set to "promiscuous-bridge" but kubenet is not enabled, falling back to "hairpin-veth"
Oct 18 23:32:49 k8s-master kubelet[25301]: E1018 23:32:49.115028   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:49 k8s-master kubelet[25301]: E1018 23:32:49.114989   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:49 k8s-master kubelet[25301]: E1018 23:32:49.114909   25301 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.042930   25301 kubelet.go:283] Watching apiserver
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.042893   25301 kubelet.go:273] Adding manifest file: /etc/kubernetes/manifests
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.042794   25301 container_manager_linux.go:288] Creating device plugin handler: false
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.042680   25301 container_manager_linux.go:257] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: ContainerRuntime:docker CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[{Signal:memory.available Operator:LessThan Value:{Quantity:100Mi Percentage:0} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.1} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.inodesFree Operator:LessThan Value:{Quantity:<nil> Percentage:0.05} GracePeriod:0s MinReclaim:<nil>}]} ExperimentalQOSReserved:map[] ExperimentalCPUManagerPolicy:none ExperimentalCPUManagerReconcilePeriod:10s}
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.042651   25301 container_manager_linux.go:252] container manager verified user specified cgroup-root exists: /
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.040315   25301 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.039020   25301 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.036625   25301 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:flannel.1 MacAddress:aa:be:27:bb:bf:3e Speed:0 Mtu:1450} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.034900   25301 fs.go:140] Filesystem partitions: map[/dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0} tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0}]
Oct 18 23:32:49 k8s-master kubelet[25301]: I1018 23:32:49.034873   25301 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 23:32:49 k8s-master kubelet[25301]: W1018 23:32:49.010725   25301 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 23:32:49 k8s-master kubelet[25301]: W1018 23:32:49.010509   25301 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 23:32:48 k8s-master kubelet[25301]: I1018 23:32:48.987408   25301 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 23:32:48 k8s-master kubelet[25301]: E1018 23:32:48.986769   25301 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 23:32:48 k8s-master kubelet[25301]: I1018 23:32:48.985942   25301 certificate_manager.go:361] Requesting new certificate.
Oct 18 23:32:48 k8s-master kubelet[25301]: W1018 23:32:48.953726   25301 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 23:32:48 k8s-master kubelet[25301]: I1018 23:32:48.953595   25301 feature_gate.go:156] feature gates: map[]
Oct 18 23:32:48 k8s-master kubelet[25301]: W1018 23:32:48.923431   25301 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 23:32:48 k8s-master kubelet[25301]: I1018 23:32:48.919351   25301 client.go:95] Start docker client with request timeout=2m0s
Oct 18 23:32:48 k8s-master kubelet[25301]: I1018 23:32:48.919299   25301 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 23:32:48 k8s-master kubelet[25301]: I1018 23:32:48.907202   25301 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 23:32:48 k8s-master kubelet[25301]: I1018 23:32:48.907197   25301 controller.go:114] kubelet config controller: starting controller
Oct 18 23:32:48 k8s-master kubelet[25301]: I1018 23:32:48.907117   25301 feature_gate.go:156] feature gates: map[]
Oct 18 23:32:48 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 23:32:48 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 23:32:48 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 23:32:38 k8s-master systemd[1]: kubelet.service failed.
Oct 18 23:32:38 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 23:32:38 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 23:32:38 k8s-master kubelet[25289]: error: failed to run Kubelet: unable to load bootstrap kubeconfig: stat /etc/kubernetes/bootstrap-kubelet.conf: no such file or directory
Oct 18 23:32:38 k8s-master kubelet[25289]: W1018 23:32:38.745391   25289 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 23:32:38 k8s-master kubelet[25289]: I1018 23:32:38.745257   25289 feature_gate.go:156] feature gates: map[]
Oct 18 23:32:38 k8s-master kubelet[25289]: W1018 23:32:38.679312   25289 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 23:32:38 k8s-master kubelet[25289]: I1018 23:32:38.657556   25289 client.go:95] Start docker client with request timeout=2m0s
Oct 18 23:32:38 k8s-master kubelet[25289]: I1018 23:32:38.657523   25289 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 23:32:37 k8s-master kubelet[25289]: I1018 23:32:37.675068   25289 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 23:32:37 k8s-master kubelet[25289]: I1018 23:32:37.675062   25289 controller.go:114] kubelet config controller: starting controller
Oct 18 23:32:37 k8s-master kubelet[25289]: I1018 23:32:37.674732   25289 feature_gate.go:156] feature gates: map[]
Oct 18 23:32:37 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 23:32:37 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 23:29:12 k8s-master systemd[1]: kubelet.service failed.
Oct 18 23:29:12 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 23:29:12 k8s-master systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Oct 18 23:29:12 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 23:29:12 k8s-master systemd[1]: Stopping kubelet: The Kubernetes Node Agent...
Oct 18 23:29:10 k8s-master kubelet[22922]: E1018 23:29:10.911829   22922 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:29:10 k8s-master kubelet[22922]: I1018 23:29:10.911801   22922 kuberuntime_manager.go:748] Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:29:10 k8s-master kubelet[22922]: I1018 23:29:10.911690   22922 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:29:10 k8s-master kubelet[22922]: I1018 23:29:10.911427   22922 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:29:09 k8s-master kubelet[22922]: E1018 23:29:09.690358   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:29:09 k8s-master kubelet[22922]: W1018 23:29:09.690226   22922 container.go:354] Failed to create summary reader for "/libcontainer_24883_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:29:09 k8s-master kubelet[22922]: W1018 23:29:09.676740   22922 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24883_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24883_systemd_test_default.slice: no such file or directory
Oct 18 23:29:05 k8s-master kubelet[22922]: E1018 23:29:05.881275   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:29:05 k8s-master kubelet[22922]: E1018 23:29:05.383853   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:29:05 k8s-master kubelet[22922]: W1018 23:29:05.383753   22922 container.go:354] Failed to create summary reader for "/libcontainer_24862_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:29:05 k8s-master kubelet[22922]: I1018 23:29:05.106975   22922 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:29:05 k8s-master kubelet[22922]: E1018 23:29:05.035185   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:29:05 k8s-master kubelet[22922]: W1018 23:29:05.034973   22922 container.go:354] Failed to create summary reader for "/libcontainer_24841_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:29:05 k8s-master kubelet[22922]: W1018 23:29:05.017818   22922 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24841_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24841_systemd_test_default.slice: no such file or directory
Oct 18 23:29:04 k8s-master kubelet[22922]: E1018 23:29:04.992463   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:29:04 k8s-master kubelet[22922]: W1018 23:29:04.992343   22922 container.go:354] Failed to create summary reader for "/libcontainer_24838_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:29:04 k8s-master kubelet[22922]: W1018 23:29:04.965152   22922 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24838_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24838_systemd_test_default.slice: no such file or directory
Oct 18 23:29:04 k8s-master kubelet[22922]: E1018 23:29:04.941441   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:29:04 k8s-master kubelet[22922]: W1018 23:29:04.941374   22922 container.go:354] Failed to create summary reader for "/libcontainer_24834_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:29:04 k8s-master kubelet[22922]: W1018 23:29:04.927790   22922 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24834_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24834_systemd_test_default.slice: no such file or directory
Oct 18 23:28:09 k8s-master kubelet[22922]: E1018 23:28:09.560914   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:28:09 k8s-master kubelet[22922]: W1018 23:28:09.560812   22922 container.go:354] Failed to create summary reader for "/libcontainer_24684_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:28:09 k8s-master kubelet[22922]: E1018 23:28:09.434347   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:28:09 k8s-master kubelet[22922]: W1018 23:28:09.434067   22922 container.go:354] Failed to create summary reader for "/libcontainer_24669_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:28:09 k8s-master kubelet[22922]: W1018 23:28:09.427926   22922 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24669_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24669_systemd_test_default.slice: no such file or directory
Oct 18 23:28:09 k8s-master kubelet[22922]: I1018 23:28:09.291169   22922 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:28:09 k8s-master kubelet[22922]: I1018 23:28:09.290950   22922 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:27:54 k8s-master kubelet[22922]: E1018 23:27:54.286488   22922 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:27:54 k8s-master kubelet[22922]: I1018 23:27:54.286383   22922 kuberuntime_manager.go:748] Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:27:54 k8s-master kubelet[22922]: I1018 23:27:54.286162   22922 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:27:54 k8s-master kubelet[22922]: I1018 23:27:54.285827   22922 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:27:43 k8s-master kubelet[22922]: E1018 23:27:43.289182   22922 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:27:43 k8s-master kubelet[22922]: I1018 23:27:43.289154   22922 kuberuntime_manager.go:748] Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:27:43 k8s-master kubelet[22922]: I1018 23:27:43.289057   22922 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:27:43 k8s-master kubelet[22922]: I1018 23:27:43.288710   22922 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:27:31 k8s-master kubelet[22922]: E1018 23:27:31.288714   22922 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:27:31 k8s-master kubelet[22922]: I1018 23:27:31.288687   22922 kuberuntime_manager.go:748] Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:27:31 k8s-master kubelet[22922]: I1018 23:27:31.288596   22922 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:27:31 k8s-master kubelet[22922]: I1018 23:27:31.288362   22922 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:27:18 k8s-master kubelet[22922]: E1018 23:27:18.128849   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:27:18 k8s-master kubelet[22922]: E1018 23:27:18.023595   22922 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:27:18 k8s-master kubelet[22922]: I1018 23:27:18.023568   22922 kuberuntime_manager.go:748] Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:27:18 k8s-master kubelet[22922]: I1018 23:27:18.023454   22922 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:27:18 k8s-master kubelet[22922]: I1018 23:27:18.023161   22922 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:27:15 k8s-master kubelet[22922]: E1018 23:27:15.863787   22922 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:27:15 k8s-master kubelet[22922]: I1018 23:27:15.863761   22922 kuberuntime_manager.go:748] Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:27:15 k8s-master kubelet[22922]: I1018 23:27:15.863655   22922 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:27:15 k8s-master kubelet[22922]: I1018 23:27:15.863364   22922 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:27:13 k8s-master kubelet[22922]: E1018 23:27:13.956872   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:27:13 k8s-master kubelet[22922]: E1018 23:27:13.946874   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:27:13 k8s-master kubelet[22922]: E1018 23:27:13.828691   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:27:10 k8s-master kubelet[22922]: E1018 23:27:10.669147   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:27:10 k8s-master kubelet[22922]: E1018 23:27:10.060639   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:27:09 k8s-master kubelet[22922]: E1018 23:27:09.348719   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:26:56 k8s-master kubelet[22922]: E1018 23:26:56.967524   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:26:55 k8s-master kubelet[22922]: E1018 23:26:55.253675   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:26:55 k8s-master kubelet[22922]: W1018 23:26:55.253582   22922 container.go:354] Failed to create summary reader for "/libcontainer_24471_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:26:55 k8s-master kubelet[22922]: W1018 23:26:55.215430   22922 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24471_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24471_systemd_test_default.slice: no such file or directory
Oct 18 23:26:55 k8s-master kubelet[22922]: E1018 23:26:55.147866   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:26:55 k8s-master kubelet[22922]: W1018 23:26:55.147667   22922 container.go:354] Failed to create summary reader for "/libcontainer_24457_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:26:55 k8s-master kubelet[22922]: W1018 23:26:55.130707   22922 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24457_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24457_systemd_test_default.slice: no such file or directory
Oct 18 23:26:55 k8s-master kubelet[22922]: I1018 23:26:55.025971   22922 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:26:55 k8s-master kubelet[22922]: E1018 23:26:55.005304   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:26:55 k8s-master kubelet[22922]: W1018 23:26:55.004951   22922 container.go:354] Failed to create summary reader for "/libcontainer_24448_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:26:54 k8s-master kubelet[22922]: W1018 23:26:54.985060   22922 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24448_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24448_systemd_test_default.slice: no such file or directory
Oct 18 23:26:54 k8s-master kubelet[22922]: E1018 23:26:54.914179   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:26:54 k8s-master kubelet[22922]: W1018 23:26:54.914065   22922 container.go:354] Failed to create summary reader for "/libcontainer_24442_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:26:54 k8s-master kubelet[22922]: W1018 23:26:54.907630   22922 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24442_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24442_systemd_test_default.slice: no such file or directory
Oct 18 23:26:15 k8s-master kubelet[22922]: E1018 23:26:15.700863   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:26:14 k8s-master kubelet[22922]: E1018 23:26:14.841235   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:26:14 k8s-master kubelet[22922]: W1018 23:26:14.838451   22922 container.go:354] Failed to create summary reader for "/libcontainer_24321_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:26:14 k8s-master kubelet[22922]: I1018 23:26:14.298406   22922 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:26:14 k8s-master kubelet[22922]: I1018 23:26:14.298158   22922 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:26:09 k8s-master kubelet[22922]: E1018 23:26:09.416345   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:26:04 k8s-master kubelet[22922]: E1018 23:26:04.600692   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:26:02 k8s-master kubelet[22922]: E1018 23:26:02.287827   22922 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:26:02 k8s-master kubelet[22922]: I1018 23:26:02.287773   22922 kuberuntime_manager.go:748] Back-off 20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:26:02 k8s-master kubelet[22922]: I1018 23:26:02.287583   22922 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:26:02 k8s-master kubelet[22922]: I1018 23:26:02.287050   22922 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:25:58 k8s-master kubelet[22922]: E1018 23:25:58.446905   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:25:58 k8s-master kubelet[22922]: E1018 23:25:58.024578   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:25:57 k8s-master kubelet[22922]: E1018 23:25:57.022714   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:25:48 k8s-master kubelet[22922]: E1018 23:25:48.023748   22922 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:25:48 k8s-master kubelet[22922]: I1018 23:25:48.023725   22922 kuberuntime_manager.go:748] Back-off 20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:25:48 k8s-master kubelet[22922]: I1018 23:25:48.023623   22922 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:25:48 k8s-master kubelet[22922]: I1018 23:25:48.023345   22922 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:25:46 k8s-master kubelet[22922]: E1018 23:25:46.966493   22922 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:25:46 k8s-master kubelet[22922]: I1018 23:25:46.966453   22922 kuberuntime_manager.go:748] Back-off 20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:25:46 k8s-master kubelet[22922]: I1018 23:25:46.966339   22922 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:25:46 k8s-master kubelet[22922]: I1018 23:25:46.965399   22922 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:25:45 k8s-master kubelet[22922]: E1018 23:25:45.765076   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:25:45 k8s-master kubelet[22922]: W1018 23:25:45.764948   22922 container.go:354] Failed to create summary reader for "/libcontainer_24259_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:25:45 k8s-master kubelet[22922]: W1018 23:25:45.749850   22922 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24259_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24259_systemd_test_default.slice: no such file or directory
Oct 18 23:25:45 k8s-master kubelet[22922]: E1018 23:25:45.731738   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:25:45 k8s-master kubelet[22922]: W1018 23:25:45.731613   22922 container.go:354] Failed to create summary reader for "/libcontainer_24256_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:25:45 k8s-master kubelet[22922]: W1018 23:25:45.711468   22922 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24256_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24256_systemd_test_default.slice: no such file or directory
Oct 18 23:25:18 k8s-master kubelet[22922]: E1018 23:25:18.289574   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:25:15 k8s-master kubelet[22922]: E1018 23:25:15.558671   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:25:14 k8s-master kubelet[22922]: E1018 23:25:14.630398   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:25:08 k8s-master kubelet[22922]: E1018 23:25:08.990154   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:25:06 k8s-master kubelet[22922]: E1018 23:25:06.055367   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:25:04 k8s-master kubelet[22922]: E1018 23:25:04.287571   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:25:03 k8s-master kubelet[22922]: E1018 23:25:03.682774   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:25:02 k8s-master kubelet[22922]: E1018 23:25:02.459889   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:25:00 k8s-master kubelet[22922]: E1018 23:25:00.107807   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:24:58 k8s-master kubelet[22922]: E1018 23:24:58.912806   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:24:58 k8s-master kubelet[22922]: E1018 23:24:58.128545   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:24:55 k8s-master kubelet[22922]: E1018 23:24:55.785999   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:24:55 k8s-master kubelet[22922]: E1018 23:24:55.374835   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:24:45 k8s-master kubelet[22922]: E1018 23:24:45.812948   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:24:45 k8s-master kubelet[22922]: W1018 23:24:45.812948   22922 container.go:354] Failed to create summary reader for "/libcontainer_24063_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:24:45 k8s-master kubelet[22922]: W1018 23:24:45.761326   22922 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24063_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24063_systemd_test_default.slice: no such file or directory
Oct 18 23:24:45 k8s-master kubelet[22922]: E1018 23:24:45.445469   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:24:45 k8s-master kubelet[22922]: W1018 23:24:45.445362   22922 container.go:354] Failed to create summary reader for "/libcontainer_24032_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:24:45 k8s-master kubelet[22922]: I1018 23:24:45.442377   22922 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:24:45 k8s-master kubelet[22922]: W1018 23:24:45.409481   22922 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24032_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24032_systemd_test_default.slice: no such file or directory
Oct 18 23:24:45 k8s-master kubelet[22922]: E1018 23:24:45.334332   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:24:45 k8s-master kubelet[22922]: W1018 23:24:45.334227   22922 container.go:354] Failed to create summary reader for "/libcontainer_24017_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:24:45 k8s-master kubelet[22922]: W1018 23:24:45.325016   22922 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24017_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24017_systemd_test_default.slice: no such file or directory
Oct 18 23:24:45 k8s-master kubelet[22922]: I1018 23:24:45.075821   22922 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:24:45 k8s-master kubelet[22922]: E1018 23:24:45.052562   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:24:45 k8s-master kubelet[22922]: W1018 23:24:45.052368   22922 container.go:354] Failed to create summary reader for "/libcontainer_24008_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:24:45 k8s-master kubelet[22922]: W1018 23:24:45.045193   22922 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24008_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24008_systemd_test_default.slice: no such file or directory
Oct 18 23:24:44 k8s-master kubelet[22922]: E1018 23:24:44.993256   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:24:44 k8s-master kubelet[22922]: W1018 23:24:44.993100   22922 container.go:354] Failed to create summary reader for "/libcontainer_24005_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:24:44 k8s-master kubelet[22922]: W1018 23:24:44.963980   22922 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_24005_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_24005_systemd_test_default.slice: no such file or directory
Oct 18 23:24:14 k8s-master kubelet[22922]: I1018 23:24:14.873208   22922 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:24:11 k8s-master kubelet[22922]: E1018 23:24:11.150606   22922 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 10s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:24:11 k8s-master kubelet[22922]: I1018 23:24:11.150518   22922 kuberuntime_manager.go:748] Back-off 10s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:24:11 k8s-master kubelet[22922]: I1018 23:24:11.150390   22922 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:24:11 k8s-master kubelet[22922]: I1018 23:24:11.149978   22922 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:24:10 k8s-master kubelet[22922]: E1018 23:24:10.324732   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:24:10 k8s-master kubelet[22922]: W1018 23:24:10.324620   22922 container.go:354] Failed to create summary reader for "/libcontainer_23885_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:24:10 k8s-master kubelet[22922]: W1018 23:24:10.304877   22922 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23885_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23885_systemd_test_default.slice: no such file or directory
Oct 18 23:24:10 k8s-master kubelet[22922]: E1018 23:24:10.285372   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:24:10 k8s-master kubelet[22922]: W1018 23:24:10.285254   22922 container.go:354] Failed to create summary reader for "/libcontainer_23882_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:24:10 k8s-master kubelet[22922]: W1018 23:24:10.267330   22922 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23882_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23882_systemd_test_default.slice: no such file or directory
Oct 18 23:24:10 k8s-master kubelet[22922]: E1018 23:24:10.253011   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:24:10 k8s-master kubelet[22922]: W1018 23:24:10.253011   22922 container.go:354] Failed to create summary reader for "/libcontainer_23879_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:24:10 k8s-master kubelet[22922]: W1018 23:24:10.233332   22922 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23879_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23879_systemd_test_default.slice: no such file or directory
Oct 18 23:23:15 k8s-master kubelet[22922]: E1018 23:23:15.425992   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:23:15 k8s-master kubelet[22922]: E1018 23:23:15.331819   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:23:12 k8s-master kubelet[22922]: E1018 23:23:12.610731   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:23:09 k8s-master kubelet[22922]: E1018 23:23:09.981773   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:23:09 k8s-master kubelet[22922]: W1018 23:23:09.960999   22922 container.go:354] Failed to create summary reader for "/libcontainer_23598_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:23:09 k8s-master kubelet[22922]: W1018 23:23:09.910295   22922 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23598_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23598_systemd_test_default.slice: no such file or directory
Oct 18 23:23:09 k8s-master kubelet[22922]: E1018 23:23:09.843857   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:23:09 k8s-master kubelet[22922]: W1018 23:23:09.843726   22922 container.go:354] Failed to create summary reader for "/libcontainer_23586_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:23:09 k8s-master kubelet[22922]: W1018 23:23:09.826889   22922 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23586_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23586_systemd_test_default.slice: no such file or directory
Oct 18 23:23:09 k8s-master kubelet[22922]: I1018 23:23:09.645181   22922 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:23:09 k8s-master kubelet[22922]: I1018 23:23:09.644923   22922 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:23:08 k8s-master kubelet[22922]: W1018 23:23:08.307831   22922 container.go:354] Failed to create summary reader for "/libcontainer_23573_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:23:08 k8s-master kubelet[22922]: W1018 23:23:08.290553   22922 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23573_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23573_systemd_test_default.slice: no such file or directory
Oct 18 23:23:08 k8s-master kubelet[22922]: E1018 23:23:08.253236   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:23:08 k8s-master kubelet[22922]: W1018 23:23:08.253085   22922 container.go:354] Failed to create summary reader for "/libcontainer_23570_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:23:08 k8s-master kubelet[22922]: W1018 23:23:08.252133   22922 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23570_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23570_systemd_test_default.slice: no such file or directory
Oct 18 23:23:08 k8s-master kubelet[22922]: E1018 23:23:08.221848   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:23:08 k8s-master kubelet[22922]: W1018 23:23:08.221848   22922 container.go:354] Failed to create summary reader for "/libcontainer_23566_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:23:08 k8s-master kubelet[22922]: W1018 23:23:08.209759   22922 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_23566_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_23566_systemd_test_default.slice: no such file or directory
Oct 18 23:23:00 k8s-master kubelet[22922]: W1018 23:23:00.813632   22922 prober.go:98] No ref for container "docker://47c8e48059681414ad1dfffb87d5cea645c85d6c3adbca3c432e51ff188084eb" (kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b):kubedns)
Oct 18 23:22:53 k8s-master kubelet[22922]: E1018 23:22:53.301315   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:22:52 k8s-master kubelet[22922]: E1018 23:22:52.658775   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:22:50 k8s-master kubelet[22922]: W1018 23:22:50.814143   22922 prober.go:98] No ref for container "docker://47c8e48059681414ad1dfffb87d5cea645c85d6c3adbca3c432e51ff188084eb" (kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b):kubedns)
Oct 18 23:22:41 k8s-master kubelet[22922]: E1018 23:22:41.432114   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:22:40 k8s-master kubelet[22922]: W1018 23:22:40.814789   22922 prober.go:98] No ref for container "docker://47c8e48059681414ad1dfffb87d5cea645c85d6c3adbca3c432e51ff188084eb" (kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b):kubedns)
Oct 18 23:22:37 k8s-master kubelet[22922]: E1018 23:22:37.352353   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:22:30 k8s-master kubelet[22922]: W1018 23:22:30.813727   22922 prober.go:98] No ref for container "docker://47c8e48059681414ad1dfffb87d5cea645c85d6c3adbca3c432e51ff188084eb" (kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b):kubedns)
Oct 18 23:22:28 k8s-master kubelet[22922]: I1018 23:22:28.053140   22922 transport.go:88] certificate rotation detected, shutting down client connections to start using new credentials
Oct 18 23:22:25 k8s-master kubelet[22922]: E1018 23:22:25.434040   22922 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:22:25 k8s-master kubelet[22922]: W1018 23:22:25.433924   22922 container.go:354] Failed to create summary reader for "/libcontainer_23163_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:22:25 k8s-master kubelet[22922]: I1018 23:22:25.186471   22922 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:22:25 k8s-master kubelet[22922]: I1018 23:22:25.186304   22922 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:22:24 k8s-master kubelet[22922]: I1018 23:22:24.089004   22922 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "flexvolume-dir" (UniqueName: "kubernetes.io/host-path/edfd16bc8bf4e5fa8ff1f9f5a41bc7fb-flexvolume-dir") pod "kube-controller-manager-k8s-master" (UID: "edfd16bc8bf4e5fa8ff1f9f5a41bc7fb")
Oct 18 23:22:24 k8s-master kubelet[22922]: I1018 23:22:24.088980   22922 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/edfd16bc8bf4e5fa8ff1f9f5a41bc7fb-kubeconfig") pod "kube-controller-manager-k8s-master" (UID: "edfd16bc8bf4e5fa8ff1f9f5a41bc7fb")
Oct 18 23:22:24 k8s-master kubelet[22922]: I1018 23:22:24.088963   22922 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs" (UniqueName: "kubernetes.io/host-path/edfd16bc8bf4e5fa8ff1f9f5a41bc7fb-ca-certs") pod "kube-controller-manager-k8s-master" (UID: "edfd16bc8bf4e5fa8ff1f9f5a41bc7fb")
Oct 18 23:22:24 k8s-master kubelet[22922]: I1018 23:22:24.088945   22922 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "xtables-lock" (UniqueName: "kubernetes.io/host-path/460f21aa-b411-11e7-b99e-08002782873b-xtables-lock") pod "kube-proxy-2jsvk" (UID: "460f21aa-b411-11e7-b99e-08002782873b")
Oct 18 23:22:24 k8s-master kubelet[22922]: I1018 23:22:24.088926   22922 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-proxy" (UniqueName: "kubernetes.io/configmap/460f21aa-b411-11e7-b99e-08002782873b-kube-proxy") pod "kube-proxy-2jsvk" (UID: "460f21aa-b411-11e7-b99e-08002782873b")
Oct 18 23:22:24 k8s-master kubelet[22922]: I1018 23:22:24.088909   22922 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "etcd" (UniqueName: "kubernetes.io/host-path/40eb0889c614345e2a2714d4ee7d1cc0-etcd") pod "etcd-k8s-master" (UID: "40eb0889c614345e2a2714d4ee7d1cc0")
Oct 18 23:22:24 k8s-master kubelet[22922]: I1018 23:22:24.088890   22922 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs-etc-pki" (UniqueName: "kubernetes.io/host-path/edfd16bc8bf4e5fa8ff1f9f5a41bc7fb-ca-certs-etc-pki") pod "kube-controller-manager-k8s-master" (UID: "edfd16bc8bf4e5fa8ff1f9f5a41bc7fb")
Oct 18 23:22:24 k8s-master kubelet[22922]: I1018 23:22:24.088860   22922 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "flannel-token-w5mq4" (UniqueName: "kubernetes.io/secret/0f66708c-b415-11e7-b99e-08002782873b-flannel-token-w5mq4") pod "kube-flannel-ds-jmhzv" (UID: "0f66708c-b415-11e7-b99e-08002782873b")
Oct 18 23:22:24 k8s-master kubelet[22922]: I1018 23:22:24.088841   22922 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "flannel-cfg" (UniqueName: "kubernetes.io/configmap/0f66708c-b415-11e7-b99e-08002782873b-flannel-cfg") pod "kube-flannel-ds-jmhzv" (UID: "0f66708c-b415-11e7-b99e-08002782873b")
Oct 18 23:22:24 k8s-master kubelet[22922]: I1018 23:22:24.088809   22922 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-dns-config" (UniqueName: "kubernetes.io/configmap/46105de0-b411-11e7-b99e-08002782873b-kube-dns-config") pod "kube-dns-545bc4bfd4-nrjmv" (UID: "46105de0-b411-11e7-b99e-08002782873b")
Oct 18 23:22:24 k8s-master kubelet[22922]: I1018 23:22:24.088784   22922 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-proxy-token-kt8j7" (UniqueName: "kubernetes.io/secret/460f21aa-b411-11e7-b99e-08002782873b-kube-proxy-token-kt8j7") pod "kube-proxy-2jsvk" (UID: "460f21aa-b411-11e7-b99e-08002782873b")
Oct 18 23:22:24 k8s-master kubelet[22922]: I1018 23:22:24.088754   22922 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs-etc-pki" (UniqueName: "kubernetes.io/host-path/091a47ad584983e0f8eaa912823695cf-ca-certs-etc-pki") pod "kube-apiserver-k8s-master" (UID: "091a47ad584983e0f8eaa912823695cf")
Oct 18 23:22:24 k8s-master kubelet[22922]: I1018 23:22:24.088727   22922 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs" (UniqueName: "kubernetes.io/host-path/091a47ad584983e0f8eaa912823695cf-ca-certs") pod "kube-apiserver-k8s-master" (UID: "091a47ad584983e0f8eaa912823695cf")
Oct 18 23:22:24 k8s-master kubelet[22922]: I1018 23:22:24.088708   22922 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/f3e5e477637a31dd77e4d4e3534d2e23-kubeconfig") pod "kube-scheduler-k8s-master" (UID: "f3e5e477637a31dd77e4d4e3534d2e23")
Oct 18 23:22:24 k8s-master kubelet[22922]: I1018 23:22:24.088688   22922 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "cni" (UniqueName: "kubernetes.io/host-path/0f66708c-b415-11e7-b99e-08002782873b-cni") pod "kube-flannel-ds-jmhzv" (UID: "0f66708c-b415-11e7-b99e-08002782873b")
Oct 18 23:22:24 k8s-master kubelet[22922]: I1018 23:22:24.088667   22922 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "run" (UniqueName: "kubernetes.io/host-path/0f66708c-b415-11e7-b99e-08002782873b-run") pod "kube-flannel-ds-jmhzv" (UID: "0f66708c-b415-11e7-b99e-08002782873b")
Oct 18 23:22:24 k8s-master kubelet[22922]: I1018 23:22:24.088649   22922 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-dns-token-s6hnz" (UniqueName: "kubernetes.io/secret/46105de0-b411-11e7-b99e-08002782873b-kube-dns-token-s6hnz") pod "kube-dns-545bc4bfd4-nrjmv" (UID: "46105de0-b411-11e7-b99e-08002782873b")
Oct 18 23:22:24 k8s-master kubelet[22922]: I1018 23:22:24.088589   22922 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "k8s-certs" (UniqueName: "kubernetes.io/host-path/091a47ad584983e0f8eaa912823695cf-k8s-certs") pod "kube-apiserver-k8s-master" (UID: "091a47ad584983e0f8eaa912823695cf")
Oct 18 23:22:24 k8s-master kubelet[22922]: I1018 23:22:24.088540   22922 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "k8s-certs" (UniqueName: "kubernetes.io/host-path/edfd16bc8bf4e5fa8ff1f9f5a41bc7fb-k8s-certs") pod "kube-controller-manager-k8s-master" (UID: "edfd16bc8bf4e5fa8ff1f9f5a41bc7fb")
Oct 18 23:22:19 k8s-master kubelet[22922]: W1018 23:22:19.786293   22922 helpers.go:843] eviction manager: no observation found for eviction signal allocatableNodeFs.available
Oct 18 23:22:19 k8s-master kubelet[22922]: E1018 23:22:19.786254   22922 helpers.go:832] Could not find capacity information for resource ephemeral-storage
Oct 18 23:22:19 k8s-master kubelet[22922]: I1018 23:22:19.607287   22922 manager.go:316] Recovery completed
Oct 18 23:22:19 k8s-master kubelet[22922]: I1018 23:22:19.303059   22922 kubelet_node_status.go:787] Node became not ready: {Type:Ready Status:False LastHeartbeatTime:2017-10-18 23:22:19.303038692 +0800 CST LastTransitionTime:2017-10-18 23:22:19.303038692 +0800 CST Reason:KubeletNotReady Message:container runtime is down}
Oct 18 23:22:19 k8s-master kubelet[22922]: I1018 23:22:19.250169   22922 kubelet_network.go:276] Setting Pod CIDR:  -> 10.244.0.0/24
Oct 18 23:22:19 k8s-master kubelet[22922]: I1018 23:22:19.229525   22922 docker_service.go:306] docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}
Oct 18 23:22:19 k8s-master kubelet[22922]: I1018 23:22:19.229274   22922 kuberuntime_manager.go:898] updating runtime config through cri with podcidr 10.244.0.0/24
Oct 18 23:22:19 k8s-master kubelet[22922]: I1018 23:22:19.221441   22922 kubelet_node_status.go:86] Successfully registered node k8s-master
Oct 18 23:22:19 k8s-master kubelet[22922]: I1018 23:22:19.221413   22922 kubelet_node_status.go:134] Node k8s-master was previously registered
Oct 18 23:22:19 k8s-master kubelet[22922]: I1018 23:22:19.180152   22922 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 23:22:19 k8s-master kubelet[22922]: I1018 23:22:19.097643   22922 manager.go:311] Starting recovery of all containers
Oct 18 23:22:19 k8s-master kubelet[22922]: I1018 23:22:19.093781   22922 manager.go:1140] Started watching for new ooms in manager
Oct 18 23:22:19 k8s-master kubelet[22922]: I1018 23:22:19.093576   22922 factory.go:86] Registering Raw factory
Oct 18 23:22:19 k8s-master kubelet[22922]: I1018 23:22:19.093576   22922 factory.go:54] Registering systemd factory
Oct 18 23:22:19 k8s-master kubelet[22922]: W1018 23:22:19.093576   22922 manager.go:276] Registration of the crio container factory failed: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 23:22:19 k8s-master kubelet[22922]: W1018 23:22:19.093576   22922 manager.go:265] Registration of the rkt container factory failed: unable to communicate with Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 23:22:19 k8s-master kubelet[22922]: I1018 23:22:19.093576   22922 factory.go:355] Registering Docker factory
Oct 18 23:22:19 k8s-master kubelet[22922]: I1018 23:22:19.092917   22922 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.988284   22922 volume_manager.go:246] Starting Kubelet Volume Manager
Oct 18 23:22:18 k8s-master kubelet[22922]: E1018 23:22:18.988254   22922 container_manager_linux.go:603] [ContainerManager]: Fail to get rootfs information unable to find data for container /
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.985701   22922 server.go:296] Adding debug handlers to kubelet server.
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.984934   22922 server.go:128] Starting to listen on 0.0.0.0:10250
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.984815   22922 kubelet.go:1779] skipping pod synchronization - [container runtime is down PLEG is not healthy: pleg was last seen active 2562047h47m16.854775807s ago; threshold is 3m0s]
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.984790   22922 kubelet.go:1768] Starting kubelet main sync loop.
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.984776   22922 status_manager.go:140] Starting to sync pod status with apiserver
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.984750   22922 fs_resource_analyzer.go:66] Starting FS ResourceAnalyzer
Oct 18 23:22:18 k8s-master kubelet[22922]: E1018 23:22:18.982823   22922 kubelet.go:1234] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data for container /
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.981964   22922 server.go:718] Started kubelet v1.8.1
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.966335   22922 kuberuntime_manager.go:177] Container runtime docker initialized, version: 17.10.0-ce, apiVersion: 1.33.0
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.961370   22922 remote_runtime.go:43] Connecting to runtime service unix:///var/run/dockershim.sock
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.914486   22922 docker_service.go:224] Setting cgroupDriver to systemd
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.864152   22922 docker_service.go:207] Docker cri networking managed by kubernetes.io/no-op
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.754861   22922 kubelet.go:517] Hairpin mode set to "hairpin-veth"
Oct 18 23:22:18 k8s-master kubelet[22922]: W1018 23:22:18.754861   22922 kubelet_network.go:69] Hairpin mode set to "promiscuous-bridge" but kubenet is not enabled, falling back to "hairpin-veth"
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.537801   22922 kubelet.go:283] Watching apiserver
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.537780   22922 kubelet.go:273] Adding manifest file: /etc/kubernetes/manifests
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.537677   22922 container_manager_linux.go:288] Creating device plugin handler: false
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.537557   22922 container_manager_linux.go:257] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: ContainerRuntime:docker CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[{Signal:memory.available Operator:LessThan Value:{Quantity:100Mi Percentage:0} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.1} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.inodesFree Operator:LessThan Value:{Quantity:<nil> Percentage:0.05} GracePeriod:0s MinReclaim:<nil>}]} ExperimentalQOSReserved:map[] ExperimentalCPUManagerPolicy:none ExperimentalCPUManagerReconcilePeriod:10s}
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.537528   22922 container_manager_linux.go:252] container manager verified user specified cgroup-root exists: /
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.535689   22922 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.535153   22922 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.530171   22922 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true} {Device:shm DeviceMajor:0 DeviceMinor:44 Capacity:67108864 Type:vfs Inodes:235563 HasInodes:true}] DiskMap:map[8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq} 253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:flannel.1 MacAddress:aa:be:27:bb:bf:3e Speed:0 Mtu:1450} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.527927   22922 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0} shm:{mountpoint:/var/lib/docker/containers/61c7de946f518033b69caded8a9725765b694ee1391c3e9cf83c4fb0bad68790/shm major:0 minor:44 fsType:tmpfs blockSize:0}]
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.527899   22922 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 23:22:18 k8s-master kubelet[22922]: W1018 23:22:18.497863   22922 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 23:22:18 k8s-master kubelet[22922]: W1018 23:22:18.497717   22922 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.438003   22922 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.059549   22922 certificate_manager.go:361] Requesting new certificate.
Oct 18 23:22:18 k8s-master kubelet[22922]: W1018 23:22:18.006114   22922 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 23:22:18 k8s-master kubelet[22922]: I1018 23:22:18.005979   22922 feature_gate.go:156] feature gates: map[]
Oct 18 23:22:17 k8s-master kubelet[22922]: I1018 23:22:17.934477   22922 client.go:95] Start docker client with request timeout=2m0s
Oct 18 23:22:17 k8s-master kubelet[22922]: I1018 23:22:17.934425   22922 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 23:22:17 k8s-master kubelet[22922]: I1018 23:22:17.902331   22922 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 23:22:17 k8s-master kubelet[22922]: I1018 23:22:17.902326   22922 controller.go:114] kubelet config controller: starting controller
Oct 18 23:22:17 k8s-master kubelet[22922]: I1018 23:22:17.902036   22922 feature_gate.go:156] feature gates: map[]
Oct 18 23:22:17 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 23:22:17 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 23:22:17 k8s-master systemd[1]: kubelet.service failed.
Oct 18 23:22:17 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 23:22:17 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 23:22:17 k8s-master systemd[1]: Stopping kubelet: The Kubernetes Node Agent...
Oct 18 23:22:08 k8s-master kubelet[14981]: E1018 23:22:08.591763   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:22:08 k8s-master kubelet[14981]: I1018 23:22:08.591736   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:22:08 k8s-master kubelet[14981]: I1018 23:22:08.591647   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:22:08 k8s-master kubelet[14981]: I1018 23:22:08.591473   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:22:08 k8s-master kubelet[14981]: E1018 23:22:08.064305   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:22:08 k8s-master kubelet[14981]: W1018 23:22:08.064122   14981 container.go:354] Failed to create summary reader for "/libcontainer_22895_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:22:08 k8s-master kubelet[14981]: E1018 23:22:08.062726   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:22:08 k8s-master kubelet[14981]: I1018 23:22:08.062701   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:22:08 k8s-master kubelet[14981]: I1018 23:22:08.062583   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:22:08 k8s-master kubelet[14981]: W1018 23:22:08.031575   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_22895_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_22895_systemd_test_default.slice: no such file or directory
Oct 18 23:22:07 k8s-master kubelet[14981]: I1018 23:22:07.794930   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:22:07 k8s-master kubelet[14981]: I1018 23:22:07.794707   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:22:07 k8s-master kubelet[14981]: I1018 23:22:07.794600   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:21:56 k8s-master kubelet[14981]: ]
Oct 18 23:21:56 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:21:56 k8s-master kubelet[14981]: E1018 23:21:56.797001   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:21:56 k8s-master kubelet[14981]: I1018 23:21:56.796972   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:21:56 k8s-master kubelet[14981]: I1018 23:21:56.796916   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:21:56 k8s-master kubelet[14981]: I1018 23:21:56.796910   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:21:56 k8s-master kubelet[14981]: I1018 23:21:56.796806   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:21:56 k8s-master kubelet[14981]: I1018 23:21:56.796635   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:21:56 k8s-master kubelet[14981]: I1018 23:21:56.796564   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:21:41 k8s-master kubelet[14981]: ]
Oct 18 23:21:41 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:21:41 k8s-master kubelet[14981]: E1018 23:21:41.794959   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:21:41 k8s-master kubelet[14981]: I1018 23:21:41.794914   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:21:41 k8s-master kubelet[14981]: I1018 23:21:41.794821   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:21:41 k8s-master kubelet[14981]: I1018 23:21:41.794815   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:21:41 k8s-master kubelet[14981]: I1018 23:21:41.794710   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:21:41 k8s-master kubelet[14981]: I1018 23:21:41.794503   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:21:41 k8s-master kubelet[14981]: I1018 23:21:41.794424   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:21:27 k8s-master kubelet[14981]: E1018 23:21:27.282218   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:21:26 k8s-master kubelet[14981]: ]
Oct 18 23:21:26 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:21:26 k8s-master kubelet[14981]: E1018 23:21:26.276759   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:21:26 k8s-master kubelet[14981]: I1018 23:21:26.276730   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:21:26 k8s-master kubelet[14981]: I1018 23:21:26.276672   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:21:26 k8s-master kubelet[14981]: I1018 23:21:26.276665   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:21:26 k8s-master kubelet[14981]: I1018 23:21:26.276564   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:21:26 k8s-master kubelet[14981]: I1018 23:21:26.276369   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:21:26 k8s-master kubelet[14981]: I1018 23:21:26.276297   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:21:25 k8s-master kubelet[14981]: ]
Oct 18 23:21:25 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:21:25 k8s-master kubelet[14981]: E1018 23:21:25.259242   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:21:25 k8s-master kubelet[14981]: I1018 23:21:25.259213   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:21:25 k8s-master kubelet[14981]: I1018 23:21:25.259148   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:21:25 k8s-master kubelet[14981]: I1018 23:21:25.259142   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:21:25 k8s-master kubelet[14981]: I1018 23:21:25.259036   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:21:25 k8s-master kubelet[14981]: I1018 23:21:25.258872   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:21:25 k8s-master kubelet[14981]: I1018 23:21:25.258810   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:21:24 k8s-master kubelet[14981]: ]
Oct 18 23:21:24 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:21:24 k8s-master kubelet[14981]: E1018 23:21:24.219880   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:21:24 k8s-master kubelet[14981]: I1018 23:21:24.219844   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:21:24 k8s-master kubelet[14981]: I1018 23:21:24.219788   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:21:24 k8s-master kubelet[14981]: I1018 23:21:24.219780   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:21:24 k8s-master kubelet[14981]: I1018 23:21:24.219595   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:21:24 k8s-master kubelet[14981]: E1018 23:21:24.194548   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:21:24 k8s-master kubelet[14981]: W1018 23:21:24.194415   14981 container.go:354] Failed to create summary reader for "/libcontainer_22745_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:21:24 k8s-master kubelet[14981]: W1018 23:21:24.178981   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_22745_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_22745_systemd_test_default.slice: no such file or directory
Oct 18 23:21:24 k8s-master kubelet[14981]: E1018 23:21:24.101666   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:21:24 k8s-master kubelet[14981]: W1018 23:21:24.101665   14981 container.go:354] Failed to create summary reader for "/libcontainer_22742_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:21:24 k8s-master kubelet[14981]: W1018 23:21:24.099136   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_22742_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_22742_systemd_test_default.slice: no such file or directory
Oct 18 23:21:24 k8s-master kubelet[14981]: E1018 23:21:24.062888   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:21:24 k8s-master kubelet[14981]: W1018 23:21:24.062075   14981 container.go:354] Failed to create summary reader for "/libcontainer_22739_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:21:24 k8s-master kubelet[14981]: W1018 23:21:24.050777   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_22739_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_22739_systemd_test_default.slice: no such file or directory
Oct 18 23:20:53 k8s-master kubelet[14981]: I1018 23:20:53.982603   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:20:52 k8s-master kubelet[14981]: E1018 23:20:52.797269   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:20:52 k8s-master kubelet[14981]: I1018 23:20:52.797269   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:20:52 k8s-master kubelet[14981]: I1018 23:20:52.797269   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:20:52 k8s-master kubelet[14981]: I1018 23:20:52.797059   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:20:37 k8s-master kubelet[14981]: E1018 23:20:37.795476   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:20:37 k8s-master kubelet[14981]: I1018 23:20:37.795444   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:20:37 k8s-master kubelet[14981]: I1018 23:20:37.795346   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:20:37 k8s-master kubelet[14981]: I1018 23:20:37.795089   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:20:31 k8s-master kubelet[14981]: E1018 23:20:31.533194   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:20:22 k8s-master kubelet[14981]: E1018 23:20:22.797875   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:20:22 k8s-master kubelet[14981]: I1018 23:20:22.797848   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:20:22 k8s-master kubelet[14981]: I1018 23:20:22.797754   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:20:22 k8s-master kubelet[14981]: I1018 23:20:22.797502   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:20:19 k8s-master kubelet[14981]: E1018 23:20:19.943839   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:20:09 k8s-master kubelet[14981]: E1018 23:20:09.794692   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:20:09 k8s-master kubelet[14981]: I1018 23:20:09.794669   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:20:09 k8s-master kubelet[14981]: I1018 23:20:09.794570   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:20:09 k8s-master kubelet[14981]: I1018 23:20:09.794345   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:19:58 k8s-master kubelet[14981]: E1018 23:19:58.797277   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:19:58 k8s-master kubelet[14981]: I1018 23:19:58.797251   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:19:58 k8s-master kubelet[14981]: I1018 23:19:58.797148   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:19:58 k8s-master kubelet[14981]: I1018 23:19:58.796899   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:19:43 k8s-master kubelet[14981]: E1018 23:19:43.795105   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:19:43 k8s-master kubelet[14981]: I1018 23:19:43.795073   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:19:43 k8s-master kubelet[14981]: I1018 23:19:43.794941   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:19:43 k8s-master kubelet[14981]: I1018 23:19:43.794668   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:19:31 k8s-master kubelet[14981]: E1018 23:19:31.806578   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:19:31 k8s-master kubelet[14981]: E1018 23:19:31.603703   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:19:29 k8s-master kubelet[14981]: E1018 23:19:29.795393   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:19:29 k8s-master kubelet[14981]: I1018 23:19:29.795364   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:19:29 k8s-master kubelet[14981]: I1018 23:19:29.795267   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:19:29 k8s-master kubelet[14981]: I1018 23:19:29.795014   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:19:28 k8s-master kubelet[14981]: E1018 23:19:28.535789   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:19:27 k8s-master kubelet[14981]: E1018 23:19:27.434156   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:19:27 k8s-master kubelet[14981]: E1018 23:19:27.135654   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:19:16 k8s-master kubelet[14981]: E1018 23:19:16.237073   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:19:16 k8s-master kubelet[14981]: I1018 23:19:16.237073   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:19:16 k8s-master kubelet[14981]: I1018 23:19:16.237073   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:19:16 k8s-master kubelet[14981]: I1018 23:19:16.236832   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:19:15 k8s-master kubelet[14981]: E1018 23:19:15.230737   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:19:15 k8s-master kubelet[14981]: I1018 23:19:15.230712   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:19:15 k8s-master kubelet[14981]: I1018 23:19:15.230609   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:19:15 k8s-master kubelet[14981]: I1018 23:19:15.230348   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:19:13 k8s-master kubelet[14981]: E1018 23:19:13.807741   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:19:13 k8s-master kubelet[14981]: W1018 23:19:13.807516   14981 container.go:354] Failed to create summary reader for "/libcontainer_22246_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:19:13 k8s-master kubelet[14981]: E1018 23:19:13.798774   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:19:13 k8s-master kubelet[14981]: W1018 23:19:13.709175   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_22246_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_22246_systemd_test_default.slice: no such file or directory
Oct 18 23:19:13 k8s-master kubelet[14981]: I1018 23:19:13.347753   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:19:13 k8s-master kubelet[14981]: I1018 23:19:13.347745   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:19:13 k8s-master kubelet[14981]: I1018 23:19:13.347607   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:19:13 k8s-master kubelet[14981]: E1018 23:19:13.336905   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:19:13 k8s-master kubelet[14981]: W1018 23:19:13.336738   14981 container.go:354] Failed to create summary reader for "/libcontainer_22219_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:19:13 k8s-master kubelet[14981]: W1018 23:19:13.304440   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_22219_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_22219_systemd_test_default.slice: no such file or directory
Oct 18 23:19:13 k8s-master kubelet[14981]: E1018 23:19:13.295288   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:19:13 k8s-master kubelet[14981]: W1018 23:19:13.294958   14981 container.go:354] Failed to create summary reader for "/libcontainer_22216_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:19:13 k8s-master kubelet[14981]: W1018 23:19:13.265742   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_22216_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_22216_systemd_test_default.slice: no such file or directory
Oct 18 23:19:13 k8s-master kubelet[14981]: W1018 23:19:13.242845   14981 container.go:354] Failed to create summary reader for "/libcontainer_22213_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:19:13 k8s-master kubelet[14981]: W1018 23:19:13.228635   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_22213_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_22213_systemd_test_default.slice: no such file or directory
Oct 18 23:18:43 k8s-master kubelet[14981]: I1018 23:18:43.168235   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:18:41 k8s-master kubelet[14981]: E1018 23:18:41.795851   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:18:41 k8s-master kubelet[14981]: I1018 23:18:41.795817   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:18:41 k8s-master kubelet[14981]: I1018 23:18:41.795712   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:18:41 k8s-master kubelet[14981]: I1018 23:18:41.795182   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:18:27 k8s-master kubelet[14981]: E1018 23:18:27.795450   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:18:27 k8s-master kubelet[14981]: I1018 23:18:27.795427   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:18:27 k8s-master kubelet[14981]: I1018 23:18:27.795328   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:18:27 k8s-master kubelet[14981]: I1018 23:18:27.795072   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:18:15 k8s-master kubelet[14981]: E1018 23:18:15.794873   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:18:15 k8s-master kubelet[14981]: I1018 23:18:15.794843   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:18:15 k8s-master kubelet[14981]: I1018 23:18:15.794729   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:18:15 k8s-master kubelet[14981]: I1018 23:18:15.794503   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:18:04 k8s-master kubelet[14981]: E1018 23:18:04.797792   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:18:04 k8s-master kubelet[14981]: I1018 23:18:04.797769   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:18:04 k8s-master kubelet[14981]: I1018 23:18:04.797661   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:18:04 k8s-master kubelet[14981]: I1018 23:18:04.797423   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:17:49 k8s-master kubelet[14981]: E1018 23:17:49.806187   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:17:49 k8s-master kubelet[14981]: I1018 23:17:49.806155   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:17:49 k8s-master kubelet[14981]: I1018 23:17:49.806026   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:17:49 k8s-master kubelet[14981]: I1018 23:17:49.802298   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:17:36 k8s-master kubelet[14981]: E1018 23:17:36.797639   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:17:36 k8s-master kubelet[14981]: I1018 23:17:36.797609   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:17:36 k8s-master kubelet[14981]: I1018 23:17:36.797518   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:17:36 k8s-master kubelet[14981]: I1018 23:17:36.797287   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:17:31 k8s-master kubelet[14981]: E1018 23:17:31.095912   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:17:25 k8s-master kubelet[14981]: E1018 23:17:25.795913   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:17:25 k8s-master kubelet[14981]: I1018 23:17:25.795881   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:17:25 k8s-master kubelet[14981]: I1018 23:17:25.795746   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:17:25 k8s-master kubelet[14981]: I1018 23:17:25.795505   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:17:25 k8s-master kubelet[14981]: E1018 23:17:25.360423   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:17:25 k8s-master kubelet[14981]: E1018 23:17:25.053582   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:17:22 k8s-master kubelet[14981]: E1018 23:17:22.779038   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:17:22 k8s-master kubelet[14981]: E1018 23:17:22.715396   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:17:20 k8s-master kubelet[14981]: E1018 23:17:20.041001   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:17:17 k8s-master kubelet[14981]: E1018 23:17:17.882741   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:17:12 k8s-master kubelet[14981]: E1018 23:17:12.994750   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:17:12 k8s-master kubelet[14981]: I1018 23:17:12.994726   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:17:12 k8s-master kubelet[14981]: I1018 23:17:12.994617   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:17:12 k8s-master kubelet[14981]: I1018 23:17:12.994335   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:17:08 k8s-master kubelet[14981]: E1018 23:17:08.065134   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:17:08 k8s-master kubelet[14981]: I1018 23:17:08.065094   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:17:08 k8s-master kubelet[14981]: I1018 23:17:08.064906   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:17:08 k8s-master kubelet[14981]: I1018 23:17:08.064560   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:17:07 k8s-master kubelet[14981]: E1018 23:17:07.351422   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:17:07 k8s-master kubelet[14981]: W1018 23:17:07.351295   14981 container.go:354] Failed to create summary reader for "/libcontainer_21908_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:17:07 k8s-master kubelet[14981]: E1018 23:17:07.349881   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:17:07 k8s-master kubelet[14981]: I1018 23:17:07.043438   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:17:07 k8s-master kubelet[14981]: I1018 23:17:07.043431   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:17:07 k8s-master kubelet[14981]: I1018 23:17:07.043336   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:17:07 k8s-master kubelet[14981]: I1018 23:17:07.043170   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:17:07 k8s-master kubelet[14981]: I1018 23:17:07.043112   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:17:06 k8s-master kubelet[14981]: E1018 23:17:06.462076   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:17:06 k8s-master kubelet[14981]: W1018 23:17:06.462076   14981 container.go:354] Failed to create summary reader for "/libcontainer_21886_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:17:06 k8s-master kubelet[14981]: W1018 23:17:06.458453   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_21886_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_21886_systemd_test_default.slice: no such file or directory
Oct 18 23:17:06 k8s-master kubelet[14981]: E1018 23:17:06.437225   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:17:06 k8s-master kubelet[14981]: W1018 23:17:06.437087   14981 container.go:354] Failed to create summary reader for "/libcontainer_21883_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:17:06 k8s-master kubelet[14981]: W1018 23:17:06.417632   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_21883_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_21883_systemd_test_default.slice: no such file or directory
Oct 18 23:17:06 k8s-master kubelet[14981]: E1018 23:17:06.353997   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:17:06 k8s-master kubelet[14981]: W1018 23:17:06.353843   14981 container.go:354] Failed to create summary reader for "/libcontainer_21879_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:17:06 k8s-master kubelet[14981]: W1018 23:17:06.350032   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_21879_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_21879_systemd_test_default.slice: no such file or directory
Oct 18 23:16:53 k8s-master kubelet[14981]: E1018 23:16:53.795142   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:16:53 k8s-master kubelet[14981]: I1018 23:16:53.795115   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:16:53 k8s-master kubelet[14981]: I1018 23:16:53.795026   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:16:53 k8s-master kubelet[14981]: I1018 23:16:53.794848   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:16:40 k8s-master kubelet[14981]: E1018 23:16:40.799237   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:16:40 k8s-master kubelet[14981]: I1018 23:16:40.799209   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:16:40 k8s-master kubelet[14981]: I1018 23:16:40.799120   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:16:40 k8s-master kubelet[14981]: I1018 23:16:40.798919   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:16:29 k8s-master kubelet[14981]: E1018 23:16:29.797418   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:16:29 k8s-master kubelet[14981]: I1018 23:16:29.797418   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:16:29 k8s-master kubelet[14981]: I1018 23:16:29.797400   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:16:29 k8s-master kubelet[14981]: I1018 23:16:29.797240   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:16:25 k8s-master kubelet[14981]: E1018 23:16:25.707667   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:16:25 k8s-master kubelet[14981]: E1018 23:16:25.647042   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:16:17 k8s-master kubelet[14981]: E1018 23:16:17.802278   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:16:17 k8s-master kubelet[14981]: I1018 23:16:17.802250   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:16:17 k8s-master kubelet[14981]: I1018 23:16:17.802150   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:16:17 k8s-master kubelet[14981]: I1018 23:16:17.801891   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:16:06 k8s-master kubelet[14981]: E1018 23:16:06.582060   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:16:06 k8s-master kubelet[14981]: I1018 23:16:06.582030   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:16:06 k8s-master kubelet[14981]: I1018 23:16:06.581943   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:16:06 k8s-master kubelet[14981]: I1018 23:16:06.581636   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:16:06 k8s-master kubelet[14981]: E1018 23:16:06.199318   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:16:06 k8s-master kubelet[14981]: W1018 23:16:06.199224   14981 container.go:354] Failed to create summary reader for "/libcontainer_21707_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:16:06 k8s-master kubelet[14981]: E1018 23:16:06.197816   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:16:06 k8s-master kubelet[14981]: I1018 23:16:06.197780   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:16:06 k8s-master kubelet[14981]: I1018 23:16:06.197626   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:16:06 k8s-master kubelet[14981]: E1018 23:16:06.052391   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:16:06 k8s-master kubelet[14981]: W1018 23:16:06.052272   14981 container.go:354] Failed to create summary reader for "/libcontainer_21693_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:16:06 k8s-master kubelet[14981]: W1018 23:16:06.023888   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_21693_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_21693_systemd_test_default.slice: no such file or directory
Oct 18 23:16:05 k8s-master kubelet[14981]: I1018 23:16:05.798395   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:16:05 k8s-master kubelet[14981]: I1018 23:16:05.798220   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:16:05 k8s-master kubelet[14981]: I1018 23:16:05.798151   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:15:50 k8s-master kubelet[14981]: ]
Oct 18 23:15:50 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:15:50 k8s-master kubelet[14981]: E1018 23:15:50.798490   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:15:50 k8s-master kubelet[14981]: I1018 23:15:50.798461   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:15:50 k8s-master kubelet[14981]: I1018 23:15:50.798403   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:15:50 k8s-master kubelet[14981]: I1018 23:15:50.798397   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:15:50 k8s-master kubelet[14981]: I1018 23:15:50.798255   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:15:50 k8s-master kubelet[14981]: I1018 23:15:50.798085   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:15:50 k8s-master kubelet[14981]: I1018 23:15:50.798026   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:15:35 k8s-master kubelet[14981]: ]
Oct 18 23:15:35 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:15:35 k8s-master kubelet[14981]: E1018 23:15:35.796650   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:15:35 k8s-master kubelet[14981]: I1018 23:15:35.796619   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:15:35 k8s-master kubelet[14981]: I1018 23:15:35.796564   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:15:35 k8s-master kubelet[14981]: I1018 23:15:35.796557   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:15:35 k8s-master kubelet[14981]: I1018 23:15:35.796425   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:15:35 k8s-master kubelet[14981]: I1018 23:15:35.796251   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:15:35 k8s-master kubelet[14981]: I1018 23:15:35.796176   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:15:22 k8s-master kubelet[14981]: ]
Oct 18 23:15:22 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:15:22 k8s-master kubelet[14981]: E1018 23:15:22.806173   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:15:22 k8s-master kubelet[14981]: I1018 23:15:22.806142   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:15:22 k8s-master kubelet[14981]: I1018 23:15:22.806082   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:15:22 k8s-master kubelet[14981]: I1018 23:15:22.806076   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:15:22 k8s-master kubelet[14981]: I1018 23:15:22.805971   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:15:22 k8s-master kubelet[14981]: I1018 23:15:22.805807   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:15:22 k8s-master kubelet[14981]: I1018 23:15:22.805738   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:15:08 k8s-master kubelet[14981]: ]
Oct 18 23:15:08 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:15:08 k8s-master kubelet[14981]: E1018 23:15:08.798596   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:15:08 k8s-master kubelet[14981]: I1018 23:15:08.798484   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:15:08 k8s-master kubelet[14981]: I1018 23:15:08.798383   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:15:08 k8s-master kubelet[14981]: I1018 23:15:08.798365   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:15:08 k8s-master kubelet[14981]: I1018 23:15:08.797708   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:15:08 k8s-master kubelet[14981]: I1018 23:15:08.797455   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:15:08 k8s-master kubelet[14981]: I1018 23:15:08.797350   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:14:56 k8s-master kubelet[14981]: ]
Oct 18 23:14:56 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:14:56 k8s-master kubelet[14981]: E1018 23:14:56.798320   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:14:56 k8s-master kubelet[14981]: I1018 23:14:56.798266   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:14:56 k8s-master kubelet[14981]: I1018 23:14:56.798163   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:14:56 k8s-master kubelet[14981]: I1018 23:14:56.798147   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:14:56 k8s-master kubelet[14981]: I1018 23:14:56.797984   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:14:56 k8s-master kubelet[14981]: I1018 23:14:56.797708   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:14:56 k8s-master kubelet[14981]: I1018 23:14:56.797611   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:14:42 k8s-master kubelet[14981]: ]
Oct 18 23:14:42 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:14:42 k8s-master kubelet[14981]: E1018 23:14:42.798221   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:14:42 k8s-master kubelet[14981]: I1018 23:14:42.798171   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:14:42 k8s-master kubelet[14981]: I1018 23:14:42.798091   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:14:42 k8s-master kubelet[14981]: I1018 23:14:42.798084   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:14:42 k8s-master kubelet[14981]: I1018 23:14:42.797975   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:14:42 k8s-master kubelet[14981]: I1018 23:14:42.797808   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:14:42 k8s-master kubelet[14981]: I1018 23:14:42.797729   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:14:31 k8s-master kubelet[14981]: ]
Oct 18 23:14:31 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:14:31 k8s-master kubelet[14981]: E1018 23:14:31.795509   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:14:31 k8s-master kubelet[14981]: I1018 23:14:31.795477   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:14:31 k8s-master kubelet[14981]: I1018 23:14:31.795419   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:14:31 k8s-master kubelet[14981]: I1018 23:14:31.795412   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:14:31 k8s-master kubelet[14981]: I1018 23:14:31.795297   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:14:31 k8s-master kubelet[14981]: I1018 23:14:31.795105   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:14:31 k8s-master kubelet[14981]: I1018 23:14:31.795032   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:14:20 k8s-master kubelet[14981]: ]
Oct 18 23:14:20 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:14:20 k8s-master kubelet[14981]: E1018 23:14:20.798125   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:14:20 k8s-master kubelet[14981]: I1018 23:14:20.798091   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:14:20 k8s-master kubelet[14981]: I1018 23:14:20.798024   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:14:20 k8s-master kubelet[14981]: I1018 23:14:20.798017   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:14:20 k8s-master kubelet[14981]: I1018 23:14:20.797898   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:14:20 k8s-master kubelet[14981]: I1018 23:14:20.797699   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:14:20 k8s-master kubelet[14981]: I1018 23:14:20.797600   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:14:08 k8s-master kubelet[14981]: ]
Oct 18 23:14:08 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:14:08 k8s-master kubelet[14981]: E1018 23:14:08.797983   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:14:08 k8s-master kubelet[14981]: I1018 23:14:08.797956   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:14:08 k8s-master kubelet[14981]: I1018 23:14:08.797901   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:14:08 k8s-master kubelet[14981]: I1018 23:14:08.797894   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:14:08 k8s-master kubelet[14981]: I1018 23:14:08.797786   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:14:08 k8s-master kubelet[14981]: I1018 23:14:08.797603   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:14:08 k8s-master kubelet[14981]: I1018 23:14:08.797546   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:13:56 k8s-master kubelet[14981]: ]
Oct 18 23:13:56 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:13:56 k8s-master kubelet[14981]: E1018 23:13:56.798807   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:13:56 k8s-master kubelet[14981]: I1018 23:13:56.798774   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:13:56 k8s-master kubelet[14981]: I1018 23:13:56.798717   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:13:56 k8s-master kubelet[14981]: I1018 23:13:56.798710   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:13:56 k8s-master kubelet[14981]: I1018 23:13:56.798610   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:13:56 k8s-master kubelet[14981]: I1018 23:13:56.798444   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:13:56 k8s-master kubelet[14981]: I1018 23:13:56.798386   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:13:44 k8s-master kubelet[14981]: ]
Oct 18 23:13:44 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:13:44 k8s-master kubelet[14981]: E1018 23:13:44.797525   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:13:44 k8s-master kubelet[14981]: I1018 23:13:44.797482   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:13:44 k8s-master kubelet[14981]: I1018 23:13:44.797396   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:13:44 k8s-master kubelet[14981]: I1018 23:13:44.797389   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:13:44 k8s-master kubelet[14981]: I1018 23:13:44.797277   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:13:44 k8s-master kubelet[14981]: I1018 23:13:44.797099   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:13:44 k8s-master kubelet[14981]: I1018 23:13:44.797040   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:13:33 k8s-master kubelet[14981]: ]
Oct 18 23:13:33 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:13:33 k8s-master kubelet[14981]: E1018 23:13:33.797486   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:13:33 k8s-master kubelet[14981]: I1018 23:13:33.797445   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:13:33 k8s-master kubelet[14981]: I1018 23:13:33.797371   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:13:33 k8s-master kubelet[14981]: I1018 23:13:33.797364   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:13:33 k8s-master kubelet[14981]: I1018 23:13:33.797255   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:13:33 k8s-master kubelet[14981]: I1018 23:13:33.797065   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:13:33 k8s-master kubelet[14981]: I1018 23:13:33.796960   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:13:32 k8s-master kubelet[14981]: E1018 23:13:32.919325   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:13:21 k8s-master kubelet[14981]: ]
Oct 18 23:13:21 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:13:21 k8s-master kubelet[14981]: E1018 23:13:21.795345   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:13:21 k8s-master kubelet[14981]: I1018 23:13:21.795297   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:13:21 k8s-master kubelet[14981]: I1018 23:13:21.795196   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:13:21 k8s-master kubelet[14981]: I1018 23:13:21.795184   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:13:21 k8s-master kubelet[14981]: I1018 23:13:21.794992   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:13:21 k8s-master kubelet[14981]: I1018 23:13:21.794712   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:13:21 k8s-master kubelet[14981]: I1018 23:13:21.794604   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:13:15 k8s-master kubelet[14981]: E1018 23:13:15.540699   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:13:07 k8s-master kubelet[14981]: ]
Oct 18 23:13:07 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:13:07 k8s-master kubelet[14981]: E1018 23:13:07.794991   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:13:07 k8s-master kubelet[14981]: I1018 23:13:07.794960   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:13:07 k8s-master kubelet[14981]: I1018 23:13:07.794902   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:13:07 k8s-master kubelet[14981]: I1018 23:13:07.794893   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:13:07 k8s-master kubelet[14981]: I1018 23:13:07.794767   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:13:07 k8s-master kubelet[14981]: I1018 23:13:07.794480   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:13:07 k8s-master kubelet[14981]: I1018 23:13:07.794382   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:12:54 k8s-master kubelet[14981]: E1018 23:12:54.510596   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:12:53 k8s-master kubelet[14981]: ]
Oct 18 23:12:53 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:53 k8s-master kubelet[14981]: E1018 23:12:53.794858   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:53 k8s-master kubelet[14981]: I1018 23:12:53.794824   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:12:53 k8s-master kubelet[14981]: I1018 23:12:53.794763   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:53 k8s-master kubelet[14981]: I1018 23:12:53.794755   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:12:53 k8s-master kubelet[14981]: I1018 23:12:53.794632   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:53 k8s-master kubelet[14981]: I1018 23:12:53.794430   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:12:53 k8s-master kubelet[14981]: I1018 23:12:53.794369   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:12:41 k8s-master kubelet[14981]: ]
Oct 18 23:12:41 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:41 k8s-master kubelet[14981]: E1018 23:12:41.795967   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:41 k8s-master kubelet[14981]: I1018 23:12:41.795926   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:12:41 k8s-master kubelet[14981]: I1018 23:12:41.795860   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:41 k8s-master kubelet[14981]: I1018 23:12:41.795852   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:12:41 k8s-master kubelet[14981]: I1018 23:12:41.795712   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:41 k8s-master kubelet[14981]: I1018 23:12:41.795508   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:12:41 k8s-master kubelet[14981]: I1018 23:12:41.794675   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:12:41 k8s-master kubelet[14981]: E1018 23:12:41.557878   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:12:32 k8s-master kubelet[14981]: E1018 23:12:32.195655   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:12:31 k8s-master kubelet[14981]: E1018 23:12:31.762217   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:12:28 k8s-master kubelet[14981]: ]
Oct 18 23:12:28 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:28 k8s-master kubelet[14981]: E1018 23:12:28.798113   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:28 k8s-master kubelet[14981]: I1018 23:12:28.798080   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:12:28 k8s-master kubelet[14981]: I1018 23:12:28.798019   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:28 k8s-master kubelet[14981]: I1018 23:12:28.798013   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:12:28 k8s-master kubelet[14981]: I1018 23:12:28.797912   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:28 k8s-master kubelet[14981]: I1018 23:12:28.797791   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:12:28 k8s-master kubelet[14981]: I1018 23:12:28.797728   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:12:28 k8s-master kubelet[14981]: E1018 23:12:28.026766   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:12:18 k8s-master kubelet[14981]: E1018 23:12:18.104080   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:12:17 k8s-master kubelet[14981]: E1018 23:12:17.049058   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:12:16 k8s-master kubelet[14981]: ]
Oct 18 23:12:16 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:16 k8s-master kubelet[14981]: E1018 23:12:16.797393   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:16 k8s-master kubelet[14981]: I1018 23:12:16.797364   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:12:16 k8s-master kubelet[14981]: I1018 23:12:16.797299   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:16 k8s-master kubelet[14981]: I1018 23:12:16.797292   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:12:16 k8s-master kubelet[14981]: I1018 23:12:16.797188   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:16 k8s-master kubelet[14981]: I1018 23:12:16.796998   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:12:16 k8s-master kubelet[14981]: I1018 23:12:16.796939   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:12:14 k8s-master kubelet[14981]: E1018 23:12:14.842389   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:12:05 k8s-master kubelet[14981]: ]
Oct 18 23:12:05 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:05 k8s-master kubelet[14981]: E1018 23:12:05.762825   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:05 k8s-master kubelet[14981]: I1018 23:12:05.762788   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:12:05 k8s-master kubelet[14981]: I1018 23:12:05.762665   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:05 k8s-master kubelet[14981]: I1018 23:12:05.762648   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:12:05 k8s-master kubelet[14981]: I1018 23:12:05.762514   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:05 k8s-master kubelet[14981]: I1018 23:12:05.762328   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:12:05 k8s-master kubelet[14981]: I1018 23:12:05.762261   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:12:04 k8s-master kubelet[14981]: ]
Oct 18 23:12:04 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:04 k8s-master kubelet[14981]: E1018 23:12:04.731414   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:04 k8s-master kubelet[14981]: I1018 23:12:04.731385   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:12:04 k8s-master kubelet[14981]: I1018 23:12:04.731330   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:04 k8s-master kubelet[14981]: I1018 23:12:04.731324   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:12:04 k8s-master kubelet[14981]: I1018 23:12:04.731209   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:04 k8s-master kubelet[14981]: I1018 23:12:04.731137   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:12:04 k8s-master kubelet[14981]: I1018 23:12:04.731077   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:12:03 k8s-master kubelet[14981]: E1018 23:12:03.495541   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:12:03 k8s-master kubelet[14981]: ]
Oct 18 23:12:03 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:03 k8s-master kubelet[14981]: E1018 23:12:03.494906   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:03 k8s-master kubelet[14981]: I1018 23:12:03.494842   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:12:03 k8s-master kubelet[14981]: I1018 23:12:03.494771   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:03 k8s-master kubelet[14981]: I1018 23:12:03.494763   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:12:03 k8s-master kubelet[14981]: I1018 23:12:03.494354   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:12:03 k8s-master kubelet[14981]: E1018 23:12:03.321037   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:12:03 k8s-master kubelet[14981]: W1018 23:12:03.320874   14981 container.go:354] Failed to create summary reader for "/libcontainer_21070_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:12:03 k8s-master kubelet[14981]: W1018 23:12:03.309500   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_21070_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_21070_systemd_test_default.slice: no such file or directory
Oct 18 23:12:03 k8s-master kubelet[14981]: W1018 23:12:03.263723   14981 container.go:354] Failed to create summary reader for "/libcontainer_21065_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:12:03 k8s-master kubelet[14981]: W1018 23:12:03.251555   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_21065_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_21065_systemd_test_default.slice: no such file or directory
Oct 18 23:11:33 k8s-master kubelet[14981]: E1018 23:11:33.878322   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:11:33 k8s-master kubelet[14981]: I1018 23:11:33.168291   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:11:30 k8s-master kubelet[14981]: E1018 23:11:30.633515   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:11:29 k8s-master kubelet[14981]: E1018 23:11:29.794841   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:11:29 k8s-master kubelet[14981]: I1018 23:11:29.794815   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:11:29 k8s-master kubelet[14981]: I1018 23:11:29.794708   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:11:29 k8s-master kubelet[14981]: I1018 23:11:29.794421   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:11:28 k8s-master kubelet[14981]: E1018 23:11:28.015210   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:11:22 k8s-master kubelet[14981]: E1018 23:11:22.418724   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:11:17 k8s-master kubelet[14981]: E1018 23:11:17.794647   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:11:17 k8s-master kubelet[14981]: I1018 23:11:17.794623   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:11:17 k8s-master kubelet[14981]: I1018 23:11:17.794529   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:11:17 k8s-master kubelet[14981]: I1018 23:11:17.794284   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:11:11 k8s-master kubelet[14981]: E1018 23:11:11.323807   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:11:08 k8s-master kubelet[14981]: E1018 23:11:08.616278   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:11:05 k8s-master kubelet[14981]: E1018 23:11:05.223453   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:11:04 k8s-master kubelet[14981]: E1018 23:11:04.912623   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:11:02 k8s-master kubelet[14981]: E1018 23:11:02.992955   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:11:02 k8s-master kubelet[14981]: I1018 23:11:02.992934   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:11:02 k8s-master kubelet[14981]: I1018 23:11:02.992842   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:11:02 k8s-master kubelet[14981]: I1018 23:11:02.992623   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:10:55 k8s-master kubelet[14981]: E1018 23:10:55.203201   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:10:55 k8s-master kubelet[14981]: I1018 23:10:55.203123   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:10:55 k8s-master kubelet[14981]: I1018 23:10:55.202870   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:10:55 k8s-master kubelet[14981]: I1018 23:10:55.201218   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:10:53 k8s-master kubelet[14981]: E1018 23:10:53.854781   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:10:53 k8s-master kubelet[14981]: W1018 23:10:53.854649   14981 container.go:354] Failed to create summary reader for "/libcontainer_20896_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:10:53 k8s-master kubelet[14981]: W1018 23:10:53.852786   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_20896_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_20896_systemd_test_default.slice: no such file or directory
Oct 18 23:10:53 k8s-master kubelet[14981]: E1018 23:10:53.823370   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:10:53 k8s-master kubelet[14981]: W1018 23:10:53.823209   14981 container.go:354] Failed to create summary reader for "/libcontainer_20893_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:10:53 k8s-master kubelet[14981]: W1018 23:10:53.809057   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_20893_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_20893_systemd_test_default.slice: no such file or directory
Oct 18 23:10:53 k8s-master kubelet[14981]: E1018 23:10:53.794597   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:10:53 k8s-master kubelet[14981]: W1018 23:10:53.794410   14981 container.go:354] Failed to create summary reader for "/libcontainer_20890_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:10:53 k8s-master kubelet[14981]: W1018 23:10:53.775678   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_20890_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_20890_systemd_test_default.slice: no such file or directory
Oct 18 23:10:33 k8s-master kubelet[14981]: E1018 23:10:33.385382   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:10:31 k8s-master kubelet[14981]: E1018 23:10:31.792228   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:10:28 k8s-master kubelet[14981]: E1018 23:10:28.976927   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:10:27 k8s-master kubelet[14981]: E1018 23:10:27.964869   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:10:26 k8s-master kubelet[14981]: E1018 23:10:26.697995   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:10:24 k8s-master kubelet[14981]: E1018 23:10:24.719888   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:10:24 k8s-master kubelet[14981]: E1018 23:10:24.047550   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:10:13 k8s-master kubelet[14981]: E1018 23:10:13.721465   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:10:13 k8s-master kubelet[14981]: E1018 23:10:13.201551   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:10:12 k8s-master kubelet[14981]: E1018 23:10:12.746219   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:10:12 k8s-master kubelet[14981]: E1018 23:10:12.365731   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:10:11 k8s-master kubelet[14981]: E1018 23:10:11.452823   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:10:08 k8s-master kubelet[14981]: E1018 23:10:08.412244   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:10:07 k8s-master kubelet[14981]: E1018 23:10:07.904258   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:10:06 k8s-master kubelet[14981]: E1018 23:10:06.918416   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:10:06 k8s-master kubelet[14981]: E1018 23:10:06.853843   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:09:58 k8s-master kubelet[14981]: E1018 23:09:58.872608   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:09:53 k8s-master kubelet[14981]: E1018 23:09:53.913042   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:09:53 k8s-master kubelet[14981]: W1018 23:09:53.912947   14981 container.go:354] Failed to create summary reader for "/libcontainer_20597_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:09:53 k8s-master kubelet[14981]: E1018 23:09:53.603350   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:09:53 k8s-master kubelet[14981]: W1018 23:09:53.603247   14981 container.go:354] Failed to create summary reader for "/libcontainer_20565_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:09:53 k8s-master kubelet[14981]: I1018 23:09:53.602139   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:09:53 k8s-master kubelet[14981]: I1018 23:09:53.318470   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:09:53 k8s-master kubelet[14981]: E1018 23:09:53.293284   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:09:53 k8s-master kubelet[14981]: W1018 23:09:53.293098   14981 container.go:354] Failed to create summary reader for "/libcontainer_20543_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:09:53 k8s-master kubelet[14981]: W1018 23:09:53.290094   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_20543_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_20543_systemd_test_default.slice: no such file or directory
Oct 18 23:09:53 k8s-master kubelet[14981]: E1018 23:09:53.263760   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:09:53 k8s-master kubelet[14981]: W1018 23:09:53.263626   14981 container.go:354] Failed to create summary reader for "/libcontainer_20540_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:09:53 k8s-master kubelet[14981]: W1018 23:09:53.249014   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_20540_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_20540_systemd_test_default.slice: no such file or directory
Oct 18 23:09:53 k8s-master kubelet[14981]: E1018 23:09:53.215516   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:09:53 k8s-master kubelet[14981]: W1018 23:09:53.215321   14981 container.go:354] Failed to create summary reader for "/libcontainer_20536_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:09:53 k8s-master kubelet[14981]: W1018 23:09:53.204162   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_20536_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_20536_systemd_test_default.slice: no such file or directory
Oct 18 23:09:23 k8s-master kubelet[14981]: I1018 23:09:23.168210   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:09:13 k8s-master kubelet[14981]: E1018 23:09:13.794797   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:09:13 k8s-master kubelet[14981]: I1018 23:09:13.794756   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:09:13 k8s-master kubelet[14981]: I1018 23:09:13.794602   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:09:13 k8s-master kubelet[14981]: I1018 23:09:13.794362   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:09:02 k8s-master kubelet[14981]: E1018 23:09:02.798708   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:09:02 k8s-master kubelet[14981]: I1018 23:09:02.798677   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:09:02 k8s-master kubelet[14981]: I1018 23:09:02.798517   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:09:02 k8s-master kubelet[14981]: I1018 23:09:02.797857   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:08:49 k8s-master kubelet[14981]: E1018 23:08:49.794971   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:08:49 k8s-master kubelet[14981]: I1018 23:08:49.794941   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:08:49 k8s-master kubelet[14981]: I1018 23:08:49.794809   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:08:49 k8s-master kubelet[14981]: I1018 23:08:49.794422   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:08:37 k8s-master kubelet[14981]: E1018 23:08:37.794893   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:08:37 k8s-master kubelet[14981]: I1018 23:08:37.794864   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:08:37 k8s-master kubelet[14981]: I1018 23:08:37.794732   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:08:37 k8s-master kubelet[14981]: I1018 23:08:37.794472   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:08:24 k8s-master kubelet[14981]: E1018 23:08:24.782327   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:08:22 k8s-master kubelet[14981]: E1018 23:08:22.797838   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:08:22 k8s-master kubelet[14981]: I1018 23:08:22.797801   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:08:22 k8s-master kubelet[14981]: I1018 23:08:22.797643   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:08:22 k8s-master kubelet[14981]: I1018 23:08:22.797306   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:08:10 k8s-master kubelet[14981]: E1018 23:08:10.797922   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:08:10 k8s-master kubelet[14981]: I1018 23:08:10.797897   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:08:10 k8s-master kubelet[14981]: I1018 23:08:10.797775   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:08:10 k8s-master kubelet[14981]: I1018 23:08:10.797508   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:07:56 k8s-master kubelet[14981]: E1018 23:07:56.797022   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:07:56 k8s-master kubelet[14981]: I1018 23:07:56.796995   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:07:56 k8s-master kubelet[14981]: I1018 23:07:56.796887   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:07:56 k8s-master kubelet[14981]: I1018 23:07:56.796624   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:07:55 k8s-master kubelet[14981]: E1018 23:07:55.173238   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:07:44 k8s-master kubelet[14981]: E1018 23:07:44.668739   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:07:44 k8s-master kubelet[14981]: I1018 23:07:44.668707   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:07:44 k8s-master kubelet[14981]: I1018 23:07:44.668596   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:07:44 k8s-master kubelet[14981]: I1018 23:07:44.668321   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:07:44 k8s-master kubelet[14981]: E1018 23:07:44.081038   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:07:44 k8s-master kubelet[14981]: W1018 23:07:44.080836   14981 container.go:354] Failed to create summary reader for "/libcontainer_20242_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:07:44 k8s-master kubelet[14981]: E1018 23:07:44.078434   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:07:43 k8s-master kubelet[14981]: W1018 23:07:43.916011   14981 container.go:354] Failed to create summary reader for "/libcontainer_20229_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:07:43 k8s-master kubelet[14981]: W1018 23:07:43.914737   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_20229_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_20229_systemd_test_default.slice: no such file or directory
Oct 18 23:07:43 k8s-master kubelet[14981]: I1018 23:07:43.794788   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:07:43 k8s-master kubelet[14981]: I1018 23:07:43.794781   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:07:43 k8s-master kubelet[14981]: I1018 23:07:43.794665   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:07:43 k8s-master kubelet[14981]: I1018 23:07:43.794465   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:07:43 k8s-master kubelet[14981]: I1018 23:07:43.794378   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:07:29 k8s-master kubelet[14981]: ]
Oct 18 23:07:29 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:07:29 k8s-master kubelet[14981]: E1018 23:07:29.794760   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:07:29 k8s-master kubelet[14981]: I1018 23:07:29.794717   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:07:29 k8s-master kubelet[14981]: I1018 23:07:29.794661   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:07:29 k8s-master kubelet[14981]: I1018 23:07:29.794654   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:07:29 k8s-master kubelet[14981]: I1018 23:07:29.794546   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:07:29 k8s-master kubelet[14981]: I1018 23:07:29.794374   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:07:29 k8s-master kubelet[14981]: I1018 23:07:29.794305   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:07:17 k8s-master kubelet[14981]: ]
Oct 18 23:07:17 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:07:17 k8s-master kubelet[14981]: E1018 23:07:17.800047   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:07:17 k8s-master kubelet[14981]: I1018 23:07:17.800017   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:07:17 k8s-master kubelet[14981]: I1018 23:07:17.799960   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:07:17 k8s-master kubelet[14981]: I1018 23:07:17.799952   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:07:17 k8s-master kubelet[14981]: I1018 23:07:17.799848   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:07:17 k8s-master kubelet[14981]: I1018 23:07:17.799670   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:07:17 k8s-master kubelet[14981]: I1018 23:07:17.799575   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:07:02 k8s-master kubelet[14981]: ]
Oct 18 23:07:02 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:07:02 k8s-master kubelet[14981]: E1018 23:07:02.804424   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:07:02 k8s-master kubelet[14981]: I1018 23:07:02.804395   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:07:02 k8s-master kubelet[14981]: I1018 23:07:02.804336   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:07:02 k8s-master kubelet[14981]: I1018 23:07:02.804328   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:07:02 k8s-master kubelet[14981]: I1018 23:07:02.804206   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:07:02 k8s-master kubelet[14981]: I1018 23:07:02.804102   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:07:02 k8s-master kubelet[14981]: I1018 23:07:02.804050   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:06:50 k8s-master kubelet[14981]: ]
Oct 18 23:06:50 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:06:50 k8s-master kubelet[14981]: E1018 23:06:50.797959   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:06:50 k8s-master kubelet[14981]: I1018 23:06:50.797928   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:06:50 k8s-master kubelet[14981]: I1018 23:06:50.797857   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:06:50 k8s-master kubelet[14981]: I1018 23:06:50.797850   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:06:50 k8s-master kubelet[14981]: I1018 23:06:50.797726   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:06:50 k8s-master kubelet[14981]: I1018 23:06:50.797544   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:06:50 k8s-master kubelet[14981]: I1018 23:06:50.797469   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:06:38 k8s-master kubelet[14981]: ]
Oct 18 23:06:38 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:06:38 k8s-master kubelet[14981]: E1018 23:06:38.798345   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:06:38 k8s-master kubelet[14981]: I1018 23:06:38.798307   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:06:38 k8s-master kubelet[14981]: I1018 23:06:38.798195   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:06:38 k8s-master kubelet[14981]: I1018 23:06:38.798188   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:06:38 k8s-master kubelet[14981]: I1018 23:06:38.798075   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:06:38 k8s-master kubelet[14981]: I1018 23:06:38.797897   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:06:38 k8s-master kubelet[14981]: I1018 23:06:38.797834   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:06:22 k8s-master kubelet[14981]: ]
Oct 18 23:06:22 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:06:22 k8s-master kubelet[14981]: E1018 23:06:22.798541   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:06:22 k8s-master kubelet[14981]: I1018 23:06:22.798504   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:06:22 k8s-master kubelet[14981]: I1018 23:06:22.798413   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:06:22 k8s-master kubelet[14981]: I1018 23:06:22.798404   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:06:22 k8s-master kubelet[14981]: I1018 23:06:22.798277   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:06:22 k8s-master kubelet[14981]: I1018 23:06:22.798084   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:06:22 k8s-master kubelet[14981]: I1018 23:06:22.798003   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:06:11 k8s-master kubelet[14981]: ]
Oct 18 23:06:11 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:06:11 k8s-master kubelet[14981]: E1018 23:06:11.794996   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:06:11 k8s-master kubelet[14981]: I1018 23:06:11.794966   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:06:11 k8s-master kubelet[14981]: I1018 23:06:11.794909   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:06:11 k8s-master kubelet[14981]: I1018 23:06:11.794902   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:06:11 k8s-master kubelet[14981]: I1018 23:06:11.794798   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:06:11 k8s-master kubelet[14981]: I1018 23:06:11.794612   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:06:11 k8s-master kubelet[14981]: I1018 23:06:11.794540   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:05:59 k8s-master kubelet[14981]: ]
Oct 18 23:05:59 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:05:59 k8s-master kubelet[14981]: E1018 23:05:59.794783   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:05:59 k8s-master kubelet[14981]: I1018 23:05:59.794751   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:05:59 k8s-master kubelet[14981]: I1018 23:05:59.794681   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:05:59 k8s-master kubelet[14981]: I1018 23:05:59.794673   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:05:59 k8s-master kubelet[14981]: I1018 23:05:59.794541   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:05:59 k8s-master kubelet[14981]: I1018 23:05:59.794327   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:05:59 k8s-master kubelet[14981]: I1018 23:05:59.794239   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:05:44 k8s-master kubelet[14981]: ]
Oct 18 23:05:44 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:05:44 k8s-master kubelet[14981]: E1018 23:05:44.797705   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:05:44 k8s-master kubelet[14981]: I1018 23:05:44.797666   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:05:44 k8s-master kubelet[14981]: I1018 23:05:44.797584   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:05:44 k8s-master kubelet[14981]: I1018 23:05:44.797575   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:05:44 k8s-master kubelet[14981]: I1018 23:05:44.797432   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:05:44 k8s-master kubelet[14981]: I1018 23:05:44.797232   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:05:44 k8s-master kubelet[14981]: I1018 23:05:44.797153   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:05:28 k8s-master kubelet[14981]: ]
Oct 18 23:05:28 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:05:28 k8s-master kubelet[14981]: E1018 23:05:28.798128   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:05:28 k8s-master kubelet[14981]: I1018 23:05:28.798080   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:05:28 k8s-master kubelet[14981]: I1018 23:05:28.798007   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:05:28 k8s-master kubelet[14981]: I1018 23:05:28.797993   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:05:28 k8s-master kubelet[14981]: I1018 23:05:28.797781   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:05:28 k8s-master kubelet[14981]: I1018 23:05:28.797522   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:05:28 k8s-master kubelet[14981]: I1018 23:05:28.797438   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:05:15 k8s-master kubelet[14981]: ]
Oct 18 23:05:15 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:05:15 k8s-master kubelet[14981]: E1018 23:05:15.794773   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:05:15 k8s-master kubelet[14981]: I1018 23:05:15.794741   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:05:15 k8s-master kubelet[14981]: I1018 23:05:15.794670   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:05:15 k8s-master kubelet[14981]: I1018 23:05:15.794662   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:05:15 k8s-master kubelet[14981]: I1018 23:05:15.794542   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:05:15 k8s-master kubelet[14981]: I1018 23:05:15.794333   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:05:15 k8s-master kubelet[14981]: I1018 23:05:15.794263   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:05:00 k8s-master kubelet[14981]: ]
Oct 18 23:05:00 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:05:00 k8s-master kubelet[14981]: E1018 23:05:00.798902   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:05:00 k8s-master kubelet[14981]: I1018 23:05:00.798892   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:05:00 k8s-master kubelet[14981]: I1018 23:05:00.798772   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:05:00 k8s-master kubelet[14981]: I1018 23:05:00.798748   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:05:00 k8s-master kubelet[14981]: I1018 23:05:00.798550   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:05:00 k8s-master kubelet[14981]: I1018 23:05:00.798238   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:05:00 k8s-master kubelet[14981]: I1018 23:05:00.798113   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:04:45 k8s-master kubelet[14981]: ]
Oct 18 23:04:45 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:04:45 k8s-master kubelet[14981]: E1018 23:04:45.795880   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:04:45 k8s-master kubelet[14981]: I1018 23:04:45.795843   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:04:45 k8s-master kubelet[14981]: I1018 23:04:45.795776   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:04:45 k8s-master kubelet[14981]: I1018 23:04:45.795769   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:04:45 k8s-master kubelet[14981]: I1018 23:04:45.795639   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:04:45 k8s-master kubelet[14981]: I1018 23:04:45.795422   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:04:45 k8s-master kubelet[14981]: I1018 23:04:45.795352   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:04:32 k8s-master kubelet[14981]: ]
Oct 18 23:04:32 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:04:32 k8s-master kubelet[14981]: E1018 23:04:32.993373   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:04:32 k8s-master kubelet[14981]: I1018 23:04:32.993331   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:04:32 k8s-master kubelet[14981]: I1018 23:04:32.993244   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:04:32 k8s-master kubelet[14981]: I1018 23:04:32.993235   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:04:32 k8s-master kubelet[14981]: I1018 23:04:32.993091   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:04:32 k8s-master kubelet[14981]: I1018 23:04:32.992899   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:04:32 k8s-master kubelet[14981]: I1018 23:04:32.992827   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:04:31 k8s-master kubelet[14981]: ]
Oct 18 23:04:31 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:04:31 k8s-master kubelet[14981]: E1018 23:04:31.239440   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:04:31 k8s-master kubelet[14981]: I1018 23:04:31.239378   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:04:31 k8s-master kubelet[14981]: I1018 23:04:31.239270   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:04:31 k8s-master kubelet[14981]: I1018 23:04:31.239252   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:04:31 k8s-master kubelet[14981]: I1018 23:04:31.239096   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:04:31 k8s-master kubelet[14981]: I1018 23:04:31.238843   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:04:31 k8s-master kubelet[14981]: I1018 23:04:31.238739   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:04:30 k8s-master kubelet[14981]: E1018 23:04:30.558934   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:04:30 k8s-master kubelet[14981]: W1018 23:04:30.558774   14981 container.go:354] Failed to create summary reader for "/libcontainer_19773_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:04:30 k8s-master kubelet[14981]: W1018 23:04:30.556469   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_19773_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_19773_systemd_test_default.slice: no such file or directory
Oct 18 23:04:30 k8s-master kubelet[14981]: E1018 23:04:30.468191   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:04:30 k8s-master kubelet[14981]: W1018 23:04:30.467999   14981 container.go:354] Failed to create summary reader for "/libcontainer_19770_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:04:30 k8s-master kubelet[14981]: W1018 23:04:30.461816   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_19770_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_19770_systemd_test_default.slice: no such file or directory
Oct 18 23:04:20 k8s-master kubelet[14981]: E1018 23:04:20.804912   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:04:20 k8s-master kubelet[14981]: I1018 23:04:20.804884   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:04:20 k8s-master kubelet[14981]: I1018 23:04:20.804790   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:04:20 k8s-master kubelet[14981]: I1018 23:04:20.804610   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:04:09 k8s-master kubelet[14981]: E1018 23:04:09.795774   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:04:09 k8s-master kubelet[14981]: I1018 23:04:09.795750   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:04:09 k8s-master kubelet[14981]: I1018 23:04:09.795660   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:04:09 k8s-master kubelet[14981]: I1018 23:04:09.795429   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:03:58 k8s-master kubelet[14981]: E1018 23:03:58.800658   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:03:58 k8s-master kubelet[14981]: I1018 23:03:58.800353   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:03:58 k8s-master kubelet[14981]: I1018 23:03:58.799514   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:03:58 k8s-master kubelet[14981]: I1018 23:03:58.797997   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:03:44 k8s-master kubelet[14981]: E1018 23:03:44.804849   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:03:44 k8s-master kubelet[14981]: I1018 23:03:44.804798   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:03:44 k8s-master kubelet[14981]: I1018 23:03:44.804597   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:03:44 k8s-master kubelet[14981]: I1018 23:03:44.803981   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:03:31 k8s-master kubelet[14981]: E1018 23:03:31.652440   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:03:31 k8s-master kubelet[14981]: I1018 23:03:31.652440   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:03:31 k8s-master kubelet[14981]: I1018 23:03:31.652440   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:03:31 k8s-master kubelet[14981]: I1018 23:03:31.652233   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:03:30 k8s-master kubelet[14981]: E1018 23:03:30.772820   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:03:30 k8s-master kubelet[14981]: E1018 23:03:30.276242   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:03:30 k8s-master kubelet[14981]: W1018 23:03:30.276121   14981 container.go:354] Failed to create summary reader for "/libcontainer_19617_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:03:30 k8s-master kubelet[14981]: E1018 23:03:30.252949   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:03:30 k8s-master kubelet[14981]: I1018 23:03:30.252919   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:03:30 k8s-master kubelet[14981]: I1018 23:03:30.252792   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:03:30 k8s-master kubelet[14981]: W1018 23:03:30.141191   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_19617_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_19617_systemd_test_default.slice: no such file or directory
Oct 18 23:03:30 k8s-master kubelet[14981]: E1018 23:03:30.066468   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:03:30 k8s-master kubelet[14981]: W1018 23:03:30.066075   14981 container.go:354] Failed to create summary reader for "/libcontainer_19604_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:03:30 k8s-master kubelet[14981]: W1018 23:03:30.049776   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_19604_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_19604_systemd_test_default.slice: no such file or directory
Oct 18 23:03:29 k8s-master kubelet[14981]: I1018 23:03:29.795976   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:03:29 k8s-master kubelet[14981]: I1018 23:03:29.795732   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:03:29 k8s-master kubelet[14981]: I1018 23:03:29.795179   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:03:25 k8s-master kubelet[14981]: E1018 23:03:25.079536   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:03:24 k8s-master kubelet[14981]: E1018 23:03:24.792032   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:03:18 k8s-master kubelet[14981]: E1018 23:03:18.688202   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:03:15 k8s-master kubelet[14981]: ]
Oct 18 23:03:15 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:03:15 k8s-master kubelet[14981]: E1018 23:03:15.795132   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:03:15 k8s-master kubelet[14981]: I1018 23:03:15.795054   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:03:15 k8s-master kubelet[14981]: I1018 23:03:15.794983   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:03:15 k8s-master kubelet[14981]: I1018 23:03:15.794972   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:03:15 k8s-master kubelet[14981]: I1018 23:03:15.794793   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:03:15 k8s-master kubelet[14981]: I1018 23:03:15.794614   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:03:15 k8s-master kubelet[14981]: I1018 23:03:15.794545   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:03:08 k8s-master kubelet[14981]: E1018 23:03:08.961860   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:03:04 k8s-master kubelet[14981]: E1018 23:03:04.934790   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:03:02 k8s-master kubelet[14981]: E1018 23:03:02.377214   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:03:02 k8s-master kubelet[14981]: E1018 23:03:02.265576   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:02:59 k8s-master kubelet[14981]: ]
Oct 18 23:02:59 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:02:59 k8s-master kubelet[14981]: E1018 23:02:59.796687   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:02:59 k8s-master kubelet[14981]: I1018 23:02:59.796637   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:02:59 k8s-master kubelet[14981]: I1018 23:02:59.796576   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:02:59 k8s-master kubelet[14981]: I1018 23:02:59.796570   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:02:59 k8s-master kubelet[14981]: I1018 23:02:59.796455   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:02:59 k8s-master kubelet[14981]: I1018 23:02:59.796272   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:02:59 k8s-master kubelet[14981]: I1018 23:02:59.796185   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:02:54 k8s-master kubelet[14981]: E1018 23:02:54.533473   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:02:54 k8s-master kubelet[14981]: E1018 23:02:54.332092   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:02:45 k8s-master kubelet[14981]: ]
Oct 18 23:02:45 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:02:45 k8s-master kubelet[14981]: E1018 23:02:45.785886   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:02:45 k8s-master kubelet[14981]: I1018 23:02:45.785851   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:02:45 k8s-master kubelet[14981]: I1018 23:02:45.785778   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:02:45 k8s-master kubelet[14981]: I1018 23:02:45.785764   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:02:45 k8s-master kubelet[14981]: I1018 23:02:45.785617   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:02:45 k8s-master kubelet[14981]: I1018 23:02:45.785431   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:02:45 k8s-master kubelet[14981]: I1018 23:02:45.785348   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:02:44 k8s-master kubelet[14981]: ]
Oct 18 23:02:44 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:02:44 k8s-master kubelet[14981]: E1018 23:02:44.767866   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:02:44 k8s-master kubelet[14981]: I1018 23:02:44.767826   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:02:44 k8s-master kubelet[14981]: I1018 23:02:44.767755   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:02:44 k8s-master kubelet[14981]: I1018 23:02:44.767747   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:02:44 k8s-master kubelet[14981]: I1018 23:02:44.767624   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:02:44 k8s-master kubelet[14981]: I1018 23:02:44.767421   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:02:44 k8s-master kubelet[14981]: I1018 23:02:44.767341   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:02:43 k8s-master kubelet[14981]: ]
Oct 18 23:02:43 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:02:43 k8s-master kubelet[14981]: E1018 23:02:43.428114   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:02:43 k8s-master kubelet[14981]: I1018 23:02:43.428061   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:02:43 k8s-master kubelet[14981]: I1018 23:02:43.427958   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:02:43 k8s-master kubelet[14981]: I1018 23:02:43.427930   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:02:43 k8s-master kubelet[14981]: I1018 23:02:43.427674   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:02:43 k8s-master kubelet[14981]: E1018 23:02:43.383857   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:02:43 k8s-master kubelet[14981]: W1018 23:02:43.383711   14981 container.go:354] Failed to create summary reader for "/libcontainer_19500_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:02:43 k8s-master kubelet[14981]: W1018 23:02:43.371923   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_19500_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_19500_systemd_test_default.slice: no such file or directory
Oct 18 23:02:13 k8s-master kubelet[14981]: I1018 23:02:13.171484   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:02:06 k8s-master kubelet[14981]: E1018 23:02:06.804771   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:02:06 k8s-master kubelet[14981]: I1018 23:02:06.804742   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:02:06 k8s-master kubelet[14981]: I1018 23:02:06.804630   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:02:06 k8s-master kubelet[14981]: I1018 23:02:06.804352   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:01:54 k8s-master kubelet[14981]: E1018 23:01:54.800829   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:01:54 k8s-master kubelet[14981]: I1018 23:01:54.800783   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:01:54 k8s-master kubelet[14981]: I1018 23:01:54.800568   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:01:54 k8s-master kubelet[14981]: I1018 23:01:54.800149   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:01:40 k8s-master kubelet[14981]: E1018 23:01:40.800281   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:01:40 k8s-master kubelet[14981]: I1018 23:01:40.800249   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:01:40 k8s-master kubelet[14981]: I1018 23:01:40.800135   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:01:40 k8s-master kubelet[14981]: I1018 23:01:40.799702   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:01:33 k8s-master kubelet[14981]: E1018 23:01:33.522008   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:01:27 k8s-master kubelet[14981]: E1018 23:01:27.795991   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:01:27 k8s-master kubelet[14981]: I1018 23:01:27.795949   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:01:27 k8s-master kubelet[14981]: I1018 23:01:27.795757   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:01:27 k8s-master kubelet[14981]: I1018 23:01:27.794707   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:01:25 k8s-master kubelet[14981]: E1018 23:01:25.411936   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:01:17 k8s-master kubelet[14981]: E1018 23:01:17.186106   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:01:16 k8s-master kubelet[14981]: E1018 23:01:16.799916   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:01:16 k8s-master kubelet[14981]: I1018 23:01:16.799890   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:01:16 k8s-master kubelet[14981]: I1018 23:01:16.799769   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:01:16 k8s-master kubelet[14981]: I1018 23:01:16.799516   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:01:12 k8s-master kubelet[14981]: E1018 23:01:12.159847   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:01:02 k8s-master kubelet[14981]: E1018 23:01:02.804932   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:01:02 k8s-master kubelet[14981]: I1018 23:01:02.804903   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:01:02 k8s-master kubelet[14981]: I1018 23:01:02.804789   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:01:02 k8s-master kubelet[14981]: I1018 23:01:02.803702   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:00:59 k8s-master kubelet[14981]: E1018 23:00:59.109423   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:00:49 k8s-master kubelet[14981]: E1018 23:00:49.795776   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:00:49 k8s-master kubelet[14981]: I1018 23:00:49.795738   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:00:49 k8s-master kubelet[14981]: I1018 23:00:49.795597   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:00:49 k8s-master kubelet[14981]: I1018 23:00:49.795181   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:00:39 k8s-master kubelet[14981]: E1018 23:00:39.812000   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:00:35 k8s-master kubelet[14981]: E1018 23:00:35.444388   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:00:35 k8s-master kubelet[14981]: I1018 23:00:35.444357   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:00:35 k8s-master kubelet[14981]: I1018 23:00:35.444213   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:00:35 k8s-master kubelet[14981]: I1018 23:00:35.443921   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:00:34 k8s-master kubelet[14981]: E1018 23:00:34.705685   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:00:34 k8s-master kubelet[14981]: E1018 23:00:34.407155   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:00:34 k8s-master kubelet[14981]: I1018 23:00:34.407117   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:00:34 k8s-master kubelet[14981]: I1018 23:00:34.406984   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:00:34 k8s-master kubelet[14981]: I1018 23:00:34.406829   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:00:34 k8s-master kubelet[14981]: E1018 23:00:34.332416   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:00:34 k8s-master kubelet[14981]: E1018 23:00:33.995663   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:00:33 k8s-master kubelet[14981]: E1018 23:00:33.965447   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:00:33 k8s-master kubelet[14981]: W1018 23:00:33.965324   14981 container.go:354] Failed to create summary reader for "/libcontainer_19021_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:00:33 k8s-master kubelet[14981]: W1018 23:00:33.872894   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_19021_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_19021_systemd_test_default.slice: no such file or directory
Oct 18 23:00:33 k8s-master kubelet[14981]: E1018 23:00:33.784050   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:00:33 k8s-master kubelet[14981]: W1018 23:00:33.783923   14981 container.go:354] Failed to create summary reader for "/libcontainer_19008_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:00:33 k8s-master kubelet[14981]: W1018 23:00:33.780369   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_19008_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_19008_systemd_test_default.slice: no such file or directory
Oct 18 23:00:33 k8s-master kubelet[14981]: E1018 23:00:33.536458   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:00:33 k8s-master kubelet[14981]: I1018 23:00:33.506594   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:00:33 k8s-master kubelet[14981]: I1018 23:00:33.506584   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 23:00:33 k8s-master kubelet[14981]: I1018 23:00:33.506417   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 23:00:33 k8s-master kubelet[14981]: I1018 23:00:33.318539   14981 container.go:471] Failed to update stats for container "/libcontainer_18993_systemd_test_default.slice": failed to parse memory.usage_in_bytes - read /sys/fs/cgroup/memory/libcontainer_18993_systemd_test_default.slice/memory.usage_in_bytes: no such device, continuing to push stats
Oct 18 23:00:33 k8s-master kubelet[14981]: E1018 23:00:33.318386   14981 helpers.go:135] readString: Failed to read "/sys/fs/cgroup/memory/libcontainer_18993_systemd_test_default.slice/memory.soft_limit_in_bytes": read /sys/fs/cgroup/memory/libcontainer_18993_systemd_test_default.slice/memory.soft_limit_in_bytes: no such device
Oct 18 23:00:33 k8s-master kubelet[14981]: E1018 23:00:33.318365   14981 helpers.go:135] readString: Failed to read "/sys/fs/cgroup/memory/libcontainer_18993_systemd_test_default.slice/memory.memsw.limit_in_bytes": read /sys/fs/cgroup/memory/libcontainer_18993_systemd_test_default.slice/memory.memsw.limit_in_bytes: no such device
Oct 18 23:00:33 k8s-master kubelet[14981]: E1018 23:00:33.318336   14981 helpers.go:135] readString: Failed to read "/sys/fs/cgroup/memory/libcontainer_18993_systemd_test_default.slice/memory.limit_in_bytes": read /sys/fs/cgroup/memory/libcontainer_18993_systemd_test_default.slice/memory.limit_in_bytes: no such device
Oct 18 23:00:33 k8s-master kubelet[14981]: E1018 23:00:33.318258   14981 helpers.go:135] readString: Failed to read "/sys/fs/cgroup/memory/libcontainer_18993_systemd_test_default.slice/memory.soft_limit_in_bytes": read /sys/fs/cgroup/memory/libcontainer_18993_systemd_test_default.slice/memory.soft_limit_in_bytes: no such device
Oct 18 23:00:33 k8s-master kubelet[14981]: E1018 23:00:33.318228   14981 helpers.go:135] readString: Failed to read "/sys/fs/cgroup/memory/libcontainer_18993_systemd_test_default.slice/memory.memsw.limit_in_bytes": read /sys/fs/cgroup/memory/libcontainer_18993_systemd_test_default.slice/memory.memsw.limit_in_bytes: no such device
Oct 18 23:00:33 k8s-master kubelet[14981]: E1018 23:00:33.316860   14981 helpers.go:135] readString: Failed to read "/sys/fs/cgroup/memory/libcontainer_18993_systemd_test_default.slice/memory.limit_in_bytes": read /sys/fs/cgroup/memory/libcontainer_18993_systemd_test_default.slice/memory.limit_in_bytes: no such device
Oct 18 23:00:23 k8s-master kubelet[14981]: E1018 23:00:23.703583   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:00:23 k8s-master kubelet[14981]: W1018 23:00:23.703464   14981 container.go:354] Failed to create summary reader for "/libcontainer_18900_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:00:23 k8s-master kubelet[14981]: E1018 23:00:23.282351   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:00:23 k8s-master kubelet[14981]: W1018 23:00:23.282098   14981 container.go:354] Failed to create summary reader for "/libcontainer_18888_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:00:23 k8s-master kubelet[14981]: W1018 23:00:23.270801   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_18888_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_18888_systemd_test_default.slice: no such file or directory
Oct 18 23:00:23 k8s-master kubelet[14981]: I1018 23:00:23.124431   14981 kuberuntime_manager.go:499] Container {Name:kube-flannel Image:quay.io/coreos/flannel:v0.9.0-amd64 Command:[/opt/bin/flanneld --ip-masq --kube-subnet-mgr] Args:[] WorkingDir: Ports:[] EnvFrom:[] Env:[{Name:POD_NAME Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.name,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}} {Name:POD_NAMESPACE Value: ValueFrom:&EnvVarSource{FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:nil,}}] Resources:{Limits:map[] Requests:map[]} VolumeMounts:[{Name:run ReadOnly:false MountPath:/run SubPath: MountPropagation:<nil>} {Name:flannel-cfg ReadOnly:false MountPath:/etc/kube-flannel/ SubPath: MountPropagation:<nil>} {Name:flannel-token-w5mq4 ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:nil ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:&SecurityContext{Capabilities:nil,Privileged:*true,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,} Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 23:00:21 k8s-master kubelet[14981]: E1018 23:00:21.929939   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:00:21 k8s-master kubelet[14981]: W1018 23:00:21.929732   14981 container.go:354] Failed to create summary reader for "/libcontainer_18874_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:00:21 k8s-master kubelet[14981]: W1018 23:00:21.929463   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_18874_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_18874_systemd_test_default.slice: no such file or directory
Oct 18 23:00:21 k8s-master kubelet[14981]: E1018 23:00:21.877397   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:00:21 k8s-master kubelet[14981]: W1018 23:00:21.877006   14981 container.go:354] Failed to create summary reader for "/libcontainer_18871_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:00:21 k8s-master kubelet[14981]: W1018 23:00:21.873015   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_18871_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_18871_systemd_test_default.slice: no such file or directory
Oct 18 23:00:21 k8s-master kubelet[14981]: E1018 23:00:21.807627   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:00:21 k8s-master kubelet[14981]: E1018 23:00:21.807507   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:00:21 k8s-master kubelet[14981]: W1018 23:00:21.807294   14981 container.go:354] Failed to create summary reader for "/libcontainer_18860_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:00:21 k8s-master kubelet[14981]: E1018 23:00:21.171156   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:00:21 k8s-master kubelet[14981]: W1018 23:00:21.171057   14981 container.go:354] Failed to create summary reader for "/libcontainer_18831_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 23:00:20 k8s-master kubelet[14981]: I1018 23:00:20.418258   14981 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "flannel-token-w5mq4" (UniqueName: "kubernetes.io/secret/0f66708c-b415-11e7-b99e-08002782873b-flannel-token-w5mq4") pod "kube-flannel-ds-jmhzv" (UID: "0f66708c-b415-11e7-b99e-08002782873b")
Oct 18 23:00:20 k8s-master kubelet[14981]: I1018 23:00:20.418218   14981 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "flannel-cfg" (UniqueName: "kubernetes.io/configmap/0f66708c-b415-11e7-b99e-08002782873b-flannel-cfg") pod "kube-flannel-ds-jmhzv" (UID: "0f66708c-b415-11e7-b99e-08002782873b")
Oct 18 23:00:20 k8s-master kubelet[14981]: I1018 23:00:20.418179   14981 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "run" (UniqueName: "kubernetes.io/host-path/0f66708c-b415-11e7-b99e-08002782873b-run") pod "kube-flannel-ds-jmhzv" (UID: "0f66708c-b415-11e7-b99e-08002782873b")
Oct 18 23:00:20 k8s-master kubelet[14981]: I1018 23:00:20.418111   14981 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "cni" (UniqueName: "kubernetes.io/host-path/0f66708c-b415-11e7-b99e-08002782873b-cni") pod "kube-flannel-ds-jmhzv" (UID: "0f66708c-b415-11e7-b99e-08002782873b")
Oct 18 23:00:18 k8s-master kubelet[14981]: E1018 23:00:18.486709   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 23:00:03 k8s-master kubelet[14981]: I1018 23:00:03.169001   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:59:53 k8s-master kubelet[14981]: E1018 22:59:53.794748   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:59:53 k8s-master kubelet[14981]: I1018 22:59:53.794719   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:59:53 k8s-master kubelet[14981]: I1018 22:59:53.794611   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:59:53 k8s-master kubelet[14981]: I1018 22:59:53.794346   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:59:41 k8s-master kubelet[14981]: E1018 22:59:41.813822   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:59:41 k8s-master kubelet[14981]: I1018 22:59:41.813738   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:59:41 k8s-master kubelet[14981]: I1018 22:59:41.813586   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:59:41 k8s-master kubelet[14981]: I1018 22:59:41.813304   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:59:28 k8s-master kubelet[14981]: E1018 22:59:28.801666   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:59:28 k8s-master kubelet[14981]: I1018 22:59:28.801630   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:59:28 k8s-master kubelet[14981]: I1018 22:59:28.801506   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:59:28 k8s-master kubelet[14981]: I1018 22:59:28.801207   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:59:16 k8s-master kubelet[14981]: E1018 22:59:16.798265   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:59:16 k8s-master kubelet[14981]: I1018 22:59:16.798232   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:59:16 k8s-master kubelet[14981]: I1018 22:59:16.798083   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:59:16 k8s-master kubelet[14981]: I1018 22:59:16.797765   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:59:00 k8s-master kubelet[14981]: E1018 22:59:00.806834   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:59:00 k8s-master kubelet[14981]: I1018 22:59:00.806802   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:59:00 k8s-master kubelet[14981]: I1018 22:59:00.806611   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:59:00 k8s-master kubelet[14981]: I1018 22:59:00.805650   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:58:48 k8s-master kubelet[14981]: E1018 22:58:48.798515   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:58:48 k8s-master kubelet[14981]: I1018 22:58:48.798483   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:58:48 k8s-master kubelet[14981]: I1018 22:58:48.798366   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:58:48 k8s-master kubelet[14981]: I1018 22:58:48.798086   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:58:32 k8s-master kubelet[14981]: E1018 22:58:32.993367   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:58:32 k8s-master kubelet[14981]: I1018 22:58:32.993335   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:58:32 k8s-master kubelet[14981]: I1018 22:58:32.993208   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:58:32 k8s-master kubelet[14981]: I1018 22:58:32.992909   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:58:30 k8s-master kubelet[14981]: E1018 22:58:30.837014   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:58:29 k8s-master kubelet[14981]: E1018 22:58:29.854251   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:58:29 k8s-master kubelet[14981]: I1018 22:58:29.854214   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:58:29 k8s-master kubelet[14981]: I1018 22:58:29.854079   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:58:29 k8s-master kubelet[14981]: I1018 22:58:29.853762   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:58:29 k8s-master kubelet[14981]: E1018 22:58:29.417596   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:58:29 k8s-master kubelet[14981]: W1018 22:58:29.417561   14981 container.go:354] Failed to create summary reader for "/libcontainer_18574_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:58:29 k8s-master kubelet[14981]: W1018 22:58:29.410605   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_18574_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_18574_systemd_test_default.slice: no such file or directory
Oct 18 22:58:29 k8s-master kubelet[14981]: E1018 22:58:29.372362   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:58:29 k8s-master kubelet[14981]: W1018 22:58:29.372169   14981 container.go:354] Failed to create summary reader for "/libcontainer_18571_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:58:29 k8s-master kubelet[14981]: W1018 22:58:29.369078   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_18571_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_18571_systemd_test_default.slice: no such file or directory
Oct 18 22:58:24 k8s-master kubelet[14981]: E1018 22:58:24.240281   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:58:24 k8s-master kubelet[14981]: W1018 22:58:24.240151   14981 container.go:354] Failed to create summary reader for "/libcontainer_18530_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:58:24 k8s-master kubelet[14981]: W1018 22:58:24.162749   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_18530_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_18530_systemd_test_default.slice: no such file or directory
Oct 18 22:58:23 k8s-master kubelet[14981]: I1018 22:58:23.794877   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:58:23 k8s-master kubelet[14981]: I1018 22:58:23.794660   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:58:08 k8s-master kubelet[14981]: E1018 22:58:08.799751   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:58:08 k8s-master kubelet[14981]: I1018 22:58:08.799722   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:58:08 k8s-master kubelet[14981]: I1018 22:58:08.799616   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:58:08 k8s-master kubelet[14981]: I1018 22:58:08.799433   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:57:54 k8s-master kubelet[14981]: E1018 22:57:54.806343   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:57:54 k8s-master kubelet[14981]: I1018 22:57:54.806050   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:57:54 k8s-master kubelet[14981]: I1018 22:57:54.805754   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:57:54 k8s-master kubelet[14981]: I1018 22:57:54.805060   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:57:43 k8s-master kubelet[14981]: E1018 22:57:43.794929   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:57:43 k8s-master kubelet[14981]: I1018 22:57:43.794897   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:57:43 k8s-master kubelet[14981]: I1018 22:57:43.794785   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:57:43 k8s-master kubelet[14981]: I1018 22:57:43.794597   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:57:30 k8s-master kubelet[14981]: E1018 22:57:30.272335   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:57:30 k8s-master kubelet[14981]: I1018 22:57:30.272303   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:57:30 k8s-master kubelet[14981]: I1018 22:57:30.272185   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:57:30 k8s-master kubelet[14981]: I1018 22:57:30.271960   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:57:29 k8s-master kubelet[14981]: E1018 22:57:29.247317   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:57:29 k8s-master kubelet[14981]: I1018 22:57:29.247282   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:57:29 k8s-master kubelet[14981]: I1018 22:57:29.247142   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:57:29 k8s-master kubelet[14981]: E1018 22:57:29.216862   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:57:29 k8s-master kubelet[14981]: W1018 22:57:29.216742   14981 container.go:354] Failed to create summary reader for "/libcontainer_18433_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:57:28 k8s-master kubelet[14981]: W1018 22:57:28.972466   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_18408_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_18408_systemd_test_default.slice: no such file or directory
Oct 18 22:57:28 k8s-master kubelet[14981]: E1018 22:57:28.972433   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:57:28 k8s-master kubelet[14981]: W1018 22:57:28.972325   14981 container.go:354] Failed to create summary reader for "/libcontainer_18408_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:57:28 k8s-master kubelet[14981]: I1018 22:57:28.799524   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:57:28 k8s-master kubelet[14981]: I1018 22:57:28.799219   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:57:28 k8s-master kubelet[14981]: I1018 22:57:28.799124   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:57:13 k8s-master kubelet[14981]: ]
Oct 18 22:57:13 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:57:13 k8s-master kubelet[14981]: E1018 22:57:13.795295   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:57:13 k8s-master kubelet[14981]: I1018 22:57:13.795239   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:57:13 k8s-master kubelet[14981]: I1018 22:57:13.795067   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:57:13 k8s-master kubelet[14981]: I1018 22:57:13.795048   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:57:13 k8s-master kubelet[14981]: I1018 22:57:13.794825   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:57:13 k8s-master kubelet[14981]: I1018 22:57:13.794537   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:57:13 k8s-master kubelet[14981]: I1018 22:57:13.794408   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:57:01 k8s-master kubelet[14981]: ]
Oct 18 22:57:01 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:57:01 k8s-master kubelet[14981]: E1018 22:57:01.794773   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:57:01 k8s-master kubelet[14981]: I1018 22:57:01.794773   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:57:01 k8s-master kubelet[14981]: I1018 22:57:01.794721   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:57:01 k8s-master kubelet[14981]: I1018 22:57:01.794713   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:57:01 k8s-master kubelet[14981]: I1018 22:57:01.794598   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:57:01 k8s-master kubelet[14981]: I1018 22:57:01.794396   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:57:01 k8s-master kubelet[14981]: I1018 22:57:01.794282   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:56:47 k8s-master kubelet[14981]: ]
Oct 18 22:56:47 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:56:47 k8s-master kubelet[14981]: E1018 22:56:47.795095   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:56:47 k8s-master kubelet[14981]: I1018 22:56:47.795046   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:56:47 k8s-master kubelet[14981]: I1018 22:56:47.794960   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:56:47 k8s-master kubelet[14981]: I1018 22:56:47.794929   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:56:47 k8s-master kubelet[14981]: I1018 22:56:47.794784   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:56:47 k8s-master kubelet[14981]: I1018 22:56:47.794534   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:56:47 k8s-master kubelet[14981]: I1018 22:56:47.794442   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:56:33 k8s-master kubelet[14981]: ]
Oct 18 22:56:33 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:56:33 k8s-master kubelet[14981]: E1018 22:56:33.795565   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:56:33 k8s-master kubelet[14981]: I1018 22:56:33.795530   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:56:33 k8s-master kubelet[14981]: I1018 22:56:33.795468   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:56:33 k8s-master kubelet[14981]: I1018 22:56:33.795461   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:56:33 k8s-master kubelet[14981]: I1018 22:56:33.795355   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:56:33 k8s-master kubelet[14981]: I1018 22:56:33.795195   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:56:33 k8s-master kubelet[14981]: I1018 22:56:33.795123   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:56:20 k8s-master kubelet[14981]: ]
Oct 18 22:56:20 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:56:20 k8s-master kubelet[14981]: E1018 22:56:20.801811   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:56:20 k8s-master kubelet[14981]: I1018 22:56:20.801757   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:56:20 k8s-master kubelet[14981]: I1018 22:56:20.801656   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:56:20 k8s-master kubelet[14981]: I1018 22:56:20.801641   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:56:20 k8s-master kubelet[14981]: I1018 22:56:20.801457   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:56:20 k8s-master kubelet[14981]: I1018 22:56:20.801183   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:56:20 k8s-master kubelet[14981]: I1018 22:56:20.801029   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:56:04 k8s-master kubelet[14981]: ]
Oct 18 22:56:04 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:56:04 k8s-master kubelet[14981]: E1018 22:56:04.801438   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:56:04 k8s-master kubelet[14981]: I1018 22:56:04.801333   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:56:04 k8s-master kubelet[14981]: I1018 22:56:04.801201   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:56:04 k8s-master kubelet[14981]: I1018 22:56:04.801177   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:56:04 k8s-master kubelet[14981]: I1018 22:56:04.800933   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:56:04 k8s-master kubelet[14981]: I1018 22:56:04.800556   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:56:04 k8s-master kubelet[14981]: I1018 22:56:04.800425   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:55:52 k8s-master kubelet[14981]: ]
Oct 18 22:55:52 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:55:52 k8s-master kubelet[14981]: E1018 22:55:52.801745   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:55:52 k8s-master kubelet[14981]: I1018 22:55:52.801659   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:55:52 k8s-master kubelet[14981]: I1018 22:55:52.801376   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:55:52 k8s-master kubelet[14981]: I1018 22:55:52.801307   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:55:52 k8s-master kubelet[14981]: I1018 22:55:52.800919   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:55:52 k8s-master kubelet[14981]: I1018 22:55:52.800581   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:55:52 k8s-master kubelet[14981]: I1018 22:55:52.800453   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:55:40 k8s-master kubelet[14981]: ]
Oct 18 22:55:40 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:55:40 k8s-master kubelet[14981]: E1018 22:55:40.801773   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:55:40 k8s-master kubelet[14981]: I1018 22:55:40.801738   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:55:40 k8s-master kubelet[14981]: I1018 22:55:40.801669   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:55:40 k8s-master kubelet[14981]: I1018 22:55:40.801662   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:55:40 k8s-master kubelet[14981]: I1018 22:55:40.801540   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:55:40 k8s-master kubelet[14981]: I1018 22:55:40.801359   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:55:40 k8s-master kubelet[14981]: I1018 22:55:40.801286   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:55:27 k8s-master kubelet[14981]: ]
Oct 18 22:55:27 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:55:27 k8s-master kubelet[14981]: E1018 22:55:27.798321   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:55:27 k8s-master kubelet[14981]: I1018 22:55:27.798234   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:55:27 k8s-master kubelet[14981]: I1018 22:55:27.798090   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:55:27 k8s-master kubelet[14981]: I1018 22:55:27.798058   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:55:27 k8s-master kubelet[14981]: I1018 22:55:27.797493   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:55:27 k8s-master kubelet[14981]: I1018 22:55:27.797493   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:55:27 k8s-master kubelet[14981]: I1018 22:55:27.797485   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:55:12 k8s-master kubelet[14981]: ]
Oct 18 22:55:12 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:55:12 k8s-master kubelet[14981]: E1018 22:55:12.798551   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:55:12 k8s-master kubelet[14981]: I1018 22:55:12.798513   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:55:12 k8s-master kubelet[14981]: I1018 22:55:12.798416   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:55:12 k8s-master kubelet[14981]: I1018 22:55:12.798405   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:55:12 k8s-master kubelet[14981]: I1018 22:55:12.798240   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:55:12 k8s-master kubelet[14981]: I1018 22:55:12.797559   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:55:12 k8s-master kubelet[14981]: I1018 22:55:12.797478   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:54:58 k8s-master kubelet[14981]: ]
Oct 18 22:54:58 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:54:58 k8s-master kubelet[14981]: E1018 22:54:58.798051   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:54:58 k8s-master kubelet[14981]: I1018 22:54:58.798020   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:54:58 k8s-master kubelet[14981]: I1018 22:54:58.797960   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:54:58 k8s-master kubelet[14981]: I1018 22:54:58.797953   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:54:58 k8s-master kubelet[14981]: I1018 22:54:58.797847   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:54:58 k8s-master kubelet[14981]: I1018 22:54:58.797676   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:54:58 k8s-master kubelet[14981]: I1018 22:54:58.797612   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:54:47 k8s-master kubelet[14981]: ]
Oct 18 22:54:47 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:54:47 k8s-master kubelet[14981]: E1018 22:54:47.795790   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:54:47 k8s-master kubelet[14981]: I1018 22:54:47.795714   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:54:47 k8s-master kubelet[14981]: I1018 22:54:47.795266   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:54:47 k8s-master kubelet[14981]: I1018 22:54:47.795247   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:54:47 k8s-master kubelet[14981]: I1018 22:54:47.795069   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:54:47 k8s-master kubelet[14981]: I1018 22:54:47.794784   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:54:47 k8s-master kubelet[14981]: I1018 22:54:47.794664   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:54:33 k8s-master kubelet[14981]: ]
Oct 18 22:54:33 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:54:33 k8s-master kubelet[14981]: E1018 22:54:33.795252   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:54:33 k8s-master kubelet[14981]: I1018 22:54:33.795200   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:54:33 k8s-master kubelet[14981]: I1018 22:54:33.795100   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:54:33 k8s-master kubelet[14981]: I1018 22:54:33.795087   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:54:33 k8s-master kubelet[14981]: I1018 22:54:33.794924   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:54:33 k8s-master kubelet[14981]: I1018 22:54:33.794673   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:54:33 k8s-master kubelet[14981]: I1018 22:54:33.794578   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:54:17 k8s-master kubelet[14981]: ]
Oct 18 22:54:17 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:54:17 k8s-master kubelet[14981]: E1018 22:54:17.795406   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:54:17 k8s-master kubelet[14981]: I1018 22:54:17.795360   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:54:17 k8s-master kubelet[14981]: I1018 22:54:17.794532   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:54:17 k8s-master kubelet[14981]: I1018 22:54:17.794532   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:54:17 k8s-master kubelet[14981]: I1018 22:54:17.794532   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:54:17 k8s-master kubelet[14981]: I1018 22:54:17.794462   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:54:17 k8s-master kubelet[14981]: I1018 22:54:17.794375   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:54:02 k8s-master kubelet[14981]: ]
Oct 18 22:54:02 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:54:02 k8s-master kubelet[14981]: E1018 22:54:02.799077   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:54:02 k8s-master kubelet[14981]: I1018 22:54:02.799041   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:54:02 k8s-master kubelet[14981]: I1018 22:54:02.798975   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:54:02 k8s-master kubelet[14981]: I1018 22:54:02.798967   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:54:02 k8s-master kubelet[14981]: I1018 22:54:02.798842   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:54:02 k8s-master kubelet[14981]: I1018 22:54:02.798617   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:54:02 k8s-master kubelet[14981]: I1018 22:54:02.798547   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:53:47 k8s-master kubelet[14981]: ]
Oct 18 22:53:47 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:53:47 k8s-master kubelet[14981]: E1018 22:53:47.795105   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:53:47 k8s-master kubelet[14981]: I1018 22:53:47.795069   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:53:47 k8s-master kubelet[14981]: I1018 22:53:47.794989   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:53:47 k8s-master kubelet[14981]: I1018 22:53:47.794981   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:53:47 k8s-master kubelet[14981]: I1018 22:53:47.794873   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:53:47 k8s-master kubelet[14981]: I1018 22:53:47.794704   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:53:47 k8s-master kubelet[14981]: I1018 22:53:47.794636   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:53:35 k8s-master kubelet[14981]: ]
Oct 18 22:53:35 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:53:35 k8s-master kubelet[14981]: E1018 22:53:35.795170   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:53:35 k8s-master kubelet[14981]: I1018 22:53:35.795119   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:53:35 k8s-master kubelet[14981]: I1018 22:53:35.795003   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:53:35 k8s-master kubelet[14981]: I1018 22:53:35.794987   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:53:35 k8s-master kubelet[14981]: I1018 22:53:35.794809   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:53:35 k8s-master kubelet[14981]: I1018 22:53:35.794566   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:53:35 k8s-master kubelet[14981]: I1018 22:53:35.794471   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:53:32 k8s-master kubelet[14981]: E1018 22:53:32.608191   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:53:24 k8s-master kubelet[14981]: ]
Oct 18 22:53:24 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:53:24 k8s-master kubelet[14981]: E1018 22:53:24.843326   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:53:24 k8s-master kubelet[14981]: I1018 22:53:24.843325   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:53:24 k8s-master kubelet[14981]: I1018 22:53:24.843299   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:53:24 k8s-master kubelet[14981]: I1018 22:53:24.843289   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:53:24 k8s-master kubelet[14981]: I1018 22:53:24.843151   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:53:24 k8s-master kubelet[14981]: I1018 22:53:24.842928   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:53:24 k8s-master kubelet[14981]: I1018 22:53:24.842826   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:53:23 k8s-master kubelet[14981]: ]
Oct 18 22:53:23 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:53:23 k8s-master kubelet[14981]: E1018 22:53:23.793425   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:53:23 k8s-master kubelet[14981]: I1018 22:53:23.793425   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:53:23 k8s-master kubelet[14981]: I1018 22:53:23.793425   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:53:23 k8s-master kubelet[14981]: I1018 22:53:23.793425   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:53:23 k8s-master kubelet[14981]: I1018 22:53:23.793311   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:53:23 k8s-master kubelet[14981]: I1018 22:53:23.793094   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:53:23 k8s-master kubelet[14981]: I1018 22:53:23.793022   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:53:23 k8s-master kubelet[14981]: ]
Oct 18 22:53:23 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:53:23 k8s-master kubelet[14981]: E1018 22:53:23.396153   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:53:23 k8s-master kubelet[14981]: I1018 22:53:23.396117   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:53:23 k8s-master kubelet[14981]: I1018 22:53:23.396054   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:53:23 k8s-master kubelet[14981]: I1018 22:53:23.396045   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:53:23 k8s-master kubelet[14981]: I1018 22:53:23.395836   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:53:23 k8s-master kubelet[14981]: E1018 22:53:23.345167   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:53:23 k8s-master kubelet[14981]: W1018 22:53:23.344809   14981 container.go:354] Failed to create summary reader for "/libcontainer_18078_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:53:23 k8s-master kubelet[14981]: W1018 22:53:23.338075   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_18078_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_18078_systemd_test_default.slice: no such file or directory
Oct 18 22:53:23 k8s-master kubelet[14981]: E1018 22:53:23.285074   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:53:23 k8s-master kubelet[14981]: W1018 22:53:23.284858   14981 container.go:354] Failed to create summary reader for "/libcontainer_18075_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:53:23 k8s-master kubelet[14981]: W1018 22:53:23.281533   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_18075_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_18075_systemd_test_default.slice: no such file or directory
Oct 18 22:53:23 k8s-master kubelet[14981]: E1018 22:53:23.236635   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:53:23 k8s-master kubelet[14981]: W1018 22:53:23.236576   14981 container.go:354] Failed to create summary reader for "/libcontainer_18072_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:53:23 k8s-master kubelet[14981]: W1018 22:53:23.232856   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_18072_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_18072_systemd_test_default.slice: no such file or directory
Oct 18 22:52:53 k8s-master kubelet[14981]: I1018 22:52:53.183152   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:52:48 k8s-master kubelet[14981]: E1018 22:52:48.800741   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:52:48 k8s-master kubelet[14981]: I1018 22:52:48.800705   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:52:48 k8s-master kubelet[14981]: I1018 22:52:48.800579   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:52:48 k8s-master kubelet[14981]: I1018 22:52:48.800222   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:52:35 k8s-master kubelet[14981]: E1018 22:52:35.796964   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:52:35 k8s-master kubelet[14981]: I1018 22:52:35.796936   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:52:35 k8s-master kubelet[14981]: I1018 22:52:35.796816   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:52:35 k8s-master kubelet[14981]: I1018 22:52:35.796494   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:52:23 k8s-master kubelet[14981]: E1018 22:52:23.015919   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:52:23 k8s-master kubelet[14981]: I1018 22:52:23.015761   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:52:23 k8s-master kubelet[14981]: I1018 22:52:23.008434   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:52:23 k8s-master kubelet[14981]: I1018 22:52:23.007494   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:52:21 k8s-master kubelet[14981]: E1018 22:52:21.887482   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:52:21 k8s-master kubelet[14981]: I1018 22:52:21.887450   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:52:21 k8s-master kubelet[14981]: I1018 22:52:21.887305   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:52:21 k8s-master kubelet[14981]: I1018 22:52:21.886968   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:52:21 k8s-master kubelet[14981]: E1018 22:52:21.416610   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:52:20 k8s-master kubelet[14981]: E1018 22:52:20.977847   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:52:20 k8s-master kubelet[14981]: W1018 22:52:20.977716   14981 container.go:354] Failed to create summary reader for "/libcontainer_17979_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:52:20 k8s-master kubelet[14981]: W1018 22:52:20.959497   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_17979_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_17979_systemd_test_default.slice: no such file or directory
Oct 18 22:51:32 k8s-master kubelet[14981]: E1018 22:51:32.448852   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:51:20 k8s-master kubelet[14981]: E1018 22:51:20.811592   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:51:20 k8s-master kubelet[14981]: W1018 22:51:20.811483   14981 container.go:354] Failed to create summary reader for "/libcontainer_17877_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:51:20 k8s-master kubelet[14981]: E1018 22:51:20.685300   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:51:20 k8s-master kubelet[14981]: W1018 22:51:20.685138   14981 container.go:354] Failed to create summary reader for "/libcontainer_17863_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:51:20 k8s-master kubelet[14981]: W1018 22:51:20.681549   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_17863_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_17863_systemd_test_default.slice: no such file or directory
Oct 18 22:51:20 k8s-master kubelet[14981]: E1018 22:51:20.438159   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:51:20 k8s-master kubelet[14981]: W1018 22:51:20.437997   14981 container.go:354] Failed to create summary reader for "/libcontainer_17846_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:51:20 k8s-master kubelet[14981]: I1018 22:51:20.422277   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:51:20 k8s-master kubelet[14981]: W1018 22:51:20.381407   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_17846_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_17846_systemd_test_default.slice: no such file or directory
Oct 18 22:51:19 k8s-master kubelet[14981]: I1018 22:51:19.803198   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:51:19 k8s-master kubelet[14981]: I1018 22:51:19.803079   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:51:19 k8s-master kubelet[14981]: I1018 22:51:19.802971   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:51:04 k8s-master kubelet[14981]: ]
Oct 18 22:51:04 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:51:04 k8s-master kubelet[14981]: E1018 22:51:04.804285   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:51:04 k8s-master kubelet[14981]: I1018 22:51:04.804240   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:51:04 k8s-master kubelet[14981]: I1018 22:51:04.804122   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:51:04 k8s-master kubelet[14981]: I1018 22:51:04.804098   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:51:04 k8s-master kubelet[14981]: I1018 22:51:04.803935   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:51:04 k8s-master kubelet[14981]: I1018 22:51:04.803819   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:51:04 k8s-master kubelet[14981]: I1018 22:51:04.803718   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:50:48 k8s-master kubelet[14981]: ]
Oct 18 22:50:48 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:50:48 k8s-master kubelet[14981]: E1018 22:50:48.806468   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:50:48 k8s-master kubelet[14981]: I1018 22:50:48.806431   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:50:48 k8s-master kubelet[14981]: I1018 22:50:48.806361   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:50:48 k8s-master kubelet[14981]: I1018 22:50:48.806353   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:50:48 k8s-master kubelet[14981]: I1018 22:50:48.806219   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:50:48 k8s-master kubelet[14981]: I1018 22:50:48.806019   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:50:48 k8s-master kubelet[14981]: I1018 22:50:48.805936   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:50:37 k8s-master kubelet[14981]: ]
Oct 18 22:50:37 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:50:37 k8s-master kubelet[14981]: E1018 22:50:37.801581   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:50:37 k8s-master kubelet[14981]: I1018 22:50:37.801581   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:50:37 k8s-master kubelet[14981]: I1018 22:50:37.801579   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:50:37 k8s-master kubelet[14981]: I1018 22:50:37.801564   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:50:37 k8s-master kubelet[14981]: I1018 22:50:37.801393   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:50:37 k8s-master kubelet[14981]: I1018 22:50:37.801160   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:50:37 k8s-master kubelet[14981]: I1018 22:50:37.801068   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:50:26 k8s-master kubelet[14981]: ]
Oct 18 22:50:26 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:50:26 k8s-master kubelet[14981]: E1018 22:50:26.830363   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:50:26 k8s-master kubelet[14981]: I1018 22:50:26.830325   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:50:26 k8s-master kubelet[14981]: I1018 22:50:26.830261   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:50:26 k8s-master kubelet[14981]: I1018 22:50:26.830251   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:50:26 k8s-master kubelet[14981]: I1018 22:50:26.830112   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:50:26 k8s-master kubelet[14981]: I1018 22:50:26.829986   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:50:26 k8s-master kubelet[14981]: I1018 22:50:26.829933   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:50:19 k8s-master kubelet[14981]: E1018 22:50:19.301989   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:50:14 k8s-master kubelet[14981]: ]
Oct 18 22:50:14 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:50:14 k8s-master kubelet[14981]: E1018 22:50:14.821419   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:50:14 k8s-master kubelet[14981]: I1018 22:50:14.821359   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:50:14 k8s-master kubelet[14981]: I1018 22:50:14.821168   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:50:14 k8s-master kubelet[14981]: I1018 22:50:14.821122   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:50:14 k8s-master kubelet[14981]: I1018 22:50:14.814538   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:50:14 k8s-master kubelet[14981]: I1018 22:50:14.814404   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:50:14 k8s-master kubelet[14981]: I1018 22:50:14.814276   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:50:01 k8s-master kubelet[14981]: ]
Oct 18 22:50:01 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:50:01 k8s-master kubelet[14981]: E1018 22:50:01.817502   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:50:01 k8s-master kubelet[14981]: I1018 22:50:01.817453   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:50:01 k8s-master kubelet[14981]: I1018 22:50:01.817354   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:50:01 k8s-master kubelet[14981]: I1018 22:50:01.817340   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:50:01 k8s-master kubelet[14981]: I1018 22:50:01.817158   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:50:01 k8s-master kubelet[14981]: I1018 22:50:01.816872   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:50:01 k8s-master kubelet[14981]: I1018 22:50:01.812354   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:49:49 k8s-master kubelet[14981]: ]
Oct 18 22:49:49 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:49:49 k8s-master kubelet[14981]: E1018 22:49:49.795425   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:49:49 k8s-master kubelet[14981]: I1018 22:49:49.795386   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:49:49 k8s-master kubelet[14981]: I1018 22:49:49.795312   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:49:49 k8s-master kubelet[14981]: I1018 22:49:49.795303   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:49:49 k8s-master kubelet[14981]: I1018 22:49:49.795177   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:49:49 k8s-master kubelet[14981]: I1018 22:49:49.794985   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:49:49 k8s-master kubelet[14981]: I1018 22:49:49.794909   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:49:36 k8s-master kubelet[14981]: ]
Oct 18 22:49:36 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:49:36 k8s-master kubelet[14981]: E1018 22:49:36.800106   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:49:36 k8s-master kubelet[14981]: I1018 22:49:36.800066   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:49:36 k8s-master kubelet[14981]: I1018 22:49:36.799999   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:49:36 k8s-master kubelet[14981]: I1018 22:49:36.799991   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:49:36 k8s-master kubelet[14981]: I1018 22:49:36.799846   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:49:36 k8s-master kubelet[14981]: I1018 22:49:36.799639   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:49:36 k8s-master kubelet[14981]: I1018 22:49:36.799568   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:49:24 k8s-master kubelet[14981]: ]
Oct 18 22:49:24 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:49:24 k8s-master kubelet[14981]: E1018 22:49:24.800319   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:49:24 k8s-master kubelet[14981]: I1018 22:49:24.800257   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:49:24 k8s-master kubelet[14981]: I1018 22:49:24.800182   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:49:24 k8s-master kubelet[14981]: I1018 22:49:24.800169   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:49:24 k8s-master kubelet[14981]: I1018 22:49:24.799326   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:49:24 k8s-master kubelet[14981]: I1018 22:49:24.799144   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI} cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:49:24 k8s-master kubelet[14981]: I1018 22:49:24.799060   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:49:13 k8s-master kubelet[14981]: ]
Oct 18 22:49:13 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:49:13 k8s-master kubelet[14981]: E1018 22:49:13.794895   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:49:13 k8s-master kubelet[14981]: I1018 22:49:13.794862   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:49:13 k8s-master kubelet[14981]: I1018 22:49:13.794796   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:49:13 k8s-master kubelet[14981]: I1018 22:49:13.794786   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:49:13 k8s-master kubelet[14981]: I1018 22:49:13.794679   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:49:13 k8s-master kubelet[14981]: I1018 22:49:13.794494   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:49:13 k8s-master kubelet[14981]: I1018 22:49:13.794422   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:49:02 k8s-master kubelet[14981]: ]
Oct 18 22:49:02 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:49:02 k8s-master kubelet[14981]: E1018 22:49:02.844774   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:49:02 k8s-master kubelet[14981]: I1018 22:49:02.844718   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:49:02 k8s-master kubelet[14981]: I1018 22:49:02.844561   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:49:02 k8s-master kubelet[14981]: I1018 22:49:02.844514   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:49:02 k8s-master kubelet[14981]: I1018 22:49:02.834243   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:49:02 k8s-master kubelet[14981]: I1018 22:49:02.832808   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:49:02 k8s-master kubelet[14981]: I1018 22:49:02.831929   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:48:48 k8s-master kubelet[14981]: ]
Oct 18 22:48:48 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:48:48 k8s-master kubelet[14981]: E1018 22:48:48.801547   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:48:48 k8s-master kubelet[14981]: I1018 22:48:48.801507   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:48:48 k8s-master kubelet[14981]: I1018 22:48:48.801445   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:48:48 k8s-master kubelet[14981]: I1018 22:48:48.801426   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:48:48 k8s-master kubelet[14981]: I1018 22:48:48.801303   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:48:48 k8s-master kubelet[14981]: I1018 22:48:48.801139   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:48:48 k8s-master kubelet[14981]: I1018 22:48:48.801064   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:48:35 k8s-master kubelet[14981]: ]
Oct 18 22:48:35 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:48:35 k8s-master kubelet[14981]: E1018 22:48:35.337238   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:48:35 k8s-master kubelet[14981]: I1018 22:48:35.337183   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:48:35 k8s-master kubelet[14981]: I1018 22:48:35.337117   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:48:35 k8s-master kubelet[14981]: I1018 22:48:35.337108   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:48:35 k8s-master kubelet[14981]: I1018 22:48:35.336971   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:48:35 k8s-master kubelet[14981]: I1018 22:48:35.336788   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:48:35 k8s-master kubelet[14981]: I1018 22:48:35.336717   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:48:34 k8s-master kubelet[14981]: ]
Oct 18 22:48:34 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:48:34 k8s-master kubelet[14981]: E1018 22:48:34.171720   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:48:34 k8s-master kubelet[14981]: I1018 22:48:34.171668   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:48:34 k8s-master kubelet[14981]: I1018 22:48:34.171489   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:48:34 k8s-master kubelet[14981]: I1018 22:48:34.171456   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:48:34 k8s-master kubelet[14981]: I1018 22:48:34.171168   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:48:34 k8s-master kubelet[14981]: I1018 22:48:34.170147   14981 kuberuntime_manager.go:499] Container {Name:dnsmasq Image:gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.5 Command:[] Args:[-v=2 -logtostderr -configDir=/etc/k8s/dns/dnsmasq-nanny -restartDnsmasq=true -- -k --cache-size=1000 --log-facility=- --server=/cluster.local/127.0.0.1#10053 --server=/in-addr.arpa/127.0.0.1#10053 --server=/ip6.arpa/127.0.0.1#10053] WorkingDir: Ports:[{Name:dns HostPort:0 ContainerPort:53 Protocol:UDP HostIP:} {Name:dns-tcp HostPort:0 ContainerPort:53 Protocol:TCP HostIP:}] EnvFrom:[] Env:[] Resources:{Limits:map[] Requests:map[cpu:{i:{value:150 scale:-3} d:{Dec:<nil>} s:150m Format:DecimalSI} memory:{i:{value:20971520 scale:0} d:{Dec:<nil>} s:20Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/etc/k8s/dns/dnsmasq-nanny SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/dnsmasq,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:nil Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:48:34 k8s-master kubelet[14981]: I1018 22:48:34.170056   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:48:33 k8s-master kubelet[14981]: ]
Oct 18 22:48:33 k8s-master kubelet[14981]: , failed to "StartContainer" for "dnsmasq" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:48:33 k8s-master kubelet[14981]: E1018 22:48:33.457073   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: [failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:48:33 k8s-master kubelet[14981]: I1018 22:48:33.456970   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=dnsmasq pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:48:33 k8s-master kubelet[14981]: I1018 22:48:33.456864   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:48:33 k8s-master kubelet[14981]: I1018 22:48:33.456842   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:48:33 k8s-master kubelet[14981]: I1018 22:48:33.456585   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:48:03 k8s-master kubelet[14981]: I1018 22:48:03.174802   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:47:53 k8s-master kubelet[14981]: E1018 22:47:53.794679   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:47:53 k8s-master kubelet[14981]: I1018 22:47:53.794651   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:47:53 k8s-master kubelet[14981]: I1018 22:47:53.794542   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:47:53 k8s-master kubelet[14981]: I1018 22:47:53.794290   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:47:42 k8s-master kubelet[14981]: E1018 22:47:42.802043   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:47:42 k8s-master kubelet[14981]: I1018 22:47:42.802000   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:47:42 k8s-master kubelet[14981]: I1018 22:47:42.801834   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:47:42 k8s-master kubelet[14981]: I1018 22:47:42.801523   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:47:27 k8s-master kubelet[14981]: E1018 22:47:27.796809   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:47:27 k8s-master kubelet[14981]: I1018 22:47:27.796771   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:47:27 k8s-master kubelet[14981]: I1018 22:47:27.796654   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:47:27 k8s-master kubelet[14981]: I1018 22:47:27.795560   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:47:13 k8s-master kubelet[14981]: E1018 22:47:13.795544   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:47:13 k8s-master kubelet[14981]: I1018 22:47:13.795513   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:47:13 k8s-master kubelet[14981]: I1018 22:47:13.795406   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:47:13 k8s-master kubelet[14981]: I1018 22:47:13.795147   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:46:59 k8s-master kubelet[14981]: E1018 22:46:59.795176   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:46:59 k8s-master kubelet[14981]: I1018 22:46:59.795145   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:46:59 k8s-master kubelet[14981]: I1018 22:46:59.795020   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:46:59 k8s-master kubelet[14981]: I1018 22:46:59.794729   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:46:46 k8s-master kubelet[14981]: E1018 22:46:46.811284   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:46:46 k8s-master kubelet[14981]: I1018 22:46:46.811252   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:46:46 k8s-master kubelet[14981]: I1018 22:46:46.811140   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:46:46 k8s-master kubelet[14981]: I1018 22:46:46.810880   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:46:32 k8s-master kubelet[14981]: E1018 22:46:32.993478   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:46:32 k8s-master kubelet[14981]: I1018 22:46:32.993445   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:46:32 k8s-master kubelet[14981]: I1018 22:46:32.993319   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:46:32 k8s-master kubelet[14981]: I1018 22:46:32.993051   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:46:32 k8s-master kubelet[14981]: E1018 22:46:32.476647   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:46:26 k8s-master kubelet[14981]: E1018 22:46:26.127053   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:46:25 k8s-master kubelet[14981]: E1018 22:46:25.881950   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:46:25 k8s-master kubelet[14981]: E1018 22:46:25.528473   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:46:25 k8s-master kubelet[14981]: I1018 22:46:25.528403   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:46:25 k8s-master kubelet[14981]: I1018 22:46:25.528214   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:46:25 k8s-master kubelet[14981]: I1018 22:46:25.527748   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:46:24 k8s-master kubelet[14981]: E1018 22:46:24.507166   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:46:24 k8s-master kubelet[14981]: I1018 22:46:24.507130   14981 kuberuntime_manager.go:748] Back-off 5m0s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:46:24 k8s-master kubelet[14981]: I1018 22:46:24.507027   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:46:24 k8s-master kubelet[14981]: I1018 22:46:24.506769   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:46:23 k8s-master kubelet[14981]: E1018 22:46:23.658883   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:46:23 k8s-master kubelet[14981]: W1018 22:46:23.658697   14981 container.go:354] Failed to create summary reader for "/libcontainer_17304_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:46:23 k8s-master kubelet[14981]: W1018 22:46:23.593575   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_17304_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_17304_systemd_test_default.slice: no such file or directory
Oct 18 22:46:23 k8s-master kubelet[14981]: E1018 22:46:23.507789   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:46:23 k8s-master kubelet[14981]: W1018 22:46:23.507642   14981 container.go:354] Failed to create summary reader for "/libcontainer_17290_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:46:23 k8s-master kubelet[14981]: W1018 22:46:23.489390   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_17290_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_17290_systemd_test_default.slice: no such file or directory
Oct 18 22:46:23 k8s-master kubelet[14981]: I1018 22:46:23.374743   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:46:23 k8s-master kubelet[14981]: E1018 22:46:23.225703   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:46:23 k8s-master kubelet[14981]: W1018 22:46:23.224794   14981 container.go:354] Failed to create summary reader for "/libcontainer_17275_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:46:23 k8s-master kubelet[14981]: W1018 22:46:23.219250   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_17275_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_17275_systemd_test_default.slice: no such file or directory
Oct 18 22:46:23 k8s-master kubelet[14981]: E1018 22:46:23.206035   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:46:21 k8s-master kubelet[14981]: E1018 22:46:21.030676   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:46:19 k8s-master kubelet[14981]: E1018 22:46:19.557506   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:46:09 k8s-master kubelet[14981]: E1018 22:46:09.458425   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:46:06 k8s-master kubelet[14981]: E1018 22:46:06.602212   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:46:06 k8s-master kubelet[14981]: W1018 22:46:06.602008   14981 container.go:354] Failed to create summary reader for "/libcontainer_17255_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:46:06 k8s-master kubelet[14981]: W1018 22:46:06.592541   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_17255_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_17255_systemd_test_default.slice: no such file or directory
Oct 18 22:45:28 k8s-master kubelet[14981]: E1018 22:45:28.871397   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:45:19 k8s-master kubelet[14981]: E1018 22:45:19.081593   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:45:17 k8s-master kubelet[14981]: E1018 22:45:17.236798   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:45:06 k8s-master kubelet[14981]: E1018 22:45:06.285925   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:45:06 k8s-master kubelet[14981]: W1018 22:45:06.276035   14981 container.go:354] Failed to create summary reader for "/libcontainer_17153_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:45:06 k8s-master kubelet[14981]: W1018 22:45:06.242430   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_17153_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_17153_systemd_test_default.slice: no such file or directory
Oct 18 22:45:05 k8s-master kubelet[14981]: I1018 22:45:05.801814   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:45:05 k8s-master kubelet[14981]: I1018 22:45:05.801471   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:44:51 k8s-master kubelet[14981]: E1018 22:44:51.813893   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:44:51 k8s-master kubelet[14981]: I1018 22:44:51.813809   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:44:51 k8s-master kubelet[14981]: I1018 22:44:51.813436   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:44:51 k8s-master kubelet[14981]: I1018 22:44:51.812933   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:44:40 k8s-master kubelet[14981]: E1018 22:44:40.805529   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:44:40 k8s-master kubelet[14981]: I1018 22:44:40.805489   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:44:40 k8s-master kubelet[14981]: I1018 22:44:40.805369   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:44:40 k8s-master kubelet[14981]: I1018 22:44:40.805072   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:44:32 k8s-master kubelet[14981]: E1018 22:44:32.943304   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:44:28 k8s-master kubelet[14981]: E1018 22:44:28.689190   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:44:28 k8s-master kubelet[14981]: E1018 22:44:28.401688   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:44:26 k8s-master kubelet[14981]: E1018 22:44:26.801549   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:44:26 k8s-master kubelet[14981]: I1018 22:44:26.801524   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:44:26 k8s-master kubelet[14981]: I1018 22:44:26.801421   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:44:26 k8s-master kubelet[14981]: I1018 22:44:26.801150   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:44:26 k8s-master kubelet[14981]: E1018 22:44:26.060308   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:44:24 k8s-master kubelet[14981]: E1018 22:44:24.960741   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:44:24 k8s-master kubelet[14981]: E1018 22:44:24.132345   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:44:15 k8s-master kubelet[14981]: E1018 22:44:15.038672   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:44:15 k8s-master kubelet[14981]: I1018 22:44:15.038621   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:44:15 k8s-master kubelet[14981]: I1018 22:44:15.038463   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:44:15 k8s-master kubelet[14981]: I1018 22:44:15.038374   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:44:14 k8s-master kubelet[14981]: E1018 22:44:14.017732   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:44:14 k8s-master kubelet[14981]: I1018 22:44:14.017732   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:44:14 k8s-master kubelet[14981]: I1018 22:44:14.017732   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:44:14 k8s-master kubelet[14981]: I1018 22:44:14.017451   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:44:13 k8s-master kubelet[14981]: E1018 22:44:13.605176   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:44:13 k8s-master kubelet[14981]: W1018 22:44:13.604367   14981 container.go:354] Failed to create summary reader for "/libcontainer_17045_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:44:13 k8s-master kubelet[14981]: E1018 22:44:13.595025   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:44:13 k8s-master kubelet[14981]: I1018 22:44:13.337226   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:44:13 k8s-master kubelet[14981]: I1018 22:44:13.337210   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:44:13 k8s-master kubelet[14981]: I1018 22:44:13.336992   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:44:13 k8s-master kubelet[14981]: E1018 22:44:13.265155   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:44:13 k8s-master kubelet[14981]: W1018 22:44:13.265017   14981 container.go:354] Failed to create summary reader for "/libcontainer_17018_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:44:13 k8s-master kubelet[14981]: W1018 22:44:13.262536   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_17018_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_17018_systemd_test_default.slice: no such file or directory
Oct 18 22:43:43 k8s-master kubelet[14981]: I1018 22:43:43.171280   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:43:39 k8s-master kubelet[14981]: E1018 22:43:39.794941   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:43:39 k8s-master kubelet[14981]: I1018 22:43:39.794910   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:43:39 k8s-master kubelet[14981]: I1018 22:43:39.794788   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:43:39 k8s-master kubelet[14981]: I1018 22:43:39.794482   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:43:27 k8s-master kubelet[14981]: E1018 22:43:27.794775   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:43:27 k8s-master kubelet[14981]: I1018 22:43:27.794739   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:43:27 k8s-master kubelet[14981]: I1018 22:43:27.794604   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:43:27 k8s-master kubelet[14981]: I1018 22:43:27.794347   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:43:15 k8s-master kubelet[14981]: E1018 22:43:15.795021   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:43:15 k8s-master kubelet[14981]: I1018 22:43:15.794987   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:43:15 k8s-master kubelet[14981]: I1018 22:43:15.794864   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:43:15 k8s-master kubelet[14981]: I1018 22:43:15.794483   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:43:02 k8s-master kubelet[14981]: E1018 22:43:02.807959   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:43:02 k8s-master kubelet[14981]: I1018 22:43:02.807913   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:43:02 k8s-master kubelet[14981]: I1018 22:43:02.807697   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:43:02 k8s-master kubelet[14981]: I1018 22:43:02.806268   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:42:47 k8s-master kubelet[14981]: E1018 22:42:47.795731   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:42:47 k8s-master kubelet[14981]: I1018 22:42:47.795695   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:42:47 k8s-master kubelet[14981]: I1018 22:42:47.795570   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:42:47 k8s-master kubelet[14981]: I1018 22:42:47.795257   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:42:33 k8s-master kubelet[14981]: E1018 22:42:33.786679   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:42:33 k8s-master kubelet[14981]: E1018 22:42:32.993771   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:42:33 k8s-master kubelet[14981]: I1018 22:42:32.993723   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:42:33 k8s-master kubelet[14981]: I1018 22:42:32.993503   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:42:32 k8s-master kubelet[14981]: I1018 22:42:32.993122   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:42:31 k8s-master kubelet[14981]: E1018 22:42:31.193160   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:42:30 k8s-master kubelet[14981]: E1018 22:42:30.523773   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:42:26 k8s-master kubelet[14981]: E1018 22:42:26.484187   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:42:24 k8s-master kubelet[14981]: E1018 22:42:24.604768   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:42:24 k8s-master kubelet[14981]: I1018 22:42:24.604736   14981 kuberuntime_manager.go:748] Back-off 2m40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:42:24 k8s-master kubelet[14981]: I1018 22:42:24.604623   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:42:24 k8s-master kubelet[14981]: I1018 22:42:24.604356   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:42:23 k8s-master kubelet[14981]: E1018 22:42:23.303340   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:42:23 k8s-master kubelet[14981]: W1018 22:42:23.303006   14981 container.go:354] Failed to create summary reader for "/libcontainer_16847_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:42:23 k8s-master kubelet[14981]: W1018 22:42:23.283948   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_16847_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_16847_systemd_test_default.slice: no such file or directory
Oct 18 22:42:21 k8s-master kubelet[14981]: E1018 22:42:21.079628   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:42:19 k8s-master kubelet[14981]: E1018 22:42:19.387633   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:42:19 k8s-master kubelet[14981]: E1018 22:42:19.363097   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:42:17 k8s-master kubelet[14981]: E1018 22:42:17.913863   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:42:16 k8s-master kubelet[14981]: E1018 22:42:16.260615   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:42:09 k8s-master kubelet[14981]: E1018 22:42:09.103810   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:42:04 k8s-master kubelet[14981]: E1018 22:42:04.001382   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:42:04 k8s-master kubelet[14981]: W1018 22:42:04.001099   14981 container.go:354] Failed to create summary reader for "/libcontainer_16807_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:42:03 k8s-master kubelet[14981]: I1018 22:42:03.658946   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:41:23 k8s-master kubelet[14981]: E1018 22:41:23.122982   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:41:23 k8s-master kubelet[14981]: W1018 22:41:23.122869   14981 container.go:354] Failed to create summary reader for "/libcontainer_16673_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:41:22 k8s-master kubelet[14981]: E1018 22:41:22.985511   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:41:22 k8s-master kubelet[14981]: W1018 22:41:22.985448   14981 container.go:354] Failed to create summary reader for "/libcontainer_16659_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:41:22 k8s-master kubelet[14981]: W1018 22:41:22.983598   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_16659_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_16659_systemd_test_default.slice: no such file or directory
Oct 18 22:41:22 k8s-master kubelet[14981]: I1018 22:41:22.798515   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:41:22 k8s-master kubelet[14981]: I1018 22:41:22.798249   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:41:10 k8s-master kubelet[14981]: E1018 22:41:10.797871   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:41:10 k8s-master kubelet[14981]: I1018 22:41:10.797837   14981 kuberuntime_manager.go:748] Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:41:10 k8s-master kubelet[14981]: I1018 22:41:10.797704   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:41:10 k8s-master kubelet[14981]: I1018 22:41:10.797355   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:40:55 k8s-master kubelet[14981]: E1018 22:40:55.794853   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:40:55 k8s-master kubelet[14981]: I1018 22:40:55.794818   14981 kuberuntime_manager.go:748] Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:40:55 k8s-master kubelet[14981]: I1018 22:40:55.794699   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:40:55 k8s-master kubelet[14981]: I1018 22:40:55.794423   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:40:43 k8s-master kubelet[14981]: E1018 22:40:43.800083   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:40:43 k8s-master kubelet[14981]: I1018 22:40:43.800005   14981 kuberuntime_manager.go:748] Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:40:43 k8s-master kubelet[14981]: I1018 22:40:43.799770   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:40:43 k8s-master kubelet[14981]: I1018 22:40:43.799248   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:40:34 k8s-master kubelet[14981]: E1018 22:40:34.259956   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:40:33 k8s-master kubelet[14981]: E1018 22:40:33.812552   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:40:31 k8s-master kubelet[14981]: E1018 22:40:31.496228   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:40:29 k8s-master kubelet[14981]: E1018 22:40:29.794866   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:40:29 k8s-master kubelet[14981]: I1018 22:40:29.794815   14981 kuberuntime_manager.go:748] Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:40:29 k8s-master kubelet[14981]: I1018 22:40:29.794634   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:40:29 k8s-master kubelet[14981]: I1018 22:40:29.794318   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:40:28 k8s-master kubelet[14981]: E1018 22:40:28.630801   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:40:28 k8s-master kubelet[14981]: E1018 22:40:28.356355   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:40:28 k8s-master kubelet[14981]: E1018 22:40:28.012946   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:40:24 k8s-master kubelet[14981]: E1018 22:40:24.137064   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:40:23 k8s-master kubelet[14981]: E1018 22:40:23.663649   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:40:19 k8s-master kubelet[14981]: E1018 22:40:19.951354   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:40:18 k8s-master kubelet[14981]: E1018 22:40:18.710174   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:40:16 k8s-master kubelet[14981]: E1018 22:40:16.798330   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:40:16 k8s-master kubelet[14981]: I1018 22:40:16.798295   14981 kuberuntime_manager.go:748] Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:40:16 k8s-master kubelet[14981]: I1018 22:40:16.798152   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:40:16 k8s-master kubelet[14981]: I1018 22:40:16.797875   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:40:16 k8s-master kubelet[14981]: E1018 22:40:16.627888   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:40:14 k8s-master kubelet[14981]: E1018 22:40:14.442456   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:40:12 k8s-master kubelet[14981]: E1018 22:40:12.528933   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:40:12 k8s-master kubelet[14981]: E1018 22:40:12.285981   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:40:10 k8s-master kubelet[14981]: E1018 22:40:10.363555   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:40:10 k8s-master kubelet[14981]: E1018 22:40:10.282415   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:40:10 k8s-master kubelet[14981]: E1018 22:40:10.150137   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:40:08 k8s-master kubelet[14981]: E1018 22:40:08.013793   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:40:07 k8s-master kubelet[14981]: E1018 22:40:07.646819   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:40:05 k8s-master kubelet[14981]: E1018 22:40:05.622241   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:40:04 k8s-master kubelet[14981]: E1018 22:40:04.481858   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:40:02 k8s-master kubelet[14981]: E1018 22:40:02.993121   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:40:02 k8s-master kubelet[14981]: I1018 22:40:02.993080   14981 kuberuntime_manager.go:748] Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:40:02 k8s-master kubelet[14981]: I1018 22:40:02.992918   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:40:02 k8s-master kubelet[14981]: I1018 22:40:02.992913   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:40:02 k8s-master kubelet[14981]: E1018 22:40:02.155819   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:39:56 k8s-master kubelet[14981]: E1018 22:39:56.182754   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:39:56 k8s-master kubelet[14981]: I1018 22:39:56.182728   14981 kuberuntime_manager.go:748] Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:39:56 k8s-master kubelet[14981]: I1018 22:39:56.182601   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:39:56 k8s-master kubelet[14981]: I1018 22:39:56.182329   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:39:55 k8s-master kubelet[14981]: E1018 22:39:55.157248   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:39:55 k8s-master kubelet[14981]: I1018 22:39:55.157210   14981 kuberuntime_manager.go:748] Back-off 1m20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:39:55 k8s-master kubelet[14981]: I1018 22:39:55.157095   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:39:55 k8s-master kubelet[14981]: I1018 22:39:55.156796   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:39:54 k8s-master kubelet[14981]: E1018 22:39:54.666879   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:39:54 k8s-master kubelet[14981]: W1018 22:39:54.666764   14981 container.go:354] Failed to create summary reader for "/libcontainer_16407_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:39:54 k8s-master kubelet[14981]: W1018 22:39:54.650798   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_16407_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_16407_systemd_test_default.slice: no such file or directory
Oct 18 22:39:54 k8s-master kubelet[14981]: E1018 22:39:54.354839   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:39:53 k8s-master kubelet[14981]: E1018 22:39:53.719199   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:39:53 k8s-master kubelet[14981]: W1018 22:39:53.718982   14981 container.go:354] Failed to create summary reader for "/libcontainer_16385_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:39:53 k8s-master kubelet[14981]: W1018 22:39:53.654536   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_16385_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_16385_systemd_test_default.slice: no such file or directory
Oct 18 22:39:53 k8s-master kubelet[14981]: W1018 22:39:53.515447   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_16371_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_16371_systemd_test_default.slice: no such file or directory
Oct 18 22:39:53 k8s-master kubelet[14981]: E1018 22:39:53.515369   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:39:53 k8s-master kubelet[14981]: W1018 22:39:53.511484   14981 container.go:354] Failed to create summary reader for "/libcontainer_16371_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:39:53 k8s-master kubelet[14981]: I1018 22:39:53.340949   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:39:53 k8s-master kubelet[14981]: E1018 22:39:53.313356   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:39:53 k8s-master kubelet[14981]: W1018 22:39:53.313197   14981 container.go:354] Failed to create summary reader for "/libcontainer_16362_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:39:53 k8s-master kubelet[14981]: W1018 22:39:53.307239   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_16362_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_16362_systemd_test_default.slice: no such file or directory
Oct 18 22:39:31 k8s-master kubelet[14981]: E1018 22:39:31.699833   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:39:22 k8s-master kubelet[14981]: E1018 22:39:22.406058   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:39:12 k8s-master kubelet[14981]: E1018 22:39:12.148058   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:39:07 k8s-master kubelet[14981]: E1018 22:39:07.254719   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:38:54 k8s-master kubelet[14981]: E1018 22:38:54.342689   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:38:54 k8s-master kubelet[14981]: W1018 22:38:54.342515   14981 container.go:354] Failed to create summary reader for "/libcontainer_16261_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:38:53 k8s-master kubelet[14981]: I1018 22:38:53.804806   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:38:53 k8s-master kubelet[14981]: I1018 22:38:53.804412   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:38:40 k8s-master kubelet[14981]: E1018 22:38:40.798729   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:38:40 k8s-master kubelet[14981]: I1018 22:38:40.798678   14981 kuberuntime_manager.go:748] Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:38:40 k8s-master kubelet[14981]: I1018 22:38:40.798523   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:38:40 k8s-master kubelet[14981]: I1018 22:38:40.798204   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:38:33 k8s-master kubelet[14981]: E1018 22:38:33.545215   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:38:32 k8s-master kubelet[14981]: E1018 22:38:32.132207   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:38:30 k8s-master kubelet[14981]: E1018 22:38:30.775441   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:38:27 k8s-master kubelet[14981]: E1018 22:38:27.152452   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:38:26 k8s-master kubelet[14981]: E1018 22:38:26.721091   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:38:26 k8s-master kubelet[14981]: E1018 22:38:26.259652   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:38:25 k8s-master kubelet[14981]: E1018 22:38:25.794783   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:38:25 k8s-master kubelet[14981]: I1018 22:38:25.794783   14981 kuberuntime_manager.go:748] Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:38:25 k8s-master kubelet[14981]: I1018 22:38:25.794783   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:38:25 k8s-master kubelet[14981]: I1018 22:38:25.794527   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:38:25 k8s-master kubelet[14981]: E1018 22:38:25.531303   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:38:25 k8s-master kubelet[14981]: E1018 22:38:25.515405   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:38:23 k8s-master kubelet[14981]: E1018 22:38:23.303844   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:38:20 k8s-master kubelet[14981]: E1018 22:38:20.937473   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:38:14 k8s-master kubelet[14981]: E1018 22:38:14.833228   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:38:12 k8s-master kubelet[14981]: E1018 22:38:12.996948   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:38:12 k8s-master kubelet[14981]: I1018 22:38:12.996915   14981 kuberuntime_manager.go:748] Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:38:12 k8s-master kubelet[14981]: I1018 22:38:12.996780   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:38:12 k8s-master kubelet[14981]: I1018 22:38:12.996416   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI} cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:38:12 k8s-master kubelet[14981]: E1018 22:38:12.333170   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:38:09 k8s-master kubelet[14981]: E1018 22:38:09.471647   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:38:08 k8s-master kubelet[14981]: E1018 22:38:08.994777   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:38:08 k8s-master kubelet[14981]: E1018 22:38:08.397811   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:38:08 k8s-master kubelet[14981]: I1018 22:38:08.397776   14981 kuberuntime_manager.go:748] Back-off 40s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:38:08 k8s-master kubelet[14981]: I1018 22:38:08.397657   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:38:08 k8s-master kubelet[14981]: I1018 22:38:08.397415   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:38:07 k8s-master kubelet[14981]: E1018 22:38:07.545611   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:38:07 k8s-master kubelet[14981]: W1018 22:38:07.545456   14981 container.go:354] Failed to create summary reader for "/libcontainer_16163_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:38:07 k8s-master kubelet[14981]: W1018 22:38:07.525449   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_16163_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_16163_systemd_test_default.slice: no such file or directory
Oct 18 22:38:06 k8s-master kubelet[14981]: E1018 22:38:06.267948   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:38:05 k8s-master kubelet[14981]: E1018 22:38:05.692132   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:37:58 k8s-master kubelet[14981]: E1018 22:37:58.832467   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:37:57 k8s-master kubelet[14981]: E1018 22:37:57.974756   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:37:55 k8s-master kubelet[14981]: E1018 22:37:55.597072   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:37:54 k8s-master kubelet[14981]: E1018 22:37:54.742519   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:37:53 k8s-master kubelet[14981]: E1018 22:37:53.601677   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:37:43 k8s-master kubelet[14981]: E1018 22:37:43.709998   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:37:43 k8s-master kubelet[14981]: W1018 22:37:43.709900   14981 container.go:354] Failed to create summary reader for "/libcontainer_16129_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:37:43 k8s-master kubelet[14981]: W1018 22:37:43.670796   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_16129_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_16129_systemd_test_default.slice: no such file or directory
Oct 18 22:37:43 k8s-master kubelet[14981]: E1018 22:37:43.571212   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:37:43 k8s-master kubelet[14981]: W1018 22:37:43.571023   14981 container.go:354] Failed to create summary reader for "/libcontainer_16115_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:37:43 k8s-master kubelet[14981]: W1018 22:37:43.568924   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_16115_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_16115_systemd_test_default.slice: no such file or directory
Oct 18 22:37:43 k8s-master kubelet[14981]: I1018 22:37:43.411221   14981 kuberuntime_manager.go:738] checking backoff for container "dnsmasq" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:37:43 k8s-master kubelet[14981]: W1018 22:37:43.350716   14981 container.go:354] Failed to create summary reader for "/libcontainer_16106_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:37:43 k8s-master kubelet[14981]: W1018 22:37:43.325157   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_16106_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_16106_systemd_test_default.slice: no such file or directory
Oct 18 22:37:26 k8s-master kubelet[14981]: E1018 22:37:26.217871   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:37:26 k8s-master kubelet[14981]: E1018 22:37:26.180679   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:37:26 k8s-master kubelet[14981]: E1018 22:37:26.066838   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:37:23 k8s-master kubelet[14981]: E1018 22:37:23.978443   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:37:23 k8s-master kubelet[14981]: E1018 22:37:23.713845   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:37:21 k8s-master kubelet[14981]: E1018 22:37:21.931748   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:37:12 k8s-master kubelet[14981]: E1018 22:37:12.576429   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:37:09 k8s-master kubelet[14981]: E1018 22:37:09.334194   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:37:07 k8s-master kubelet[14981]: E1018 22:37:07.147129   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:37:07 k8s-master kubelet[14981]: W1018 22:37:07.146935   14981 container.go:354] Failed to create summary reader for "/libcontainer_16013_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:37:06 k8s-master kubelet[14981]: I1018 22:37:06.800641   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:37:06 k8s-master kubelet[14981]: I1018 22:37:06.800408   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:37:04 k8s-master kubelet[14981]: E1018 22:37:04.189463   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:37:02 k8s-master kubelet[14981]: E1018 22:37:02.231537   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:36:53 k8s-master kubelet[14981]: E1018 22:36:53.799494   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:36:53 k8s-master kubelet[14981]: I1018 22:36:53.799436   14981 kuberuntime_manager.go:748] Back-off 20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:36:53 k8s-master kubelet[14981]: I1018 22:36:53.796752   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:36:53 k8s-master kubelet[14981]: I1018 22:36:53.796350   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:36:50 k8s-master kubelet[14981]: E1018 22:36:50.485759   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:36:47 k8s-master kubelet[14981]: E1018 22:36:47.533454   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:36:46 k8s-master kubelet[14981]: E1018 22:36:46.071925   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:36:45 k8s-master kubelet[14981]: E1018 22:36:45.666835   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:36:42 k8s-master kubelet[14981]: E1018 22:36:42.993309   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:36:42 k8s-master kubelet[14981]: I1018 22:36:42.993276   14981 kuberuntime_manager.go:748] Back-off 20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:36:42 k8s-master kubelet[14981]: I1018 22:36:42.993141   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:36:42 k8s-master kubelet[14981]: I1018 22:36:42.992840   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:36:36 k8s-master kubelet[14981]: E1018 22:36:36.353473   14981 pod_workers.go:182] Error syncing pod 46105de0-b411-11e7-b99e-08002782873b ("kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"), skipping: failed to "StartContainer" for "kubedns" with CrashLoopBackOff: "Back-off 20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:36:36 k8s-master kubelet[14981]: I1018 22:36:36.353386   14981 kuberuntime_manager.go:748] Back-off 20s restarting failed container=kubedns pod=kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)
Oct 18 22:36:36 k8s-master kubelet[14981]: I1018 22:36:36.353247   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:36:36 k8s-master kubelet[14981]: I1018 22:36:36.352854   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:36:35 k8s-master kubelet[14981]: E1018 22:36:35.382639   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:36:35 k8s-master kubelet[14981]: W1018 22:36:35.382498   14981 container.go:354] Failed to create summary reader for "/libcontainer_15957_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:36:35 k8s-master kubelet[14981]: W1018 22:36:35.366023   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_15957_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_15957_systemd_test_default.slice: no such file or directory
Oct 18 22:36:35 k8s-master kubelet[14981]: E1018 22:36:35.320511   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:36:35 k8s-master kubelet[14981]: W1018 22:36:35.320330   14981 container.go:354] Failed to create summary reader for "/libcontainer_15954_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:36:35 k8s-master kubelet[14981]: W1018 22:36:35.317724   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_15954_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_15954_systemd_test_default.slice: no such file or directory
Oct 18 22:36:23 k8s-master kubelet[14981]: E1018 22:36:23.703119   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:36:09 k8s-master kubelet[14981]: E1018 22:36:09.396245   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:35:45 k8s-master kubelet[14981]: E1018 22:35:45.499914   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:35:35 k8s-master kubelet[14981]: E1018 22:35:35.082337   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:35:35 k8s-master kubelet[14981]: W1018 22:35:35.082218   14981 container.go:354] Failed to create summary reader for "/libcontainer_15853_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:35:35 k8s-master kubelet[14981]: W1018 22:35:35.045626   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_15853_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_15853_systemd_test_default.slice: no such file or directory
Oct 18 22:35:34 k8s-master kubelet[14981]: E1018 22:35:34.888502   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:35:34 k8s-master kubelet[14981]: W1018 22:35:34.887890   14981 container.go:354] Failed to create summary reader for "/libcontainer_15839_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:35:34 k8s-master kubelet[14981]: W1018 22:35:34.864345   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_15839_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_15839_systemd_test_default.slice: no such file or directory
Oct 18 22:35:34 k8s-master kubelet[14981]: I1018 22:35:34.697494   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:35:34 k8s-master kubelet[14981]: I1018 22:35:34.696817   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:35:33 k8s-master kubelet[14981]: E1018 22:35:33.798451   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:35:33 k8s-master kubelet[14981]: W1018 22:35:33.798287   14981 container.go:354] Failed to create summary reader for "/libcontainer_15809_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:35:33 k8s-master kubelet[14981]: W1018 22:35:33.673478   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_15809_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_15809_systemd_test_default.slice: no such file or directory
Oct 18 22:35:33 k8s-master kubelet[14981]: E1018 22:35:33.365936   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:35:33 k8s-master kubelet[14981]: W1018 22:35:33.365813   14981 container.go:354] Failed to create summary reader for "/libcontainer_15785_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:35:33 k8s-master kubelet[14981]: W1018 22:35:33.345902   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_15785_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_15785_systemd_test_default.slice: no such file or directory
Oct 18 22:35:33 k8s-master kubelet[14981]: E1018 22:35:33.269097   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:35:33 k8s-master kubelet[14981]: W1018 22:35:33.269070   14981 container.go:354] Failed to create summary reader for "/libcontainer_15779_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:35:33 k8s-master kubelet[14981]: W1018 22:35:33.248055   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_15779_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_15779_systemd_test_default.slice: no such file or directory
Oct 18 22:35:24 k8s-master kubelet[14981]: E1018 22:35:24.661549   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:35:23 k8s-master kubelet[14981]: E1018 22:35:23.909532   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:35:23 k8s-master kubelet[14981]: W1018 22:35:23.907692   14981 container.go:354] Failed to create summary reader for "/libcontainer_15750_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:35:23 k8s-master kubelet[14981]: W1018 22:35:23.901655   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_15750_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_15750_systemd_test_default.slice: no such file or directory
Oct 18 22:34:34 k8s-master kubelet[14981]: E1018 22:34:34.788865   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:34:29 k8s-master kubelet[14981]: E1018 22:34:29.139594   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:34:23 k8s-master kubelet[14981]: E1018 22:34:23.084627   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:34:23 k8s-master kubelet[14981]: W1018 22:34:23.084501   14981 container.go:354] Failed to create summary reader for "/libcontainer_15646_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:34:22 k8s-master kubelet[14981]: I1018 22:34:22.711948   14981 kuberuntime_manager.go:738] checking backoff for container "kubedns" in pod "kube-dns-545bc4bfd4-nrjmv_kube-system(46105de0-b411-11e7-b99e-08002782873b)"
Oct 18 22:34:22 k8s-master kubelet[14981]: I1018 22:34:22.711650   14981 kuberuntime_manager.go:499] Container {Name:kubedns Image:gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.5 Command:[] Args:[--domain=cluster.local. --dns-port=10053 --config-dir=/kube-dns-config --v=2] WorkingDir: Ports:[{Name:dns-local HostPort:0 ContainerPort:10053 Protocol:UDP HostIP:} {Name:dns-tcp-local HostPort:0 ContainerPort:10053 Protocol:TCP HostIP:} {Name:metrics HostPort:0 ContainerPort:10055 Protocol:TCP HostIP:}] EnvFrom:[] Env:[{Name:PROMETHEUS_PORT Value:10055 ValueFrom:nil}] Resources:{Limits:map[memory:{i:{value:178257920 scale:0} d:{Dec:<nil>} s:170Mi Format:BinarySI}] Requests:map[cpu:{i:{value:100 scale:-3} d:{Dec:<nil>} s:100m Format:DecimalSI} memory:{i:{value:73400320 scale:0} d:{Dec:<nil>} s:70Mi Format:BinarySI}]} VolumeMounts:[{Name:kube-dns-config ReadOnly:false MountPath:/kube-dns-config SubPath: MountPropagation:<nil>} {Name:kube-dns-token-s6hnz ReadOnly:true MountPath:/var/run/secrets/kubernetes.io/serviceaccount SubPath: MountPropagation:<nil>}] LivenessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/healthcheck/kubedns,Port:10054,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:60,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:5,} ReadinessProbe:&Probe{Handler:Handler{Exec:nil,HTTPGet:&HTTPGetAction{Path:/readiness,Port:8081,Host:,Scheme:HTTP,HTTPHeaders:[],},TCPSocket:nil,},InitialDelaySeconds:3,TimeoutSeconds:5,PeriodSeconds:10,SuccessThreshold:1,FailureThreshold:3,} Lifecycle:nil TerminationMessagePath:/dev/termination-log TerminationMessagePolicy:File ImagePullPolicy:IfNotPresent SecurityContext:nil Stdin:false StdinOnce:false TTY:false} is dead, but RestartPolicy says that we should restart it.
Oct 18 22:33:34 k8s-master kubelet[14981]: E1018 22:33:34.828048   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:33 k8s-master kubelet[14981]: E1018 22:33:33.615710   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:33 k8s-master kubelet[14981]: E1018 22:33:33.256563   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:32 k8s-master kubelet[14981]: E1018 22:33:32.810407   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:31 k8s-master kubelet[14981]: E1018 22:33:31.362113   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:31 k8s-master kubelet[14981]: E1018 22:33:31.062223   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:29 k8s-master kubelet[14981]: E1018 22:33:29.807558   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:29 k8s-master kubelet[14981]: E1018 22:33:29.684084   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:28 k8s-master kubelet[14981]: E1018 22:33:28.120030   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:27 k8s-master kubelet[14981]: E1018 22:33:27.803113   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:25 k8s-master kubelet[14981]: E1018 22:33:25.811801   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:25 k8s-master kubelet[14981]: W1018 22:33:25.811218   14981 container.go:354] Failed to create summary reader for "/libcontainer_15484_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:33:25 k8s-master kubelet[14981]: E1018 22:33:25.164843   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:23 k8s-master kubelet[14981]: W1018 22:33:23.137864   14981 kuberuntime_container.go:191] Non-root verification doesn't support non-numeric user (nobody)
Oct 18 22:33:23 k8s-master kubelet[14981]: E1018 22:33:23.105782   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:23 k8s-master kubelet[14981]: W1018 22:33:23.105499   14981 container.go:354] Failed to create summary reader for "/libcontainer_15442_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:33:22 k8s-master kubelet[14981]: E1018 22:33:22.206169   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:20 k8s-master kubelet[14981]: E1018 22:33:20.664784   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:20 k8s-master kubelet[14981]: W1018 22:33:20.664671   14981 container.go:354] Failed to create summary reader for "/libcontainer_15402_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:33:19 k8s-master kubelet[14981]: E1018 22:33:19.788981   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:19 k8s-master kubelet[14981]: E1018 22:33:19.788976   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:19 k8s-master kubelet[14981]: W1018 22:33:19.776107   14981 container.go:354] Failed to create summary reader for "/libcontainer_15387_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:33:19 k8s-master kubelet[14981]: W1018 22:33:19.771402   14981 container.go:354] Failed to create summary reader for "/libcontainer_15375_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:33:19 k8s-master kubelet[14981]: W1018 22:33:19.149201   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_15387_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_15387_systemd_test_default.slice: no such file or directory
Oct 18 22:33:19 k8s-master kubelet[14981]: W1018 22:33:19.085821   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_15375_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_15375_systemd_test_default.slice: no such file or directory
Oct 18 22:33:18 k8s-master kubelet[14981]: E1018 22:33:18.788654   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:18 k8s-master kubelet[14981]: W1018 22:33:18.788467   14981 container.go:354] Failed to create summary reader for "/libcontainer_15346_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:33:18 k8s-master kubelet[14981]: W1018 22:33:18.782994   14981 pod_container_deletor.go:77] Container "ad5b5c0ab40a46463bbe413148941df388d0595a3ea34a35e13997aa6a7415bc" not found in pod's containers
Oct 18 22:33:18 k8s-master kubelet[14981]: W1018 22:33:18.172786   14981 pod_container_deletor.go:77] Container "917c6ff00c1fd9226fecff6916fabe9ce56f4a8c819f4f29da59ec46e4ed8627" not found in pod's containers
Oct 18 22:33:18 k8s-master kubelet[14981]: E1018 22:33:18.069207   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:18 k8s-master kubelet[14981]: W1018 22:33:18.068959   14981 container.go:354] Failed to create summary reader for "/libcontainer_15317_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:33:17 k8s-master kubelet[14981]: E1018 22:33:17.704901   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:17 k8s-master kubelet[14981]: E1018 22:33:17.353299   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:17 k8s-master kubelet[14981]: E1018 22:33:17.014713   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:16 k8s-master kubelet[14981]: E1018 22:33:16.386955   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:16 k8s-master kubelet[14981]: W1018 22:33:16.386830   14981 container.go:354] Failed to create summary reader for "/libcontainer_15295_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:33:16 k8s-master kubelet[14981]: W1018 22:33:16.384682   14981 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_15295_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_15295_systemd_test_default.slice: no such file or directory
Oct 18 22:33:14 k8s-master kubelet[14981]: E1018 22:33:14.913975   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:14 k8s-master kubelet[14981]: I1018 22:33:14.741847   14981 kubelet_network.go:276] Setting Pod CIDR:  -> 10.244.0.0/24
Oct 18 22:33:14 k8s-master kubelet[14981]: I1018 22:33:14.741517   14981 docker_service.go:306] docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}
Oct 18 22:33:14 k8s-master kubelet[14981]: I1018 22:33:14.740048   14981 kuberuntime_manager.go:898] updating runtime config through cri with podcidr 10.244.0.0/24
Oct 18 22:33:14 k8s-master kubelet[14981]: E1018 22:33:14.716217   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:14 k8s-master kubelet[14981]: I1018 22:33:14.543298   14981 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-dns-config" (UniqueName: "kubernetes.io/configmap/46105de0-b411-11e7-b99e-08002782873b-kube-dns-config") pod "kube-dns-545bc4bfd4-nrjmv" (UID: "46105de0-b411-11e7-b99e-08002782873b")
Oct 18 22:33:14 k8s-master kubelet[14981]: I1018 22:33:14.543222   14981 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-dns-token-s6hnz" (UniqueName: "kubernetes.io/secret/46105de0-b411-11e7-b99e-08002782873b-kube-dns-token-s6hnz") pod "kube-dns-545bc4bfd4-nrjmv" (UID: "46105de0-b411-11e7-b99e-08002782873b")
Oct 18 22:33:14 k8s-master kubelet[14981]: I1018 22:33:14.403841   14981 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-proxy" (UniqueName: "kubernetes.io/configmap/460f21aa-b411-11e7-b99e-08002782873b-kube-proxy") pod "kube-proxy-2jsvk" (UID: "460f21aa-b411-11e7-b99e-08002782873b")
Oct 18 22:33:14 k8s-master kubelet[14981]: I1018 22:33:14.403778   14981 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kube-proxy-token-kt8j7" (UniqueName: "kubernetes.io/secret/460f21aa-b411-11e7-b99e-08002782873b-kube-proxy-token-kt8j7") pod "kube-proxy-2jsvk" (UID: "460f21aa-b411-11e7-b99e-08002782873b")
Oct 18 22:33:14 k8s-master kubelet[14981]: I1018 22:33:14.403648   14981 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "xtables-lock" (UniqueName: "kubernetes.io/host-path/460f21aa-b411-11e7-b99e-08002782873b-xtables-lock") pod "kube-proxy-2jsvk" (UID: "460f21aa-b411-11e7-b99e-08002782873b")
Oct 18 22:33:13 k8s-master kubelet[14981]: E1018 22:33:13.022207   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:11 k8s-master kubelet[14981]: E1018 22:33:11.703169   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:01 k8s-master kubelet[14981]: E1018 22:33:01.459758   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:33:01 k8s-master kubelet[14981]: E1018 22:33:01.263029   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:32:59 k8s-master kubelet[14981]: E1018 22:32:59.943006   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:32:57 k8s-master kubelet[14981]: E1018 22:32:57.054897   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:32:55 k8s-master kubelet[14981]: E1018 22:32:55.757826   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:32:54 k8s-master kubelet[14981]: I1018 22:32:54.562195   14981 kubelet_node_status.go:86] Successfully registered node k8s-master
Oct 18 22:32:54 k8s-master kubelet[14981]: I1018 22:32:54.556685   14981 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:32:54 k8s-master kubelet[14981]: I1018 22:32:54.550875   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:48 k8s-master kubelet[14981]: E1018 22:32:48.655083   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:48 k8s-master kubelet[14981]: E1018 22:32:48.053074   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:47 k8s-master kubelet[14981]: E1018 22:32:47.854660   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:47 k8s-master kubelet[14981]: E1018 22:32:47.646732   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:47 k8s-master kubelet[14981]: E1018 22:32:47.550599   14981 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:47 k8s-master kubelet[14981]: I1018 22:32:47.549943   14981 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:32:47 k8s-master kubelet[14981]: I1018 22:32:47.546339   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:47 k8s-master kubelet[14981]: E1018 22:32:47.045001   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:46 k8s-master kubelet[14981]: E1018 22:32:46.847295   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:46 k8s-master kubelet[14981]: E1018 22:32:46.639644   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:46 k8s-master kubelet[14981]: E1018 22:32:46.029914   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:45 k8s-master kubelet[14981]: E1018 22:32:45.835006   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:45 k8s-master kubelet[14981]: E1018 22:32:45.634909   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:45 k8s-master kubelet[14981]: E1018 22:32:45.024416   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:44 k8s-master kubelet[14981]: E1018 22:32:44.886052   14981 eviction_manager.go:238] eviction manager: unexpected err: failed to get node info: node 'k8s-master' not found
Oct 18 22:32:44 k8s-master kubelet[14981]: E1018 22:32:44.827849   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:44 k8s-master kubelet[14981]: E1018 22:32:44.626865   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:44 k8s-master kubelet[14981]: E1018 22:32:44.426786   14981 kubelet.go:1612] Failed creating a mirror pod for "kube-apiserver-k8s-master_kube-system(091a47ad584983e0f8eaa912823695cf)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:44 k8s-master kubelet[14981]: E1018 22:32:44.227080   14981 kubelet.go:1612] Failed creating a mirror pod for "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:44 k8s-master kubelet[14981]: E1018 22:32:44.024938   14981 kubelet.go:1612] Failed creating a mirror pod for "kube-controller-manager-k8s-master_kube-system(edfd16bc8bf4e5fa8ff1f9f5a41bc7fb)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:43 k8s-master kubelet[14981]: E1018 22:32:43.834730   14981 kubelet.go:1612] Failed creating a mirror pod for "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:43 k8s-master kubelet[14981]: I1018 22:32:43.663938   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:43 k8s-master kubelet[14981]: I1018 22:32:43.663529   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:43 k8s-master kubelet[14981]: I1018 22:32:43.663267   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:43 k8s-master kubelet[14981]: I1018 22:32:43.662898   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:43 k8s-master kubelet[14981]: E1018 22:32:43.626704   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:43 k8s-master kubelet[14981]: E1018 22:32:43.446777   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:43 k8s-master kubelet[14981]: E1018 22:32:43.394786   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:43 k8s-master kubelet[14981]: E1018 22:32:43.270570   14981 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/default/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:32:43 k8s-master kubelet[14981]: W1018 22:32:43.028463   14981 status_manager.go:431] Failed to get status for pod "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/etcd-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:42 k8s-master kubelet[14981]: E1018 22:32:42.818651   14981 kubelet.go:1612] Failed creating a mirror pod for "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:42 k8s-master kubelet[14981]: I1018 22:32:42.649156   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:42 k8s-master kubelet[14981]: E1018 22:32:42.625114   14981 kubelet.go:1612] Failed creating a mirror pod for "kube-apiserver-k8s-master_kube-system(091a47ad584983e0f8eaa912823695cf)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:42 k8s-master kubelet[14981]: W1018 22:32:42.625028   14981 status_manager.go:431] Failed to get status for pod "kube-apiserver-k8s-master_kube-system(091a47ad584983e0f8eaa912823695cf)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:42 k8s-master kubelet[14981]: I1018 22:32:42.619010   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:42 k8s-master kubelet[14981]: E1018 22:32:42.615721   14981 kubelet.go:1612] Failed creating a mirror pod for "kube-controller-manager-k8s-master_kube-system(edfd16bc8bf4e5fa8ff1f9f5a41bc7fb)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:42 k8s-master kubelet[14981]: W1018 22:32:42.615643   14981 status_manager.go:431] Failed to get status for pod "kube-controller-manager-k8s-master_kube-system(edfd16bc8bf4e5fa8ff1f9f5a41bc7fb)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:42 k8s-master kubelet[14981]: I1018 22:32:42.600700   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:42 k8s-master kubelet[14981]: E1018 22:32:42.595477   14981 kubelet.go:1612] Failed creating a mirror pod for "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:42 k8s-master kubelet[14981]: W1018 22:32:42.595372   14981 status_manager.go:431] Failed to get status for pod "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:42 k8s-master kubelet[14981]: I1018 22:32:42.560572   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:42 k8s-master kubelet[14981]: E1018 22:32:42.435456   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:42 k8s-master kubelet[14981]: E1018 22:32:42.435316   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:42 k8s-master kubelet[14981]: E1018 22:32:42.388808   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:42 k8s-master kubelet[14981]: E1018 22:32:42.384107   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:32:42 k8s-master kubelet[14981]: W1018 22:32:42.383443   14981 container.go:354] Failed to create summary reader for "/libcontainer_15222_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:32:42 k8s-master kubelet[14981]: W1018 22:32:42.360867   14981 docker_container.go:202] Deleted previously existing symlink file: "/var/log/pods/f3e5e477637a31dd77e4d4e3534d2e23/kube-scheduler_0.log"
Oct 18 22:32:41 k8s-master kubelet[14981]: W1018 22:32:41.542443   14981 pod_container_deletor.go:77] Container "743fc4ea07060b86b830ae8d3983768372247c56fe5a76cc2628d65e87ab03f8" not found in pod's containers
Oct 18 22:32:41 k8s-master kubelet[14981]: I1018 22:32:41.530295   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:41 k8s-master kubelet[14981]: E1018 22:32:41.507194   14981 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:32:41 k8s-master kubelet[14981]: W1018 22:32:41.506990   14981 container.go:354] Failed to create summary reader for "/libcontainer_15154_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:32:41 k8s-master kubelet[14981]: E1018 22:32:41.415857   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:41 k8s-master kubelet[14981]: E1018 22:32:41.386735   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:41 k8s-master kubelet[14981]: E1018 22:32:41.369086   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:41 k8s-master kubelet[14981]: W1018 22:32:41.318303   14981 pod_container_deletor.go:77] Container "ef321e78c13ae4a7c4a5ce2c071f2b12c92531fb62fd34c79d7b3db62b8765c1" not found in pod's containers
Oct 18 22:32:41 k8s-master kubelet[14981]: I1018 22:32:41.293159   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:41 k8s-master kubelet[14981]: W1018 22:32:41.233889   14981 pod_container_deletor.go:77] Container "0b41f7558fb7f1573a70d9c82ecac7bcaf7b10f36964dff283815b2ba04d931c" not found in pod's containers
Oct 18 22:32:41 k8s-master kubelet[14981]: I1018 22:32:41.159590   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:41 k8s-master kubelet[14981]: E1018 22:32:41.141256   14981 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:41 k8s-master kubelet[14981]: I1018 22:32:41.140841   14981 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:32:41 k8s-master kubelet[14981]: I1018 22:32:41.095768   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:40 k8s-master kubelet[14981]: E1018 22:32:40.414982   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:40 k8s-master kubelet[14981]: E1018 22:32:40.381252   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:40 k8s-master kubelet[14981]: E1018 22:32:40.363161   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:39 k8s-master kubelet[14981]: E1018 22:32:39.604925   14981 kubelet.go:1612] Failed creating a mirror pod for "kube-controller-manager-k8s-master_kube-system(edfd16bc8bf4e5fa8ff1f9f5a41bc7fb)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:39 k8s-master kubelet[14981]: I1018 22:32:39.595973   14981 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/f3e5e477637a31dd77e4d4e3534d2e23-kubeconfig") pod "kube-scheduler-k8s-master" (UID: "f3e5e477637a31dd77e4d4e3534d2e23")
Oct 18 22:32:39 k8s-master kubelet[14981]: I1018 22:32:39.595948   14981 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs-etc-pki" (UniqueName: "kubernetes.io/host-path/edfd16bc8bf4e5fa8ff1f9f5a41bc7fb-ca-certs-etc-pki") pod "kube-controller-manager-k8s-master" (UID: "edfd16bc8bf4e5fa8ff1f9f5a41bc7fb")
Oct 18 22:32:39 k8s-master kubelet[14981]: I1018 22:32:39.595926   14981 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/edfd16bc8bf4e5fa8ff1f9f5a41bc7fb-kubeconfig") pod "kube-controller-manager-k8s-master" (UID: "edfd16bc8bf4e5fa8ff1f9f5a41bc7fb")
Oct 18 22:32:39 k8s-master kubelet[14981]: I1018 22:32:39.595906   14981 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs" (UniqueName: "kubernetes.io/host-path/091a47ad584983e0f8eaa912823695cf-ca-certs") pod "kube-apiserver-k8s-master" (UID: "091a47ad584983e0f8eaa912823695cf")
Oct 18 22:32:39 k8s-master kubelet[14981]: I1018 22:32:39.595882   14981 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "k8s-certs" (UniqueName: "kubernetes.io/host-path/091a47ad584983e0f8eaa912823695cf-k8s-certs") pod "kube-apiserver-k8s-master" (UID: "091a47ad584983e0f8eaa912823695cf")
Oct 18 22:32:39 k8s-master kubelet[14981]: I1018 22:32:39.595860   14981 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "etcd" (UniqueName: "kubernetes.io/host-path/40eb0889c614345e2a2714d4ee7d1cc0-etcd") pod "etcd-k8s-master" (UID: "40eb0889c614345e2a2714d4ee7d1cc0")
Oct 18 22:32:39 k8s-master kubelet[14981]: I1018 22:32:39.595838   14981 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "flexvolume-dir" (UniqueName: "kubernetes.io/host-path/edfd16bc8bf4e5fa8ff1f9f5a41bc7fb-flexvolume-dir") pod "kube-controller-manager-k8s-master" (UID: "edfd16bc8bf4e5fa8ff1f9f5a41bc7fb")
Oct 18 22:32:39 k8s-master kubelet[14981]: I1018 22:32:39.595816   14981 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs" (UniqueName: "kubernetes.io/host-path/edfd16bc8bf4e5fa8ff1f9f5a41bc7fb-ca-certs") pod "kube-controller-manager-k8s-master" (UID: "edfd16bc8bf4e5fa8ff1f9f5a41bc7fb")
Oct 18 22:32:39 k8s-master kubelet[14981]: I1018 22:32:39.595793   14981 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "k8s-certs" (UniqueName: "kubernetes.io/host-path/edfd16bc8bf4e5fa8ff1f9f5a41bc7fb-k8s-certs") pod "kube-controller-manager-k8s-master" (UID: "edfd16bc8bf4e5fa8ff1f9f5a41bc7fb")
Oct 18 22:32:39 k8s-master kubelet[14981]: I1018 22:32:39.595746   14981 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs-etc-pki" (UniqueName: "kubernetes.io/host-path/091a47ad584983e0f8eaa912823695cf-ca-certs-etc-pki") pod "kube-apiserver-k8s-master" (UID: "091a47ad584983e0f8eaa912823695cf")
Oct 18 22:32:39 k8s-master kubelet[14981]: E1018 22:32:39.571585   14981 kubelet.go:1612] Failed creating a mirror pod for "kube-apiserver-k8s-master_kube-system(091a47ad584983e0f8eaa912823695cf)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:39 k8s-master kubelet[14981]: E1018 22:32:39.559349   14981 kubelet.go:1612] Failed creating a mirror pod for "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:39 k8s-master kubelet[14981]: W1018 22:32:39.526665   14981 status_manager.go:431] Failed to get status for pod "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:39 k8s-master kubelet[14981]: E1018 22:32:39.518963   14981 kubelet.go:1612] Failed creating a mirror pod for "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:39 k8s-master kubelet[14981]: W1018 22:32:39.516993   14981 status_manager.go:431] Failed to get status for pod "kube-controller-manager-k8s-master_kube-system(edfd16bc8bf4e5fa8ff1f9f5a41bc7fb)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:39 k8s-master kubelet[14981]: I1018 22:32:39.516650   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:39 k8s-master kubelet[14981]: W1018 22:32:39.510280   14981 status_manager.go:431] Failed to get status for pod "kube-apiserver-k8s-master_kube-system(091a47ad584983e0f8eaa912823695cf)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:39 k8s-master kubelet[14981]: I1018 22:32:39.509924   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:39 k8s-master kubelet[14981]: I1018 22:32:39.508982   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:39 k8s-master kubelet[14981]: W1018 22:32:39.502525   14981 status_manager.go:431] Failed to get status for pod "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/etcd-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:39 k8s-master kubelet[14981]: I1018 22:32:39.501891   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:39 k8s-master kubelet[14981]: I1018 22:32:39.500447   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:39 k8s-master kubelet[14981]: I1018 22:32:39.496476   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:39 k8s-master kubelet[14981]: I1018 22:32:39.495889   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:39 k8s-master kubelet[14981]: I1018 22:32:39.493675   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:39 k8s-master kubelet[14981]: E1018 22:32:39.408367   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:39 k8s-master kubelet[14981]: E1018 22:32:39.379699   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:39 k8s-master kubelet[14981]: E1018 22:32:39.361327   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:38 k8s-master kubelet[14981]: E1018 22:32:38.407131   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:38 k8s-master kubelet[14981]: E1018 22:32:38.379098   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:38 k8s-master kubelet[14981]: E1018 22:32:38.360657   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:37 k8s-master kubelet[14981]: E1018 22:32:37.893758   14981 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:37 k8s-master kubelet[14981]: I1018 22:32:37.893383   14981 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:32:37 k8s-master kubelet[14981]: I1018 22:32:37.891163   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:37 k8s-master kubelet[14981]: E1018 22:32:37.406496   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:37 k8s-master kubelet[14981]: E1018 22:32:37.378526   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:37 k8s-master kubelet[14981]: E1018 22:32:37.360094   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:36 k8s-master kubelet[14981]: E1018 22:32:36.405786   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:36 k8s-master kubelet[14981]: E1018 22:32:36.377896   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:36 k8s-master kubelet[14981]: E1018 22:32:36.359555   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:36 k8s-master kubelet[14981]: E1018 22:32:36.290898   14981 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:36 k8s-master kubelet[14981]: I1018 22:32:36.290572   14981 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:32:36 k8s-master kubelet[14981]: I1018 22:32:36.287952   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:35 k8s-master kubelet[14981]: E1018 22:32:35.487190   14981 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:35 k8s-master kubelet[14981]: I1018 22:32:35.486826   14981 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:32:35 k8s-master kubelet[14981]: I1018 22:32:35.482744   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:35 k8s-master kubelet[14981]: E1018 22:32:35.403844   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:35 k8s-master kubelet[14981]: E1018 22:32:35.377276   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:35 k8s-master kubelet[14981]: E1018 22:32:35.358836   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:35 k8s-master kubelet[14981]: E1018 22:32:35.082554   14981 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:35 k8s-master kubelet[14981]: I1018 22:32:35.081290   14981 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:32:35 k8s-master kubelet[14981]: I1018 22:32:35.074585   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:34 k8s-master kubelet[14981]: E1018 22:32:34.878205   14981 eviction_manager.go:238] eviction manager: unexpected err: failed to get node info: node 'k8s-master' not found
Oct 18 22:32:34 k8s-master kubelet[14981]: E1018 22:32:34.874123   14981 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.841325   14981 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.751240   14981 manager.go:316] Recovery completed
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.613173   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.565816   14981 manager.go:311] Starting recovery of all containers
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.563958   14981 manager.go:1140] Started watching for new ooms in manager
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.563707   14981 factory.go:86] Registering Raw factory
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.563448   14981 factory.go:54] Registering systemd factory
Oct 18 22:32:34 k8s-master kubelet[14981]: W1018 22:32:34.563428   14981 manager.go:276] Registration of the crio container factory failed: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:32:34 k8s-master kubelet[14981]: W1018 22:32:34.563226   14981 manager.go:265] Registration of the rkt container factory failed: unable to communicate with Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.563185   14981 factory.go:355] Registering Docker factory
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.495161   14981 volume_manager.go:246] Starting Kubelet Volume Manager
Oct 18 22:32:34 k8s-master kubelet[14981]: E1018 22:32:34.495099   14981 container_manager_linux.go:603] [ContainerManager]: Fail to get rootfs information unable to find data for container /
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.493324   14981 kubelet.go:1779] skipping pod synchronization - [container runtime is down PLEG is not healthy: pleg was last seen active 2562047h47m16.854775807s ago; threshold is 3m0s]
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.493311   14981 kubelet.go:1768] Starting kubelet main sync loop.
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.493297   14981 status_manager.go:140] Starting to sync pod status with apiserver
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.493259   14981 fs_resource_analyzer.go:66] Starting FS ResourceAnalyzer
Oct 18 22:32:34 k8s-master kubelet[14981]: E1018 22:32:34.470660   14981 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/default/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.467376   14981 server.go:296] Adding debug handlers to kubelet server.
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.466434   14981 server.go:128] Starting to listen on 0.0.0.0:10250
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.466095   14981 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:32:34 k8s-master kubelet[14981]: E1018 22:32:34.465308   14981 kubelet.go:1234] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data for container /
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.464717   14981 server.go:718] Started kubelet v1.8.1
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.461717   14981 kuberuntime_manager.go:177] Container runtime docker initialized, version: 17.10.0-ce, apiVersion: 1.33.0
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.456631   14981 remote_runtime.go:43] Connecting to runtime service unix:///var/run/dockershim.sock
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.435188   14981 docker_service.go:224] Setting cgroupDriver to systemd
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.418001   14981 docker_service.go:207] Docker cri networking managed by kubernetes.io/no-op
Oct 18 22:32:34 k8s-master kubelet[14981]: W1018 22:32:34.408817   14981 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.396547   14981 kubelet.go:517] Hairpin mode set to "hairpin-veth"
Oct 18 22:32:34 k8s-master kubelet[14981]: W1018 22:32:34.396495   14981 kubelet_network.go:69] Hairpin mode set to "promiscuous-bridge" but kubenet is not enabled, falling back to "hairpin-veth"
Oct 18 22:32:34 k8s-master kubelet[14981]: E1018 22:32:34.356077   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:34 k8s-master kubelet[14981]: E1018 22:32:34.356039   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:34 k8s-master kubelet[14981]: E1018 22:32:34.356031   14981 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.293419   14981 kubelet.go:283] Watching apiserver
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.293379   14981 kubelet.go:273] Adding manifest file: /etc/kubernetes/manifests
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.293253   14981 container_manager_linux.go:288] Creating device plugin handler: false
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.293116   14981 container_manager_linux.go:257] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: ContainerRuntime:docker CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[{Signal:memory.available Operator:LessThan Value:{Quantity:100Mi Percentage:0} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.1} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.inodesFree Operator:LessThan Value:{Quantity:<nil> Percentage:0.05} GracePeriod:0s MinReclaim:<nil>}]} ExperimentalQOSReserved:map[] ExperimentalCPUManagerPolicy:none ExperimentalCPUManagerReconcilePeriod:10s}
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.293078   14981 container_manager_linux.go:252] container manager verified user specified cgroup-root exists: /
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.290973   14981 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.290329   14981 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.287873   14981 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.282482   14981 fs.go:140] Filesystem partitions: map[/dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0} tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0}]
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.282454   14981 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 22:32:34 k8s-master kubelet[14981]: W1018 22:32:34.262344   14981 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:32:34 k8s-master kubelet[14981]: W1018 22:32:34.262174   14981 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.242093   14981 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:32:34 k8s-master kubelet[14981]: E1018 22:32:34.240824   14981 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.240094   14981 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:32:34 k8s-master kubelet[14981]: W1018 22:32:34.212908   14981 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.212758   14981 feature_gate.go:156] feature gates: map[]
Oct 18 22:32:34 k8s-master kubelet[14981]: W1018 22:32:34.197712   14981 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.191267   14981 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.191215   14981 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.177439   14981 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.177434   14981 controller.go:114] kubelet config controller: starting controller
Oct 18 22:32:34 k8s-master kubelet[14981]: I1018 22:32:34.177352   14981 feature_gate.go:156] feature gates: map[]
Oct 18 22:32:34 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:32:34 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:32:34 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:32:23 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:32:23 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:32:23 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:32:23 k8s-master kubelet[14968]: error: unable to load client CA file /etc/kubernetes/pki/ca.crt: open /etc/kubernetes/pki/ca.crt: no such file or directory
Oct 18 22:32:23 k8s-master kubelet[14968]: I1018 22:32:23.523741   14968 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:32:23 k8s-master kubelet[14968]: I1018 22:32:23.523734   14968 controller.go:114] kubelet config controller: starting controller
Oct 18 22:32:23 k8s-master kubelet[14968]: I1018 22:32:23.523419   14968 feature_gate.go:156] feature gates: map[]
Oct 18 22:32:23 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:32:23 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:32:03 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:32:03 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:32:03 k8s-master systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Oct 18 22:32:03 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:32:03 k8s-master systemd[1]: Stopping kubelet: The Kubernetes Node Agent...
Oct 18 22:31:20 k8s-master kubelet[13964]: E1018 22:31:20.396038   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:31:09 k8s-master kubelet[13964]: E1018 22:31:09.835812   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:30:20 k8s-master kubelet[13964]: E1018 22:30:20.462061   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:25:26 k8s-master kubelet[13964]: E1018 22:25:26.635864   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:25:23 k8s-master kubelet[13964]: E1018 22:25:23.463858   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:25:22 k8s-master kubelet[13964]: E1018 22:25:22.015074   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:25:21 k8s-master kubelet[13964]: E1018 22:25:21.904832   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:25:19 k8s-master kubelet[13964]: E1018 22:25:19.687978   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:25:16 k8s-master kubelet[13964]: E1018 22:25:16.084819   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:25:15 k8s-master kubelet[13964]: E1018 22:25:15.630917   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:25:15 k8s-master kubelet[13964]: E1018 22:25:15.355705   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:25:14 k8s-master kubelet[13964]: E1018 22:25:14.939659   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:25:11 k8s-master kubelet[13964]: E1018 22:25:11.264901   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:25:08 k8s-master kubelet[13964]: E1018 22:25:08.058997   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:25:07 k8s-master kubelet[13964]: E1018 22:25:07.905064   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:25:07 k8s-master kubelet[13964]: E1018 22:25:07.278101   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:25:03 k8s-master kubelet[13964]: E1018 22:25:03.841686   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:25:03 k8s-master kubelet[13964]: I1018 22:25:03.675771   13964 kubelet_network.go:276] Setting Pod CIDR:  -> 10.244.0.0/24
Oct 18 22:25:03 k8s-master kubelet[13964]: I1018 22:25:03.675521   13964 docker_service.go:306] docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}
Oct 18 22:25:03 k8s-master kubelet[13964]: I1018 22:25:03.675281   13964 kuberuntime_manager.go:898] updating runtime config through cri with podcidr 10.244.0.0/24
Oct 18 22:24:55 k8s-master kubelet[13964]: E1018 22:24:55.959538   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:24:52 k8s-master kubelet[13964]: E1018 22:24:52.516253   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:24:52 k8s-master kubelet[13964]: E1018 22:24:52.401366   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:24:51 k8s-master kubelet[13964]: E1018 22:24:51.322884   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:24:49 k8s-master kubelet[13964]: E1018 22:24:49.856331   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:24:49 k8s-master kubelet[13964]: E1018 22:24:49.549789   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:24:48 k8s-master kubelet[13964]: E1018 22:24:48.684686   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:24:47 k8s-master kubelet[13964]: E1018 22:24:47.213475   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:24:45 k8s-master kubelet[13964]: E1018 22:24:45.472316   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:24:43 k8s-master kubelet[13964]: E1018 22:24:43.813239   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:24:43 k8s-master kubelet[13964]: E1018 22:24:43.793544   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:24:43 k8s-master kubelet[13964]: I1018 22:24:43.565018   13964 kubelet_node_status.go:86] Successfully registered node k8s-master
Oct 18 22:24:43 k8s-master kubelet[13964]: I1018 22:24:43.564985   13964 kubelet_node_status.go:134] Node k8s-master was previously registered
Oct 18 22:24:39 k8s-master kubelet[13964]: W1018 22:24:39.634603   13964 kubelet.go:1601] Deleting mirror pod "kube-controller-manager-k8s-master_kube-system(4522cb94-b40f-11e7-aa6c-08002782873b)" because it is outdated
Oct 18 22:24:39 k8s-master kubelet[13964]: W1018 22:24:39.632184   13964 kubelet.go:1601] Deleting mirror pod "kube-apiserver-k8s-master_kube-system(42c0e5b5-b40f-11e7-aa6c-08002782873b)" because it is outdated
Oct 18 22:24:39 k8s-master kubelet[13964]: W1018 22:24:39.631711   13964 kubelet.go:1601] Deleting mirror pod "kube-scheduler-k8s-master_kube-system(4a7e7b8d-b40f-11e7-aa6c-08002782873b)" because it is outdated
Oct 18 22:24:39 k8s-master kubelet[13964]: I1018 22:24:39.551157   13964 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:24:39 k8s-master kubelet[13964]: I1018 22:24:39.547921   13964 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:24:38 k8s-master kubelet[13964]: E1018 22:24:38.626888   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:38 k8s-master kubelet[13964]: E1018 22:24:38.401910   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:38 k8s-master kubelet[13964]: E1018 22:24:38.226833   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:37 k8s-master kubelet[13964]: E1018 22:24:37.606797   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:37 k8s-master kubelet[13964]: E1018 22:24:37.396073   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:37 k8s-master kubelet[13964]: E1018 22:24:37.214182   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:36 k8s-master kubelet[13964]: E1018 22:24:36.839810   13964 eviction_manager.go:238] eviction manager: unexpected err: failed to get node info: node 'k8s-master' not found
Oct 18 22:24:36 k8s-master kubelet[13964]: E1018 22:24:36.588524   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:36 k8s-master kubelet[13964]: E1018 22:24:36.588407   13964 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/default/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:24:36 k8s-master kubelet[13964]: E1018 22:24:36.389034   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:36 k8s-master kubelet[13964]: E1018 22:24:36.201931   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:36 k8s-master kubelet[13964]: E1018 22:24:36.008467   13964 kubelet.go:1612] Failed creating a mirror pod for "kube-controller-manager-k8s-master_kube-system(a61c40aac2e15292422c9e836cc018f8)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:35 k8s-master kubelet[13964]: E1018 22:24:35.796684   13964 kubelet.go:1612] Failed creating a mirror pod for "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:35 k8s-master kubelet[13964]: E1018 22:24:35.593363   13964 kubelet.go:1612] Failed creating a mirror pod for "kube-apiserver-k8s-master_kube-system(5bac89d6fa5c197aedb63c14ea44959a)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:35 k8s-master kubelet[13964]: I1018 22:24:35.459215   13964 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:24:35 k8s-master kubelet[13964]: I1018 22:24:35.459004   13964 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:24:35 k8s-master kubelet[13964]: I1018 22:24:35.458677   13964 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:24:35 k8s-master kubelet[13964]: E1018 22:24:35.387227   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:35 k8s-master kubelet[13964]: E1018 22:24:35.195038   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:35 k8s-master kubelet[13964]: E1018 22:24:35.160210   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:34 k8s-master kubelet[13964]: W1018 22:24:34.797075   13964 status_manager.go:431] Failed to get status for pod "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:34 k8s-master kubelet[13964]: E1018 22:24:34.588242   13964 kubelet.go:1612] Failed creating a mirror pod for "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:34 k8s-master kubelet[13964]: I1018 22:24:34.437503   13964 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:24:34 k8s-master kubelet[13964]: E1018 22:24:34.433828   13964 kubelet.go:1612] Failed creating a mirror pod for "kube-apiserver-k8s-master_kube-system(5bac89d6fa5c197aedb63c14ea44959a)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:34 k8s-master kubelet[13964]: W1018 22:24:34.433724   13964 status_manager.go:431] Failed to get status for pod "kube-apiserver-k8s-master_kube-system(5bac89d6fa5c197aedb63c14ea44959a)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:34 k8s-master kubelet[13964]: I1018 22:24:34.427278   13964 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:24:34 k8s-master kubelet[13964]: E1018 22:24:34.409813   13964 kubelet.go:1612] Failed creating a mirror pod for "kube-controller-manager-k8s-master_kube-system(a61c40aac2e15292422c9e836cc018f8)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:34 k8s-master kubelet[13964]: W1018 22:24:34.409739   13964 status_manager.go:431] Failed to get status for pod "kube-controller-manager-k8s-master_kube-system(a61c40aac2e15292422c9e836cc018f8)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:34 k8s-master kubelet[13964]: I1018 22:24:34.387142   13964 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:24:34 k8s-master kubelet[13964]: E1018 22:24:34.198929   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:34 k8s-master kubelet[13964]: E1018 22:24:34.155001   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:34 k8s-master kubelet[13964]: E1018 22:24:34.154935   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:33 k8s-master kubelet[13964]: E1018 22:24:33.456773   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:24:33 k8s-master kubelet[13964]: E1018 22:24:33.440167   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:24:33 k8s-master kubelet[13964]: W1018 22:24:33.440084   13964 container.go:354] Failed to create summary reader for "/libcontainer_14275_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:24:33 k8s-master kubelet[13964]: W1018 22:24:33.439974   13964 container.go:354] Failed to create summary reader for "/libcontainer_14262_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:24:33 k8s-master kubelet[13964]: W1018 22:24:33.387880   13964 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_14275_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_14275_systemd_test_default.slice: no such file or directory
Oct 18 22:24:33 k8s-master kubelet[13964]: E1018 22:24:33.366786   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:24:33 k8s-master kubelet[13964]: W1018 22:24:33.336137   13964 container.go:354] Failed to create summary reader for "/libcontainer_14243_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:24:33 k8s-master kubelet[13964]: W1018 22:24:33.260918   13964 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_14262_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_14262_systemd_test_default.slice: no such file or directory
Oct 18 22:24:33 k8s-master kubelet[13964]: E1018 22:24:33.193870   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:33 k8s-master kubelet[13964]: E1018 22:24:33.147923   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:33 k8s-master kubelet[13964]: E1018 22:24:33.147686   13964 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:33 k8s-master kubelet[13964]: I1018 22:24:33.147265   13964 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:24:33 k8s-master kubelet[13964]: E1018 22:24:33.147046   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:33 k8s-master kubelet[13964]: E1018 22:24:33.032211   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:24:33 k8s-master kubelet[13964]: E1018 22:24:33.032114   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:24:33 k8s-master kubelet[13964]: W1018 22:24:33.028641   13964 container.go:354] Failed to create summary reader for "/libcontainer_14226_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:24:33 k8s-master kubelet[13964]: W1018 22:24:33.028538   13964 container.go:354] Failed to create summary reader for "/libcontainer_14216_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:24:32 k8s-master kubelet[13964]: I1018 22:24:32.967676   13964 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:24:32 k8s-master kubelet[13964]: W1018 22:24:32.935779   13964 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_14226_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_14226_systemd_test_default.slice: no such file or directory
Oct 18 22:24:32 k8s-master kubelet[13964]: E1018 22:24:32.705971   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:24:32 k8s-master kubelet[13964]: E1018 22:24:32.705807   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:24:32 k8s-master kubelet[13964]: E1018 22:24:32.699360   13964 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:24:32 k8s-master kubelet[13964]: W1018 22:24:32.684797   13964 container.go:354] Failed to create summary reader for "/libcontainer_14201_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:24:32 k8s-master kubelet[13964]: W1018 22:24:32.684703   13964 container.go:354] Failed to create summary reader for "/libcontainer_14185_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:24:32 k8s-master kubelet[13964]: W1018 22:24:32.684575   13964 container.go:354] Failed to create summary reader for "/libcontainer_14173_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:24:32 k8s-master kubelet[13964]: E1018 22:24:32.191276   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:32 k8s-master kubelet[13964]: E1018 22:24:32.095968   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:32 k8s-master kubelet[13964]: E1018 22:24:32.052250   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:31 k8s-master kubelet[13964]: E1018 22:24:31.678518   13964 kubelet.go:1612] Failed creating a mirror pod for "etcd-k8s-master_kube-system(632cf36548152dcede5f0a5254ea0e6d)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:31 k8s-master kubelet[13964]: W1018 22:24:31.678449   13964 status_manager.go:431] Failed to get status for pod "etcd-k8s-master_kube-system(632cf36548152dcede5f0a5254ea0e6d)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/etcd-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:31 k8s-master kubelet[13964]: I1018 22:24:31.673421   13964 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:24:31 k8s-master kubelet[13964]: I1018 22:24:31.344304   13964 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/f3e5e477637a31dd77e4d4e3534d2e23-kubeconfig") pod "kube-scheduler-k8s-master" (UID: "f3e5e477637a31dd77e4d4e3534d2e23")
Oct 18 22:24:31 k8s-master kubelet[13964]: I1018 22:24:31.344257   13964 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs" (UniqueName: "kubernetes.io/host-path/5bac89d6fa5c197aedb63c14ea44959a-ca-certs") pod "kube-apiserver-k8s-master" (UID: "5bac89d6fa5c197aedb63c14ea44959a")
Oct 18 22:24:31 k8s-master kubelet[13964]: I1018 22:24:31.344216   13964 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "k8s-certs" (UniqueName: "kubernetes.io/host-path/5bac89d6fa5c197aedb63c14ea44959a-k8s-certs") pod "kube-apiserver-k8s-master" (UID: "5bac89d6fa5c197aedb63c14ea44959a")
Oct 18 22:24:31 k8s-master kubelet[13964]: I1018 22:24:31.344182   13964 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "etcd" (UniqueName: "kubernetes.io/host-path/632cf36548152dcede5f0a5254ea0e6d-etcd") pod "etcd-k8s-master" (UID: "632cf36548152dcede5f0a5254ea0e6d")
Oct 18 22:24:31 k8s-master kubelet[13964]: I1018 22:24:31.344141   13964 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs-etc-pki" (UniqueName: "kubernetes.io/host-path/5bac89d6fa5c197aedb63c14ea44959a-ca-certs-etc-pki") pod "kube-apiserver-k8s-master" (UID: "5bac89d6fa5c197aedb63c14ea44959a")
Oct 18 22:24:31 k8s-master kubelet[13964]: I1018 22:24:31.344099   13964 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs-etc-pki" (UniqueName: "kubernetes.io/host-path/a61c40aac2e15292422c9e836cc018f8-ca-certs-etc-pki") pod "kube-controller-manager-k8s-master" (UID: "a61c40aac2e15292422c9e836cc018f8")
Oct 18 22:24:31 k8s-master kubelet[13964]: I1018 22:24:31.344063   13964 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "flexvolume-dir" (UniqueName: "kubernetes.io/host-path/a61c40aac2e15292422c9e836cc018f8-flexvolume-dir") pod "kube-controller-manager-k8s-master" (UID: "a61c40aac2e15292422c9e836cc018f8")
Oct 18 22:24:31 k8s-master kubelet[13964]: I1018 22:24:31.344022   13964 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/a61c40aac2e15292422c9e836cc018f8-kubeconfig") pod "kube-controller-manager-k8s-master" (UID: "a61c40aac2e15292422c9e836cc018f8")
Oct 18 22:24:31 k8s-master kubelet[13964]: I1018 22:24:31.343944   13964 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs" (UniqueName: "kubernetes.io/host-path/a61c40aac2e15292422c9e836cc018f8-ca-certs") pod "kube-controller-manager-k8s-master" (UID: "a61c40aac2e15292422c9e836cc018f8")
Oct 18 22:24:31 k8s-master kubelet[13964]: I1018 22:24:31.343903   13964 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "k8s-certs" (UniqueName: "kubernetes.io/host-path/a61c40aac2e15292422c9e836cc018f8-k8s-certs") pod "kube-controller-manager-k8s-master" (UID: "a61c40aac2e15292422c9e836cc018f8")
Oct 18 22:24:31 k8s-master kubelet[13964]: E1018 22:24:31.343377   13964 kubelet.go:1612] Failed creating a mirror pod for "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:31 k8s-master kubelet[13964]: E1018 22:24:31.329344   13964 kubelet.go:1612] Failed creating a mirror pod for "kube-controller-manager-k8s-master_kube-system(a61c40aac2e15292422c9e836cc018f8)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:31 k8s-master kubelet[13964]: E1018 22:24:31.305371   13964 kubelet.go:1612] Failed creating a mirror pod for "kube-apiserver-k8s-master_kube-system(5bac89d6fa5c197aedb63c14ea44959a)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:31 k8s-master kubelet[13964]: W1018 22:24:31.262464   13964 status_manager.go:431] Failed to get status for pod "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:31 k8s-master kubelet[13964]: W1018 22:24:31.258553   13964 status_manager.go:431] Failed to get status for pod "kube-controller-manager-k8s-master_kube-system(a61c40aac2e15292422c9e836cc018f8)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:31 k8s-master kubelet[13964]: I1018 22:24:31.256133   13964 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:24:31 k8s-master kubelet[13964]: W1018 22:24:31.253338   13964 status_manager.go:431] Failed to get status for pod "kube-apiserver-k8s-master_kube-system(5bac89d6fa5c197aedb63c14ea44959a)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:31 k8s-master kubelet[13964]: I1018 22:24:31.253019   13964 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:24:31 k8s-master kubelet[13964]: I1018 22:24:31.250138   13964 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:24:31 k8s-master kubelet[13964]: E1018 22:24:31.246925   13964 kubelet.go:1612] Failed creating a mirror pod for "etcd-k8s-master_kube-system(632cf36548152dcede5f0a5254ea0e6d)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:31 k8s-master kubelet[13964]: W1018 22:24:31.246859   13964 status_manager.go:431] Failed to get status for pod "etcd-k8s-master_kube-system(632cf36548152dcede5f0a5254ea0e6d)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/etcd-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:31 k8s-master kubelet[13964]: I1018 22:24:31.246316   13964 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:24:31 k8s-master kubelet[13964]: I1018 22:24:31.245620   13964 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:24:31 k8s-master kubelet[13964]: I1018 22:24:31.242299   13964 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:24:31 k8s-master kubelet[13964]: I1018 22:24:31.241741   13964 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:24:31 k8s-master kubelet[13964]: I1018 22:24:31.236633   13964 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:24:31 k8s-master kubelet[13964]: E1018 22:24:31.098683   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:31 k8s-master kubelet[13964]: E1018 22:24:31.093960   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:31 k8s-master kubelet[13964]: E1018 22:24:31.051560   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:30 k8s-master kubelet[13964]: E1018 22:24:30.098056   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:30 k8s-master kubelet[13964]: E1018 22:24:30.092214   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:30 k8s-master kubelet[13964]: E1018 22:24:30.050860   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:29 k8s-master kubelet[13964]: E1018 22:24:29.733232   13964 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:29 k8s-master kubelet[13964]: I1018 22:24:29.732931   13964 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:24:29 k8s-master kubelet[13964]: I1018 22:24:29.731055   13964 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:24:29 k8s-master kubelet[13964]: E1018 22:24:29.097446   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:29 k8s-master kubelet[13964]: E1018 22:24:29.091641   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:29 k8s-master kubelet[13964]: E1018 22:24:29.050197   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:28 k8s-master kubelet[13964]: E1018 22:24:28.130870   13964 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:28 k8s-master kubelet[13964]: I1018 22:24:28.130540   13964 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:24:28 k8s-master kubelet[13964]: I1018 22:24:28.127776   13964 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:24:28 k8s-master kubelet[13964]: E1018 22:24:28.096706   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:28 k8s-master kubelet[13964]: E1018 22:24:28.090286   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:28 k8s-master kubelet[13964]: E1018 22:24:28.049507   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:27 k8s-master kubelet[13964]: E1018 22:24:27.327437   13964 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:27 k8s-master kubelet[13964]: I1018 22:24:27.327112   13964 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:24:27 k8s-master kubelet[13964]: I1018 22:24:27.324286   13964 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:24:27 k8s-master kubelet[13964]: E1018 22:24:27.095977   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:27 k8s-master kubelet[13964]: E1018 22:24:27.089650   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:27 k8s-master kubelet[13964]: E1018 22:24:27.048755   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:26 k8s-master kubelet[13964]: E1018 22:24:26.923936   13964 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:26 k8s-master kubelet[13964]: I1018 22:24:26.923496   13964 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:24:26 k8s-master kubelet[13964]: I1018 22:24:26.917288   13964 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:24:26 k8s-master kubelet[13964]: E1018 22:24:26.829375   13964 eviction_manager.go:238] eviction manager: unexpected err: failed to get node info: node 'k8s-master' not found
Oct 18 22:24:26 k8s-master kubelet[13964]: E1018 22:24:26.716860   13964 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:26 k8s-master kubelet[13964]: I1018 22:24:26.667763   13964 manager.go:316] Recovery completed
Oct 18 22:24:26 k8s-master kubelet[13964]: I1018 22:24:26.659441   13964 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:24:26 k8s-master kubelet[13964]: E1018 22:24:26.573926   13964 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/default/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:24:26 k8s-master kubelet[13964]: I1018 22:24:26.363042   13964 manager.go:311] Starting recovery of all containers
Oct 18 22:24:26 k8s-master kubelet[13964]: I1018 22:24:26.361801   13964 manager.go:1140] Started watching for new ooms in manager
Oct 18 22:24:26 k8s-master kubelet[13964]: I1018 22:24:26.361630   13964 factory.go:86] Registering Raw factory
Oct 18 22:24:26 k8s-master kubelet[13964]: I1018 22:24:26.361465   13964 factory.go:54] Registering systemd factory
Oct 18 22:24:26 k8s-master kubelet[13964]: W1018 22:24:26.361452   13964 manager.go:276] Registration of the crio container factory failed: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:24:26 k8s-master kubelet[13964]: W1018 22:24:26.361304   13964 manager.go:265] Registration of the rkt container factory failed: unable to communicate with Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:24:26 k8s-master kubelet[13964]: I1018 22:24:26.361271   13964 factory.go:355] Registering Docker factory
Oct 18 22:24:26 k8s-master kubelet[13964]: I1018 22:24:26.357975   13964 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:24:26 k8s-master kubelet[13964]: I1018 22:24:26.236004   13964 volume_manager.go:246] Starting Kubelet Volume Manager
Oct 18 22:24:26 k8s-master kubelet[13964]: E1018 22:24:26.235985   13964 container_manager_linux.go:603] [ContainerManager]: Fail to get rootfs information unable to find data for container /
Oct 18 22:24:26 k8s-master kubelet[13964]: I1018 22:24:26.235544   13964 kubelet.go:1779] skipping pod synchronization - [container runtime is down PLEG is not healthy: pleg was last seen active 2562047h47m16.854775807s ago; threshold is 3m0s]
Oct 18 22:24:26 k8s-master kubelet[13964]: I1018 22:24:26.235529   13964 kubelet.go:1768] Starting kubelet main sync loop.
Oct 18 22:24:26 k8s-master kubelet[13964]: I1018 22:24:26.235516   13964 status_manager.go:140] Starting to sync pod status with apiserver
Oct 18 22:24:26 k8s-master kubelet[13964]: I1018 22:24:26.235479   13964 fs_resource_analyzer.go:66] Starting FS ResourceAnalyzer
Oct 18 22:24:26 k8s-master kubelet[13964]: E1018 22:24:26.213723   13964 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/default/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:24:26 k8s-master kubelet[13964]: I1018 22:24:26.211201   13964 server.go:296] Adding debug handlers to kubelet server.
Oct 18 22:24:26 k8s-master kubelet[13964]: I1018 22:24:26.210480   13964 server.go:128] Starting to listen on 0.0.0.0:10250
Oct 18 22:24:26 k8s-master kubelet[13964]: I1018 22:24:26.210175   13964 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:24:26 k8s-master kubelet[13964]: E1018 22:24:26.209648   13964 kubelet.go:1234] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data for container /
Oct 18 22:24:26 k8s-master kubelet[13964]: I1018 22:24:26.209008   13964 server.go:718] Started kubelet v1.8.1
Oct 18 22:24:26 k8s-master kubelet[13964]: I1018 22:24:26.204761   13964 kuberuntime_manager.go:177] Container runtime docker initialized, version: 17.10.0-ce, apiVersion: 1.33.0
Oct 18 22:24:26 k8s-master kubelet[13964]: I1018 22:24:26.202334   13964 remote_runtime.go:43] Connecting to runtime service unix:///var/run/dockershim.sock
Oct 18 22:24:26 k8s-master kubelet[13964]: I1018 22:24:26.161517   13964 docker_service.go:224] Setting cgroupDriver to systemd
Oct 18 22:24:26 k8s-master kubelet[13964]: I1018 22:24:26.138058   13964 docker_service.go:207] Docker cri networking managed by kubernetes.io/no-op
Oct 18 22:24:26 k8s-master kubelet[13964]: W1018 22:24:26.102898   13964 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:24:26 k8s-master kubelet[13964]: I1018 22:24:26.095638   13964 kubelet.go:517] Hairpin mode set to "hairpin-veth"
Oct 18 22:24:26 k8s-master kubelet[13964]: W1018 22:24:26.095612   13964 kubelet_network.go:69] Hairpin mode set to "promiscuous-bridge" but kubenet is not enabled, falling back to "hairpin-veth"
Oct 18 22:24:26 k8s-master kubelet[13964]: E1018 22:24:26.048171   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:26 k8s-master kubelet[13964]: E1018 22:24:26.048126   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:26 k8s-master kubelet[13964]: E1018 22:24:26.048054   13964 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:25 k8s-master kubelet[13964]: I1018 22:24:25.916311   13964 kubelet.go:283] Watching apiserver
Oct 18 22:24:25 k8s-master kubelet[13964]: I1018 22:24:25.916252   13964 kubelet.go:273] Adding manifest file: /etc/kubernetes/manifests
Oct 18 22:24:25 k8s-master kubelet[13964]: I1018 22:24:25.916047   13964 container_manager_linux.go:288] Creating device plugin handler: false
Oct 18 22:24:25 k8s-master kubelet[13964]: I1018 22:24:25.915511   13964 container_manager_linux.go:257] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: ContainerRuntime:docker CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[{Signal:memory.available Operator:LessThan Value:{Quantity:100Mi Percentage:0} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.1} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.inodesFree Operator:LessThan Value:{Quantity:<nil> Percentage:0.05} GracePeriod:0s MinReclaim:<nil>}]} ExperimentalQOSReserved:map[] ExperimentalCPUManagerPolicy:none ExperimentalCPUManagerReconcilePeriod:10s}
Oct 18 22:24:25 k8s-master kubelet[13964]: I1018 22:24:25.915030   13964 container_manager_linux.go:252] container manager verified user specified cgroup-root exists: /
Oct 18 22:24:25 k8s-master kubelet[13964]: I1018 22:24:25.904154   13964 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:24:25 k8s-master kubelet[13964]: I1018 22:24:25.903603   13964 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:24:25 k8s-master kubelet[13964]: I1018 22:24:25.900916   13964 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true} {Device:shm DeviceMajor:0 DeviceMinor:45 Capacity:67108864 Type:vfs Inodes:235563 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true}] DiskMap:map[253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq} 253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:24:25 k8s-master kubelet[13964]: I1018 22:24:25.898899   13964 fs.go:140] Filesystem partitions: map[/dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0} shm:{mountpoint:/var/lib/docker/containers/ae752291e088659adba78f215ccad2d35546490ffa09bc55105b0b57fa989df8/shm major:0 minor:45 fsType:tmpfs blockSize:0} tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0}]
Oct 18 22:24:25 k8s-master kubelet[13964]: I1018 22:24:25.898870   13964 fs.go:139] Filesystem UUIDs: map[a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0 752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1]
Oct 18 22:24:25 k8s-master kubelet[13964]: W1018 22:24:25.873261   13964 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:24:25 k8s-master kubelet[13964]: W1018 22:24:25.873134   13964 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:24:25 k8s-master kubelet[13964]: I1018 22:24:25.839232   13964 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:24:25 k8s-master kubelet[13964]: E1018 22:24:25.836460   13964 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:25 k8s-master kubelet[13964]: I1018 22:24:25.827075   13964 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:24:25 k8s-master kubelet[13964]: W1018 22:24:25.771472   13964 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:24:25 k8s-master kubelet[13964]: I1018 22:24:25.771327   13964 feature_gate.go:156] feature gates: map[]
Oct 18 22:24:25 k8s-master kubelet[13964]: W1018 22:24:25.741762   13964 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:24:25 k8s-master kubelet[13964]: I1018 22:24:25.739070   13964 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:24:25 k8s-master kubelet[13964]: I1018 22:24:25.739022   13964 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:24:25 k8s-master kubelet[13964]: I1018 22:24:25.693184   13964 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:24:25 k8s-master kubelet[13964]: I1018 22:24:25.693179   13964 controller.go:114] kubelet config controller: starting controller
Oct 18 22:24:25 k8s-master kubelet[13964]: I1018 22:24:25.693031   13964 feature_gate.go:156] feature gates: map[]
Oct 18 22:24:25 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:24:25 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:24:25 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:24:25 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:24:25 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:24:25 k8s-master systemd[1]: Stopping kubelet: The Kubernetes Node Agent...
Oct 18 22:24:25 k8s-master kubelet[12990]: E1018 22:24:25.020441   12990 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:24:24 k8s-master kubelet[12990]: E1018 22:24:24.594750   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:24 k8s-master kubelet[12990]: E1018 22:24:24.578306   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:24 k8s-master kubelet[12990]: E1018 22:24:24.577305   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:23 k8s-master kubelet[12990]: E1018 22:24:23.594173   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:23 k8s-master kubelet[12990]: E1018 22:24:23.577842   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:23 k8s-master kubelet[12990]: E1018 22:24:23.576672   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:23 k8s-master kubelet[12990]: E1018 22:24:23.542991   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:23 k8s-master kubelet[12990]: E1018 22:24:23.542877   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:23 k8s-master kubelet[12990]: E1018 22:24:23.542733   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:22 k8s-master kubelet[12990]: E1018 22:24:22.981030   12990 kubelet_node_status.go:378] Unable to update node status: update node status exceeds retry count
Oct 18 22:24:22 k8s-master kubelet[12990]: E1018 22:24:22.981023   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:22 k8s-master kubelet[12990]: E1018 22:24:22.980894   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:22 k8s-master kubelet[12990]: E1018 22:24:22.980764   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:22 k8s-master kubelet[12990]: E1018 22:24:22.980621   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:22 k8s-master kubelet[12990]: E1018 22:24:22.980460   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:22 k8s-master kubelet[12990]: E1018 22:24:22.593350   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:22 k8s-master kubelet[12990]: E1018 22:24:22.577106   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:22 k8s-master kubelet[12990]: E1018 22:24:22.575984   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:22 k8s-master kubelet[12990]: E1018 22:24:22.450777   12990 pod_workers.go:182] Error syncing pod 5bac89d6fa5c197aedb63c14ea44959a ("kube-apiserver-k8s-master_kube-system(5bac89d6fa5c197aedb63c14ea44959a)"), skipping: timeout expired waiting for volumes to attach/mount for pod "kube-system"/"kube-apiserver-k8s-master". list of unattached/unmounted volumes=[k8s-certs ca-certs ca-certs-etc-pki]
Oct 18 22:24:22 k8s-master kubelet[12990]: E1018 22:24:22.450695   12990 kubelet.go:1628] Unable to mount volumes for pod "kube-apiserver-k8s-master_kube-system(5bac89d6fa5c197aedb63c14ea44959a)": timeout expired waiting for volumes to attach/mount for pod "kube-system"/"kube-apiserver-k8s-master". list of unattached/unmounted volumes=[k8s-certs ca-certs ca-certs-etc-pki]; skipping pod
Oct 18 22:24:21 k8s-master kubelet[12990]: E1018 22:24:21.591596   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:21 k8s-master kubelet[12990]: E1018 22:24:21.576691   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:21 k8s-master kubelet[12990]: E1018 22:24:21.575311   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:21 k8s-master kubelet[12990]: E1018 22:24:21.562267   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:21 k8s-master kubelet[12990]: E1018 22:24:21.562147   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:21 k8s-master kubelet[12990]: E1018 22:24:21.561932   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:20 k8s-master kubelet[12990]: E1018 22:24:20.591049   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:20 k8s-master kubelet[12990]: E1018 22:24:20.576203   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:20 k8s-master kubelet[12990]: E1018 22:24:20.574696   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:19 k8s-master kubelet[12990]: E1018 22:24:19.590405   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:19 k8s-master kubelet[12990]: E1018 22:24:19.575791   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:19 k8s-master kubelet[12990]: E1018 22:24:19.573970   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:19 k8s-master kubelet[12990]: E1018 22:24:19.548263   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:19 k8s-master kubelet[12990]: E1018 22:24:19.548135   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:19 k8s-master kubelet[12990]: E1018 22:24:19.547982   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:18 k8s-master kubelet[12990]: E1018 22:24:18.589758   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:18 k8s-master kubelet[12990]: E1018 22:24:18.575343   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:18 k8s-master kubelet[12990]: E1018 22:24:18.573366   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:17 k8s-master kubelet[12990]: E1018 22:24:17.588770   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:17 k8s-master kubelet[12990]: E1018 22:24:17.574862   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:17 k8s-master kubelet[12990]: E1018 22:24:17.572748   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:17 k8s-master kubelet[12990]: E1018 22:24:17.545823   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:17 k8s-master kubelet[12990]: E1018 22:24:17.545647   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:17 k8s-master kubelet[12990]: E1018 22:24:17.545431   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:16 k8s-master kubelet[12990]: E1018 22:24:16.585764   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:16 k8s-master kubelet[12990]: E1018 22:24:16.574341   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:16 k8s-master kubelet[12990]: E1018 22:24:16.572147   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:15 k8s-master kubelet[12990]: E1018 22:24:15.585147   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:15 k8s-master kubelet[12990]: E1018 22:24:15.573707   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:15 k8s-master kubelet[12990]: E1018 22:24:15.571470   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:15 k8s-master kubelet[12990]: E1018 22:24:15.545126   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:15 k8s-master kubelet[12990]: E1018 22:24:15.544995   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:15 k8s-master kubelet[12990]: E1018 22:24:15.544852   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:15 k8s-master kubelet[12990]: E1018 22:24:15.019772   12990 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:24:14 k8s-master kubelet[12990]: E1018 22:24:14.584532   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:14 k8s-master kubelet[12990]: E1018 22:24:14.571943   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:14 k8s-master kubelet[12990]: E1018 22:24:14.570864   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:13 k8s-master kubelet[12990]: E1018 22:24:13.583917   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:13 k8s-master kubelet[12990]: E1018 22:24:13.571522   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:13 k8s-master kubelet[12990]: E1018 22:24:13.570173   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:13 k8s-master kubelet[12990]: E1018 22:24:13.543251   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:13 k8s-master kubelet[12990]: E1018 22:24:13.543124   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:13 k8s-master kubelet[12990]: E1018 22:24:13.542936   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:12 k8s-master kubelet[12990]: E1018 22:24:12.979839   12990 kubelet_node_status.go:378] Unable to update node status: update node status exceeds retry count
Oct 18 22:24:12 k8s-master kubelet[12990]: E1018 22:24:12.979833   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:12 k8s-master kubelet[12990]: E1018 22:24:12.979701   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:12 k8s-master kubelet[12990]: E1018 22:24:12.979571   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:12 k8s-master kubelet[12990]: E1018 22:24:12.979426   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:12 k8s-master kubelet[12990]: E1018 22:24:12.979252   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:12 k8s-master kubelet[12990]: E1018 22:24:12.583313   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:12 k8s-master kubelet[12990]: E1018 22:24:12.571057   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:12 k8s-master kubelet[12990]: E1018 22:24:12.569565   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:11 k8s-master kubelet[12990]: E1018 22:24:11.582730   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:11 k8s-master kubelet[12990]: E1018 22:24:11.570355   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:11 k8s-master kubelet[12990]: E1018 22:24:11.568885   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:11 k8s-master kubelet[12990]: E1018 22:24:11.544718   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:11 k8s-master kubelet[12990]: E1018 22:24:11.544598   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:11 k8s-master kubelet[12990]: E1018 22:24:11.544434   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:10 k8s-master kubelet[12990]: E1018 22:24:10.582141   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:10 k8s-master kubelet[12990]: E1018 22:24:10.569834   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:10 k8s-master kubelet[12990]: E1018 22:24:10.568270   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:09 k8s-master kubelet[12990]: E1018 22:24:09.581530   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:09 k8s-master kubelet[12990]: E1018 22:24:09.569364   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:09 k8s-master kubelet[12990]: E1018 22:24:09.567614   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:09 k8s-master kubelet[12990]: E1018 22:24:09.559632   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:09 k8s-master kubelet[12990]: E1018 22:24:09.559469   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:09 k8s-master kubelet[12990]: E1018 22:24:09.559324   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:08 k8s-master kubelet[12990]: E1018 22:24:08.580908   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:08 k8s-master kubelet[12990]: E1018 22:24:08.568713   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:08 k8s-master kubelet[12990]: E1018 22:24:08.567035   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:07 k8s-master kubelet[12990]: E1018 22:24:07.599211   12990 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:24:07 k8s-master kubelet[12990]: E1018 22:24:07.598929   12990 event.go:144] Unable to write event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"kube-apiserver-k8s-master.14eeb0175d941a2a", GenerateName:"", Namespace:"kube-system", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{sec:0, nsec:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:""}, InvolvedObject:v1.ObjectReference{Kind:"Pod", Namespace:"kube-system", Name:"kube-apiserver-k8s-master", UID:"c4260572c37ef028f5a8df0c2e45419f", APIVersion:"v1", ResourceVersion:"", FieldPath:"spec.containers{kube-apiserver}"}, Reason:"Killing", Message:"Killing container with id docker://kube-apiserver:Need to kill Pod", Source:v1.EventSource{Component:"kubelet", Host:"k8s-master"}, FirstTimestamp:v1.Time{Time:time.Time{sec:63643933339, nsec:616418346, loc:(*time.Location)(0x5314a60)}}, LastTimestamp:v1.Time{Time:time.Time{sec:63643933339, nsec:616418346, loc:(*time.Location)(0x5314a60)}}, Count:1, Type:"Normal"}' (retry limit exceeded!)
Oct 18 22:24:07 k8s-master kubelet[12990]: E1018 22:24:07.598899   12990 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:24:07 k8s-master kubelet[12990]: E1018 22:24:07.580299   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:07 k8s-master kubelet[12990]: E1018 22:24:07.568226   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:07 k8s-master kubelet[12990]: E1018 22:24:07.565957   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:07 k8s-master kubelet[12990]: E1018 22:24:07.546076   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:07 k8s-master kubelet[12990]: E1018 22:24:07.544526   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:07 k8s-master kubelet[12990]: E1018 22:24:07.544348   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:06 k8s-master kubelet[12990]: E1018 22:24:06.579716   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:06 k8s-master kubelet[12990]: E1018 22:24:06.567742   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:06 k8s-master kubelet[12990]: E1018 22:24:06.565186   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:05 k8s-master kubelet[12990]: E1018 22:24:05.579086   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:05 k8s-master kubelet[12990]: E1018 22:24:05.567123   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:05 k8s-master kubelet[12990]: E1018 22:24:05.564524   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:05 k8s-master kubelet[12990]: E1018 22:24:05.543146   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:05 k8s-master kubelet[12990]: E1018 22:24:05.543016   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:05 k8s-master kubelet[12990]: E1018 22:24:05.542868   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:04 k8s-master kubelet[12990]: E1018 22:24:04.578529   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:04 k8s-master kubelet[12990]: E1018 22:24:04.566592   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:04 k8s-master kubelet[12990]: E1018 22:24:04.563834   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:03 k8s-master kubelet[12990]: E1018 22:24:03.577918   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:03 k8s-master kubelet[12990]: E1018 22:24:03.566070   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:03 k8s-master kubelet[12990]: E1018 22:24:03.563191   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:03 k8s-master kubelet[12990]: E1018 22:24:03.544519   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:03 k8s-master kubelet[12990]: E1018 22:24:03.544400   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:03 k8s-master kubelet[12990]: E1018 22:24:03.544256   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:02 k8s-master kubelet[12990]: E1018 22:24:02.978700   12990 kubelet_node_status.go:378] Unable to update node status: update node status exceeds retry count
Oct 18 22:24:02 k8s-master kubelet[12990]: E1018 22:24:02.978693   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:02 k8s-master kubelet[12990]: E1018 22:24:02.978558   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:02 k8s-master kubelet[12990]: E1018 22:24:02.978424   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:02 k8s-master kubelet[12990]: E1018 22:24:02.978289   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:02 k8s-master kubelet[12990]: E1018 22:24:02.978121   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:02 k8s-master kubelet[12990]: E1018 22:24:02.577304   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:02 k8s-master kubelet[12990]: E1018 22:24:02.565454   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:02 k8s-master kubelet[12990]: E1018 22:24:02.562569   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:01 k8s-master kubelet[12990]: E1018 22:24:01.576657   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:01 k8s-master kubelet[12990]: E1018 22:24:01.564891   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:01 k8s-master kubelet[12990]: E1018 22:24:01.561948   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:01 k8s-master kubelet[12990]: E1018 22:24:01.543154   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:01 k8s-master kubelet[12990]: E1018 22:24:01.543024   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:01 k8s-master kubelet[12990]: E1018 22:24:01.542881   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:00 k8s-master kubelet[12990]: E1018 22:24:00.576081   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:00 k8s-master kubelet[12990]: E1018 22:24:00.564387   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:24:00 k8s-master kubelet[12990]: E1018 22:24:00.561378   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:59 k8s-master kubelet[12990]: E1018 22:23:59.575352   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:59 k8s-master kubelet[12990]: E1018 22:23:59.563804   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:59 k8s-master kubelet[12990]: E1018 22:23:59.560770   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:59 k8s-master kubelet[12990]: E1018 22:23:59.546039   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:59 k8s-master kubelet[12990]: E1018 22:23:59.545871   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:59 k8s-master kubelet[12990]: E1018 22:23:59.545670   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:58 k8s-master kubelet[12990]: E1018 22:23:58.574692   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:58 k8s-master kubelet[12990]: E1018 22:23:58.563203   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:58 k8s-master kubelet[12990]: E1018 22:23:58.560184   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:57 k8s-master kubelet[12990]: E1018 22:23:57.598267   12990 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:23:57 k8s-master kubelet[12990]: E1018 22:23:57.573967   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:57 k8s-master kubelet[12990]: E1018 22:23:57.562667   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:57 k8s-master kubelet[12990]: E1018 22:23:57.559562   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:57 k8s-master kubelet[12990]: E1018 22:23:57.543357   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:57 k8s-master kubelet[12990]: E1018 22:23:57.543238   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:57 k8s-master kubelet[12990]: E1018 22:23:57.543094   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:56 k8s-master kubelet[12990]: E1018 22:23:56.573384   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:56 k8s-master kubelet[12990]: E1018 22:23:56.562106   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:56 k8s-master kubelet[12990]: E1018 22:23:56.558928   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:55 k8s-master kubelet[12990]: E1018 22:23:55.572769   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:55 k8s-master kubelet[12990]: E1018 22:23:55.561615   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:55 k8s-master kubelet[12990]: E1018 22:23:55.558251   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:55 k8s-master kubelet[12990]: E1018 22:23:55.544861   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:55 k8s-master kubelet[12990]: E1018 22:23:55.544740   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:55 k8s-master kubelet[12990]: E1018 22:23:55.544585   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:54 k8s-master kubelet[12990]: E1018 22:23:54.572166   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:54 k8s-master kubelet[12990]: E1018 22:23:54.561048   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:54 k8s-master kubelet[12990]: E1018 22:23:54.557674   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:53 k8s-master kubelet[12990]: E1018 22:23:53.571541   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:53 k8s-master kubelet[12990]: E1018 22:23:53.560567   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:53 k8s-master kubelet[12990]: E1018 22:23:53.556981   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:53 k8s-master kubelet[12990]: E1018 22:23:53.544539   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:53 k8s-master kubelet[12990]: E1018 22:23:53.544406   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:53 k8s-master kubelet[12990]: E1018 22:23:53.544224   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:52 k8s-master kubelet[12990]: E1018 22:23:52.977491   12990 kubelet_node_status.go:378] Unable to update node status: update node status exceeds retry count
Oct 18 22:23:52 k8s-master kubelet[12990]: E1018 22:23:52.977483   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:52 k8s-master kubelet[12990]: E1018 22:23:52.977342   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:52 k8s-master kubelet[12990]: E1018 22:23:52.977206   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:52 k8s-master kubelet[12990]: E1018 22:23:52.977071   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:52 k8s-master kubelet[12990]: E1018 22:23:52.976903   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:52 k8s-master kubelet[12990]: E1018 22:23:52.570943   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:52 k8s-master kubelet[12990]: E1018 22:23:52.560057   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:52 k8s-master kubelet[12990]: E1018 22:23:52.556405   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:51 k8s-master kubelet[12990]: E1018 22:23:51.570325   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:51 k8s-master kubelet[12990]: E1018 22:23:51.559551   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:51 k8s-master kubelet[12990]: E1018 22:23:51.555775   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:51 k8s-master kubelet[12990]: E1018 22:23:51.543197   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:51 k8s-master kubelet[12990]: E1018 22:23:51.543054   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:51 k8s-master kubelet[12990]: E1018 22:23:51.542907   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:50 k8s-master kubelet[12990]: E1018 22:23:50.569707   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:50 k8s-master kubelet[12990]: E1018 22:23:50.558890   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:50 k8s-master kubelet[12990]: E1018 22:23:50.553151   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:49 k8s-master kubelet[12990]: E1018 22:23:49.569123   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:49 k8s-master kubelet[12990]: E1018 22:23:49.558327   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:49 k8s-master kubelet[12990]: E1018 22:23:49.552333   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:49 k8s-master kubelet[12990]: E1018 22:23:49.545284   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:49 k8s-master kubelet[12990]: E1018 22:23:49.545159   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:49 k8s-master kubelet[12990]: E1018 22:23:49.545001   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:48 k8s-master kubelet[12990]: E1018 22:23:48.568493   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:48 k8s-master kubelet[12990]: E1018 22:23:48.557739   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:48 k8s-master kubelet[12990]: E1018 22:23:48.551718   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:47 k8s-master kubelet[12990]: E1018 22:23:47.597641   12990 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:23:47 k8s-master kubelet[12990]: E1018 22:23:47.567762   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:47 k8s-master kubelet[12990]: E1018 22:23:47.557143   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:47 k8s-master kubelet[12990]: E1018 22:23:47.551092   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:47 k8s-master kubelet[12990]: E1018 22:23:47.544079   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:47 k8s-master kubelet[12990]: E1018 22:23:47.543934   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:47 k8s-master kubelet[12990]: E1018 22:23:47.543785   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:46 k8s-master kubelet[12990]: E1018 22:23:46.567180   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:46 k8s-master kubelet[12990]: E1018 22:23:46.556463   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:46 k8s-master kubelet[12990]: E1018 22:23:46.550364   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:45 k8s-master kubelet[12990]: E1018 22:23:45.557212   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:45 k8s-master kubelet[12990]: E1018 22:23:45.557092   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:45 k8s-master kubelet[12990]: E1018 22:23:45.556928   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:45 k8s-master kubelet[12990]: E1018 22:23:45.550074   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:45 k8s-master kubelet[12990]: E1018 22:23:45.550038   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:45 k8s-master kubelet[12990]: E1018 22:23:45.549806   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:44 k8s-master kubelet[12990]: E1018 22:23:44.545132   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:44 k8s-master kubelet[12990]: E1018 22:23:44.529142   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:44 k8s-master kubelet[12990]: E1018 22:23:44.529074   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:43 k8s-master kubelet[12990]: E1018 22:23:43.560558   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:43 k8s-master kubelet[12990]: E1018 22:23:43.560443   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:43 k8s-master kubelet[12990]: E1018 22:23:43.560296   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:43 k8s-master kubelet[12990]: E1018 22:23:43.538740   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:43 k8s-master kubelet[12990]: E1018 22:23:43.525549   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:43 k8s-master kubelet[12990]: E1018 22:23:43.524096   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:42 k8s-master kubelet[12990]: E1018 22:23:42.976303   12990 kubelet_node_status.go:378] Unable to update node status: update node status exceeds retry count
Oct 18 22:23:42 k8s-master kubelet[12990]: E1018 22:23:42.976296   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:42 k8s-master kubelet[12990]: E1018 22:23:42.976178   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:42 k8s-master kubelet[12990]: E1018 22:23:42.975954   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:42 k8s-master kubelet[12990]: E1018 22:23:42.975215   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:42 k8s-master kubelet[12990]: E1018 22:23:42.973586   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:42 k8s-master kubelet[12990]: E1018 22:23:42.538054   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:42 k8s-master kubelet[12990]: E1018 22:23:42.525007   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:42 k8s-master kubelet[12990]: E1018 22:23:42.523326   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:41 k8s-master kubelet[12990]: E1018 22:23:41.544848   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:41 k8s-master kubelet[12990]: E1018 22:23:41.544719   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:41 k8s-master kubelet[12990]: E1018 22:23:41.544550   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:41 k8s-master kubelet[12990]: E1018 22:23:41.537424   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:41 k8s-master kubelet[12990]: E1018 22:23:41.523626   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:41 k8s-master kubelet[12990]: E1018 22:23:41.522452   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:40 k8s-master kubelet[12990]: E1018 22:23:40.536803   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:40 k8s-master kubelet[12990]: E1018 22:23:40.523041   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:40 k8s-master kubelet[12990]: E1018 22:23:40.520739   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:39 k8s-master kubelet[12990]: E1018 22:23:39.543334   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:39 k8s-master kubelet[12990]: E1018 22:23:39.543195   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:39 k8s-master kubelet[12990]: E1018 22:23:39.543027   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:39 k8s-master kubelet[12990]: E1018 22:23:39.536192   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:39 k8s-master kubelet[12990]: E1018 22:23:39.522589   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:39 k8s-master kubelet[12990]: E1018 22:23:39.518858   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:38 k8s-master kubelet[12990]: E1018 22:23:38.535559   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:38 k8s-master kubelet[12990]: E1018 22:23:38.521115   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:38 k8s-master kubelet[12990]: E1018 22:23:38.518187   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:37 k8s-master kubelet[12990]: E1018 22:23:37.597002   12990 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:23:37 k8s-master kubelet[12990]: E1018 22:23:37.544888   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:37 k8s-master kubelet[12990]: E1018 22:23:37.544754   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:37 k8s-master kubelet[12990]: E1018 22:23:37.544586   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:37 k8s-master kubelet[12990]: E1018 22:23:37.534834   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:37 k8s-master kubelet[12990]: E1018 22:23:37.519647   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:37 k8s-master kubelet[12990]: E1018 22:23:37.517544   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:36 k8s-master kubelet[12990]: E1018 22:23:36.533294   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:36 k8s-master kubelet[12990]: E1018 22:23:36.518071   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:36 k8s-master kubelet[12990]: E1018 22:23:36.516723   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:35 k8s-master kubelet[12990]: E1018 22:23:35.543579   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:35 k8s-master kubelet[12990]: E1018 22:23:35.543448   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:35 k8s-master kubelet[12990]: E1018 22:23:35.543277   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:35 k8s-master kubelet[12990]: E1018 22:23:35.532636   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:35 k8s-master kubelet[12990]: E1018 22:23:35.517431   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:35 k8s-master kubelet[12990]: E1018 22:23:35.516024   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:34 k8s-master kubelet[12990]: E1018 22:23:34.532010   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:34 k8s-master kubelet[12990]: E1018 22:23:34.516609   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:34 k8s-master kubelet[12990]: E1018 22:23:34.515316   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:33 k8s-master kubelet[12990]: E1018 22:23:33.544904   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:33 k8s-master kubelet[12990]: E1018 22:23:33.544774   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:33 k8s-master kubelet[12990]: E1018 22:23:33.544558   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:33 k8s-master kubelet[12990]: E1018 22:23:33.530390   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:33 k8s-master kubelet[12990]: E1018 22:23:33.515614   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:33 k8s-master kubelet[12990]: E1018 22:23:33.514510   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:32 k8s-master kubelet[12990]: E1018 22:23:32.972615   12990 kubelet_node_status.go:378] Unable to update node status: update node status exceeds retry count
Oct 18 22:23:32 k8s-master kubelet[12990]: E1018 22:23:32.972607   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:32 k8s-master kubelet[12990]: E1018 22:23:32.972466   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:32 k8s-master kubelet[12990]: E1018 22:23:32.972327   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:32 k8s-master kubelet[12990]: E1018 22:23:32.972163   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:32 k8s-master kubelet[12990]: E1018 22:23:32.971979   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:32 k8s-master kubelet[12990]: E1018 22:23:32.529744   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:32 k8s-master kubelet[12990]: E1018 22:23:32.514265   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:32 k8s-master kubelet[12990]: E1018 22:23:32.512941   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:31 k8s-master kubelet[12990]: - exit status 1
Oct 18 22:23:31 k8s-master kubelet[12990]: ; err: exit status 1, extraDiskErr: du command failed on /var/lib/docker/containers/2acd19a6d0ac7789f6ccd31ef88306d78b5c732223f984ce5c4c8e8f9caa5980 with output stdout: , stderr: du: cannot access ‘/var/lib/docker/containers/2acd19a6d0ac7789f6ccd31ef88306d78b5c732223f984ce5c4c8e8f9caa5980’: No such file or directory
Oct 18 22:23:31 k8s-master kubelet[12990]: - exit status 1, rootInodeErr: cmd [find /var/lib/docker/overlay/e0cb63a5100e63e3d868addd58223b00a8c9803e68bed418dce495dd0a30486b -xdev -printf .] failed. stderr: find: ‘/var/lib/docker/overlay/e0cb63a5100e63e3d868addd58223b00a8c9803e68bed418dce495dd0a30486b’: No such file or directory
Oct 18 22:23:31 k8s-master kubelet[12990]: E1018 22:23:31.752617   12990 fsHandler.go:121] failed to collect filesystem stats - rootDiskErr: du command failed on /var/lib/docker/overlay/e0cb63a5100e63e3d868addd58223b00a8c9803e68bed418dce495dd0a30486b with output stdout: , stderr: du: cannot access ‘/var/lib/docker/overlay/e0cb63a5100e63e3d868addd58223b00a8c9803e68bed418dce495dd0a30486b’: No such file or directory
Oct 18 22:23:31 k8s-master kubelet[12990]: E1018 22:23:31.543096   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:31 k8s-master kubelet[12990]: E1018 22:23:31.542966   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:31 k8s-master kubelet[12990]: E1018 22:23:31.542800   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:31 k8s-master kubelet[12990]: E1018 22:23:31.529133   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:31 k8s-master kubelet[12990]: E1018 22:23:31.513369   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:31 k8s-master kubelet[12990]: E1018 22:23:31.512209   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:30 k8s-master kubelet[12990]: E1018 22:23:30.528492   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:30 k8s-master kubelet[12990]: E1018 22:23:30.512490   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:30 k8s-master kubelet[12990]: E1018 22:23:30.511460   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:29 k8s-master kubelet[12990]: E1018 22:23:29.544777   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:29 k8s-master kubelet[12990]: E1018 22:23:29.544649   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:29 k8s-master kubelet[12990]: E1018 22:23:29.544491   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:29 k8s-master kubelet[12990]: E1018 22:23:29.527898   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:29 k8s-master kubelet[12990]: E1018 22:23:29.511951   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:29 k8s-master kubelet[12990]: E1018 22:23:29.510757   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:28 k8s-master kubelet[12990]: E1018 22:23:28.526407   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:28 k8s-master kubelet[12990]: E1018 22:23:28.511480   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:28 k8s-master kubelet[12990]: E1018 22:23:28.510022   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:27 k8s-master kubelet[12990]: E1018 22:23:27.596428   12990 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:23:27 k8s-master kubelet[12990]: E1018 22:23:27.545417   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:27 k8s-master kubelet[12990]: E1018 22:23:27.545289   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:27 k8s-master kubelet[12990]: E1018 22:23:27.545133   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:27 k8s-master kubelet[12990]: E1018 22:23:27.525819   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:27 k8s-master kubelet[12990]: E1018 22:23:27.510878   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:27 k8s-master kubelet[12990]: E1018 22:23:27.509449   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:26 k8s-master kubelet[12990]: E1018 22:23:26.525241   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:26 k8s-master kubelet[12990]: E1018 22:23:26.510239   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:26 k8s-master kubelet[12990]: E1018 22:23:26.508775   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:25 k8s-master kubelet[12990]: E1018 22:23:25.568504   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:25 k8s-master kubelet[12990]: E1018 22:23:25.568383   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:25 k8s-master kubelet[12990]: E1018 22:23:25.568240   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:25 k8s-master kubelet[12990]: E1018 22:23:25.563081   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:25 k8s-master kubelet[12990]: E1018 22:23:25.522390   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:25 k8s-master kubelet[12990]: E1018 22:23:25.508869   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:25 k8s-master kubelet[12990]: E1018 22:23:25.507549   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:25 k8s-master kubelet[12990]: E1018 22:23:25.165659   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:24 k8s-master kubelet[12990]: E1018 22:23:24.520901   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:24 k8s-master kubelet[12990]: E1018 22:23:24.508139   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:24 k8s-master kubelet[12990]: E1018 22:23:24.506681   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:24 k8s-master kubelet[12990]: E1018 22:23:24.455111   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:23 k8s-master kubelet[12990]: E1018 22:23:23.958942   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:23 k8s-master kubelet[12990]: E1018 22:23:23.545268   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:23 k8s-master kubelet[12990]: E1018 22:23:23.545141   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:23 k8s-master kubelet[12990]: E1018 22:23:23.544974   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:23 k8s-master kubelet[12990]: E1018 22:23:23.519300   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:23 k8s-master kubelet[12990]: E1018 22:23:23.506961   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:23 k8s-master kubelet[12990]: E1018 22:23:23.505984   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:22 k8s-master kubelet[12990]: E1018 22:23:22.971352   12990 kubelet_node_status.go:378] Unable to update node status: update node status exceeds retry count
Oct 18 22:23:22 k8s-master kubelet[12990]: E1018 22:23:22.971344   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:22 k8s-master kubelet[12990]: E1018 22:23:22.971210   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:22 k8s-master kubelet[12990]: E1018 22:23:22.971070   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:22 k8s-master kubelet[12990]: E1018 22:23:22.970916   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:22 k8s-master kubelet[12990]: E1018 22:23:22.970739   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:22 k8s-master kubelet[12990]: E1018 22:23:22.517904   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:22 k8s-master kubelet[12990]: E1018 22:23:22.506553   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:22 k8s-master kubelet[12990]: E1018 22:23:22.505341   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:21 k8s-master kubelet[12990]: E1018 22:23:21.572757   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:21 k8s-master kubelet[12990]: E1018 22:23:21.543304   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:21 k8s-master kubelet[12990]: E1018 22:23:21.543175   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:21 k8s-master kubelet[12990]: E1018 22:23:21.543023   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:21 k8s-master kubelet[12990]: E1018 22:23:21.517267   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:21 k8s-master kubelet[12990]: E1018 22:23:21.503419   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:21 k8s-master kubelet[12990]: E1018 22:23:21.503364   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:21 k8s-master kubelet[12990]: E1018 22:23:21.434621   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:20 k8s-master kubelet[12990]: E1018 22:23:20.516608   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:20 k8s-master kubelet[12990]: E1018 22:23:20.500584   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:20 k8s-master kubelet[12990]: E1018 22:23:20.498519   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:20 k8s-master kubelet[12990]: E1018 22:23:20.098230   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:19 k8s-master kubelet[12990]: E1018 22:23:19.958540   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:19 k8s-master kubelet[12990]: E1018 22:23:19.545863   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:19 k8s-master kubelet[12990]: E1018 22:23:19.545715   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:19 k8s-master kubelet[12990]: E1018 22:23:19.545531   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:19 k8s-master kubelet[12990]: E1018 22:23:19.515948   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:19 k8s-master kubelet[12990]: E1018 22:23:19.500058   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:19 k8s-master kubelet[12990]: E1018 22:23:19.497422   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:18 k8s-master kubelet[12990]: E1018 22:23:18.515349   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:18 k8s-master kubelet[12990]: E1018 22:23:18.499548   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:18 k8s-master kubelet[12990]: E1018 22:23:18.496679   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:17 k8s-master kubelet[12990]: E1018 22:23:17.700789   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:17 k8s-master kubelet[12990]: E1018 22:23:17.595851   12990 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:23:17 k8s-master kubelet[12990]: E1018 22:23:17.545047   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:17 k8s-master kubelet[12990]: E1018 22:23:17.544919   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:17 k8s-master kubelet[12990]: E1018 22:23:17.544748   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:17 k8s-master kubelet[12990]: E1018 22:23:17.514748   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:17 k8s-master kubelet[12990]: E1018 22:23:17.498562   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:17 k8s-master kubelet[12990]: E1018 22:23:17.495956   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:16 k8s-master kubelet[12990]: E1018 22:23:16.513983   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:16 k8s-master kubelet[12990]: E1018 22:23:16.497783   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:16 k8s-master kubelet[12990]: E1018 22:23:16.495297   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:15 k8s-master kubelet[12990]: E1018 22:23:15.917343   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:15 k8s-master kubelet[12990]: E1018 22:23:15.692182   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:15 k8s-master kubelet[12990]: E1018 22:23:15.543574   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:15 k8s-master kubelet[12990]: E1018 22:23:15.543391   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:15 k8s-master kubelet[12990]: E1018 22:23:15.543223   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:15 k8s-master kubelet[12990]: E1018 22:23:15.536544   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:15 k8s-master kubelet[12990]: E1018 22:23:15.513458   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:15 k8s-master kubelet[12990]: E1018 22:23:15.497201   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:15 k8s-master kubelet[12990]: E1018 22:23:15.494665   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:15 k8s-master kubelet[12990]: E1018 22:23:15.068565   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:14 k8s-master kubelet[12990]: E1018 22:23:14.889983   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:14 k8s-master kubelet[12990]: E1018 22:23:14.512839   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:14 k8s-master kubelet[12990]: E1018 22:23:14.496725   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:14 k8s-master kubelet[12990]: E1018 22:23:14.494006   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:13 k8s-master kubelet[12990]: E1018 22:23:13.559720   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:13 k8s-master kubelet[12990]: E1018 22:23:13.559602   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:13 k8s-master kubelet[12990]: E1018 22:23:13.559453   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:13 k8s-master kubelet[12990]: E1018 22:23:13.512224   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:13 k8s-master kubelet[12990]: E1018 22:23:13.496198   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:13 k8s-master kubelet[12990]: E1018 22:23:13.493185   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:12 k8s-master kubelet[12990]: E1018 22:23:12.970119   12990 kubelet_node_status.go:378] Unable to update node status: update node status exceeds retry count
Oct 18 22:23:12 k8s-master kubelet[12990]: E1018 22:23:12.970112   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:12 k8s-master kubelet[12990]: E1018 22:23:12.969974   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:12 k8s-master kubelet[12990]: E1018 22:23:12.969836   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:12 k8s-master kubelet[12990]: E1018 22:23:12.969677   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:12 k8s-master kubelet[12990]: E1018 22:23:12.969467   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:12 k8s-master kubelet[12990]: E1018 22:23:12.511593   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:12 k8s-master kubelet[12990]: E1018 22:23:12.495664   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:12 k8s-master kubelet[12990]: E1018 22:23:12.492605   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:12 k8s-master kubelet[12990]: E1018 22:23:12.264287   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:11 k8s-master kubelet[12990]: E1018 22:23:11.543318   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:11 k8s-master kubelet[12990]: E1018 22:23:11.543160   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:11 k8s-master kubelet[12990]: E1018 22:23:11.543008   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:11 k8s-master kubelet[12990]: E1018 22:23:11.510997   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:11 k8s-master kubelet[12990]: E1018 22:23:11.495118   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:11 k8s-master kubelet[12990]: E1018 22:23:11.491995   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:11 k8s-master kubelet[12990]: E1018 22:23:11.045359   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:10 k8s-master kubelet[12990]: E1018 22:23:10.510415   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:10 k8s-master kubelet[12990]: E1018 22:23:10.494522   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:10 k8s-master kubelet[12990]: E1018 22:23:10.491390   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:09 k8s-master kubelet[12990]: E1018 22:23:09.986387   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:09 k8s-master kubelet[12990]: E1018 22:23:09.545518   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:09 k8s-master kubelet[12990]: E1018 22:23:09.545397   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:09 k8s-master kubelet[12990]: E1018 22:23:09.545245   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:09 k8s-master kubelet[12990]: E1018 22:23:09.509820   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:09 k8s-master kubelet[12990]: E1018 22:23:09.494042   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:09 k8s-master kubelet[12990]: E1018 22:23:09.490784   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:08 k8s-master kubelet[12990]: E1018 22:23:08.509229   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:08 k8s-master kubelet[12990]: E1018 22:23:08.492103   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:08 k8s-master kubelet[12990]: E1018 22:23:08.489973   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:07 k8s-master kubelet[12990]: E1018 22:23:07.618804   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:07 k8s-master kubelet[12990]: E1018 22:23:07.595240   12990 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:23:07 k8s-master kubelet[12990]: E1018 22:23:07.543504   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:07 k8s-master kubelet[12990]: E1018 22:23:07.543370   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:07 k8s-master kubelet[12990]: E1018 22:23:07.543216   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:07 k8s-master kubelet[12990]: E1018 22:23:07.508643   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:07 k8s-master kubelet[12990]: E1018 22:23:07.490827   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:07 k8s-master kubelet[12990]: E1018 22:23:07.489268   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:06 k8s-master kubelet[12990]: E1018 22:23:06.508027   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:06 k8s-master kubelet[12990]: E1018 22:23:06.489257   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:06 k8s-master kubelet[12990]: E1018 22:23:06.487667   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:06 k8s-master kubelet[12990]: E1018 22:23:06.287154   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:05 k8s-master kubelet[12990]: E1018 22:23:05.546923   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:05 k8s-master kubelet[12990]: E1018 22:23:05.546806   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:05 k8s-master kubelet[12990]: E1018 22:23:05.546664   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:05 k8s-master kubelet[12990]: E1018 22:23:05.507399   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:05 k8s-master kubelet[12990]: E1018 22:23:05.488296   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:05 k8s-master kubelet[12990]: E1018 22:23:05.486924   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:05 k8s-master kubelet[12990]: E1018 22:23:05.228121   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:04 k8s-master kubelet[12990]: E1018 22:23:04.707117   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:04 k8s-master kubelet[12990]: E1018 22:23:04.506854   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:04 k8s-master kubelet[12990]: E1018 22:23:04.487842   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:04 k8s-master kubelet[12990]: E1018 22:23:04.486196   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:04 k8s-master kubelet[12990]: E1018 22:23:04.245338   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:03 k8s-master kubelet[12990]: E1018 22:23:03.656155   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:03 k8s-master kubelet[12990]: E1018 22:23:03.544930   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:03 k8s-master kubelet[12990]: E1018 22:23:03.544809   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:03 k8s-master kubelet[12990]: E1018 22:23:03.544652   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:03 k8s-master kubelet[12990]: E1018 22:23:03.506253   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:03 k8s-master kubelet[12990]: E1018 22:23:03.487181   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:03 k8s-master kubelet[12990]: E1018 22:23:03.485283   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:02 k8s-master kubelet[12990]: E1018 22:23:02.968813   12990 kubelet_node_status.go:378] Unable to update node status: update node status exceeds retry count
Oct 18 22:23:02 k8s-master kubelet[12990]: E1018 22:23:02.968806   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:02 k8s-master kubelet[12990]: E1018 22:23:02.968668   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:02 k8s-master kubelet[12990]: E1018 22:23:02.968536   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:02 k8s-master kubelet[12990]: E1018 22:23:02.968400   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:02 k8s-master kubelet[12990]: E1018 22:23:02.968229   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:02 k8s-master kubelet[12990]: E1018 22:23:02.505722   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:02 k8s-master kubelet[12990]: E1018 22:23:02.486229   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:02 k8s-master kubelet[12990]: E1018 22:23:02.484670   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:02 k8s-master kubelet[12990]: E1018 22:23:02.315620   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:01 k8s-master kubelet[12990]: E1018 22:23:01.584598   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:01 k8s-master kubelet[12990]: E1018 22:23:01.573592   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:01 k8s-master kubelet[12990]: E1018 22:23:01.543472   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:01 k8s-master kubelet[12990]: E1018 22:23:01.543343   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:01 k8s-master kubelet[12990]: E1018 22:23:01.543189   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:01 k8s-master kubelet[12990]: E1018 22:23:01.505094   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:01 k8s-master kubelet[12990]: E1018 22:23:01.485628   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:01 k8s-master kubelet[12990]: E1018 22:23:01.484026   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:01 k8s-master kubelet[12990]: E1018 22:23:01.234968   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:01 k8s-master kubelet[12990]: E1018 22:23:01.007882   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:23:00 k8s-master kubelet[12990]: E1018 22:23:00.504464   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:00 k8s-master kubelet[12990]: E1018 22:23:00.484636   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:23:00 k8s-master kubelet[12990]: E1018 22:23:00.483353   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:59 k8s-master kubelet[12990]: E1018 22:22:59.786877   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:59 k8s-master kubelet[12990]: E1018 22:22:59.650933   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:59 k8s-master kubelet[12990]: E1018 22:22:59.545356   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:59 k8s-master kubelet[12990]: E1018 22:22:59.545214   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:59 k8s-master kubelet[12990]: E1018 22:22:59.545068   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:59 k8s-master kubelet[12990]: E1018 22:22:59.503843   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:59 k8s-master kubelet[12990]: E1018 22:22:59.484163   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:59 k8s-master kubelet[12990]: E1018 22:22:59.482679   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:58 k8s-master kubelet[12990]: E1018 22:22:58.978954   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:58 k8s-master kubelet[12990]: E1018 22:22:58.503239   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:58 k8s-master kubelet[12990]: E1018 22:22:58.483757   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:58 k8s-master kubelet[12990]: E1018 22:22:58.481884   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:57 k8s-master kubelet[12990]: E1018 22:22:57.594645   12990 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:22:57 k8s-master kubelet[12990]: E1018 22:22:57.545039   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:57 k8s-master kubelet[12990]: E1018 22:22:57.544971   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:57 k8s-master kubelet[12990]: E1018 22:22:57.544971   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:57 k8s-master kubelet[12990]: E1018 22:22:57.502608   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:57 k8s-master kubelet[12990]: E1018 22:22:57.483328   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:57 k8s-master kubelet[12990]: E1018 22:22:57.481364   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:56 k8s-master kubelet[12990]: E1018 22:22:56.501985   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:56 k8s-master kubelet[12990]: E1018 22:22:56.482845   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:56 k8s-master kubelet[12990]: E1018 22:22:56.480681   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:55 k8s-master kubelet[12990]: E1018 22:22:55.543522   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:55 k8s-master kubelet[12990]: E1018 22:22:55.543391   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:55 k8s-master kubelet[12990]: E1018 22:22:55.543250   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:55 k8s-master kubelet[12990]: E1018 22:22:55.500610   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:55 k8s-master kubelet[12990]: E1018 22:22:55.482331   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:55 k8s-master kubelet[12990]: E1018 22:22:55.479987   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:55 k8s-master kubelet[12990]: E1018 22:22:55.123761   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:54 k8s-master kubelet[12990]: E1018 22:22:54.500056   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:54 k8s-master kubelet[12990]: E1018 22:22:54.481874   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:54 k8s-master kubelet[12990]: E1018 22:22:54.479307   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:53 k8s-master kubelet[12990]: E1018 22:22:53.663607   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:53 k8s-master kubelet[12990]: E1018 22:22:53.545223   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:53 k8s-master kubelet[12990]: E1018 22:22:53.545108   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:53 k8s-master kubelet[12990]: E1018 22:22:53.544965   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:53 k8s-master kubelet[12990]: E1018 22:22:53.499480   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:53 k8s-master kubelet[12990]: E1018 22:22:53.481264   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:53 k8s-master kubelet[12990]: E1018 22:22:53.477919   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:53 k8s-master kubelet[12990]: E1018 22:22:53.284159   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:53 k8s-master kubelet[12990]: E1018 22:22:53.122089   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:52 k8s-master kubelet[12990]: E1018 22:22:52.967606   12990 kubelet_node_status.go:378] Unable to update node status: update node status exceeds retry count
Oct 18 22:22:52 k8s-master kubelet[12990]: E1018 22:22:52.967606   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:52 k8s-master kubelet[12990]: E1018 22:22:52.967606   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:52 k8s-master kubelet[12990]: E1018 22:22:52.967605   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:52 k8s-master kubelet[12990]: E1018 22:22:52.967504   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:52 k8s-master kubelet[12990]: E1018 22:22:52.967268   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:52 k8s-master kubelet[12990]: E1018 22:22:52.498893   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:52 k8s-master kubelet[12990]: E1018 22:22:52.480780   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:52 k8s-master kubelet[12990]: E1018 22:22:52.477324   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:51 k8s-master kubelet[12990]: E1018 22:22:51.866757   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:51 k8s-master kubelet[12990]: E1018 22:22:51.543360   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:51 k8s-master kubelet[12990]: E1018 22:22:51.543246   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:51 k8s-master kubelet[12990]: E1018 22:22:51.543101   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:51 k8s-master kubelet[12990]: E1018 22:22:51.498356   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:51 k8s-master kubelet[12990]: E1018 22:22:51.480252   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:51 k8s-master kubelet[12990]: E1018 22:22:51.476654   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:50 k8s-master kubelet[12990]: E1018 22:22:50.497388   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:50 k8s-master kubelet[12990]: E1018 22:22:50.479612   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:50 k8s-master kubelet[12990]: E1018 22:22:50.475985   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:50 k8s-master kubelet[12990]: E1018 22:22:50.388754   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:49 k8s-master kubelet[12990]: E1018 22:22:49.706586   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:49 k8s-master kubelet[12990]: E1018 22:22:49.545383   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:49 k8s-master kubelet[12990]: E1018 22:22:49.545265   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:49 k8s-master kubelet[12990]: E1018 22:22:49.545089   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:49 k8s-master kubelet[12990]: E1018 22:22:49.496788   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:49 k8s-master kubelet[12990]: E1018 22:22:49.479052   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:49 k8s-master kubelet[12990]: E1018 22:22:49.475275   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:49 k8s-master kubelet[12990]: E1018 22:22:49.229219   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:48 k8s-master kubelet[12990]: E1018 22:22:48.496203   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:48 k8s-master kubelet[12990]: E1018 22:22:48.478671   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:48 k8s-master kubelet[12990]: E1018 22:22:48.474537   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:47 k8s-master kubelet[12990]: E1018 22:22:47.922405   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:47 k8s-master kubelet[12990]: E1018 22:22:47.593945   12990 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:22:47 k8s-master kubelet[12990]: E1018 22:22:47.545458   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:47 k8s-master kubelet[12990]: E1018 22:22:47.545301   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:47 k8s-master kubelet[12990]: E1018 22:22:47.545149   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:47 k8s-master kubelet[12990]: E1018 22:22:47.495600   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:47 k8s-master kubelet[12990]: E1018 22:22:47.478030   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:47 k8s-master kubelet[12990]: E1018 22:22:47.473896   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:47 k8s-master kubelet[12990]: E1018 22:22:47.400297   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:47 k8s-master kubelet[12990]: E1018 22:22:47.218862   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:46 k8s-master kubelet[12990]: E1018 22:22:46.495014   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:46 k8s-master kubelet[12990]: E1018 22:22:46.477353   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:46 k8s-master kubelet[12990]: E1018 22:22:46.473182   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:45 k8s-master kubelet[12990]: E1018 22:22:45.567469   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:45 k8s-master kubelet[12990]: E1018 22:22:45.545358   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:45 k8s-master kubelet[12990]: E1018 22:22:45.545229   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:45 k8s-master kubelet[12990]: E1018 22:22:45.545078   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:45 k8s-master kubelet[12990]: E1018 22:22:45.494338   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:45 k8s-master kubelet[12990]: E1018 22:22:45.476807   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:45 k8s-master kubelet[12990]: E1018 22:22:45.471115   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:45 k8s-master kubelet[12990]: I1018 22:22:45.116576   12990 reconciler.go:290] Volume detached for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/5c60da2dd3a2204955600c1da11b55d9-kubeconfig") on node "k8s-master" DevicePath ""
Oct 18 22:22:45 k8s-master kubelet[12990]: I1018 22:22:45.016276   12990 operation_generator.go:535] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/5c60da2dd3a2204955600c1da11b55d9-kubeconfig" (OuterVolumeSpecName: "kubeconfig") pod "5c60da2dd3a2204955600c1da11b55d9" (UID: "5c60da2dd3a2204955600c1da11b55d9"). InnerVolumeSpecName "kubeconfig". PluginName "kubernetes.io/host-path", VolumeGidValue ""
Oct 18 22:22:45 k8s-master kubelet[12990]: I1018 22:22:45.016229   12990 reconciler.go:186] operationExecutor.UnmountVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/5c60da2dd3a2204955600c1da11b55d9-kubeconfig") pod "5c60da2dd3a2204955600c1da11b55d9" (UID: "5c60da2dd3a2204955600c1da11b55d9")
Oct 18 22:22:44 k8s-master kubelet[12990]: E1018 22:22:44.706613   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:44 k8s-master kubelet[12990]: E1018 22:22:44.493724   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:44 k8s-master kubelet[12990]: E1018 22:22:44.476209   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:44 k8s-master kubelet[12990]: E1018 22:22:44.469597   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:44 k8s-master kubelet[12990]: W1018 22:22:44.165202   12990 pod_container_deletor.go:77] Container "2acd19a6d0ac7789f6ccd31ef88306d78b5c732223f984ce5c4c8e8f9caa5980" not found in pod's containers
Oct 18 22:22:43 k8s-master kubelet[12990]: E1018 22:22:43.549033   12990 kuberuntime_container.go:66] Can't make a ref to pod "kube-scheduler-k8s-master_kube-system(5c60da2dd3a2204955600c1da11b55d9)", container kube-scheduler: selfLink was empty, can't make reference
Oct 18 22:22:43 k8s-master kubelet[12990]: E1018 22:22:43.545615   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:43 k8s-master kubelet[12990]: E1018 22:22:43.545481   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:43 k8s-master kubelet[12990]: E1018 22:22:43.545182   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:43 k8s-master kubelet[12990]: E1018 22:22:43.493167   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:43 k8s-master kubelet[12990]: E1018 22:22:43.475616   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:43 k8s-master kubelet[12990]: E1018 22:22:43.468959   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:43 k8s-master kubelet[12990]: E1018 22:22:43.175600   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:43 k8s-master kubelet[12990]: W1018 22:22:43.175449   12990 container.go:354] Failed to create summary reader for "/libcontainer_13880_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:22:43 k8s-master kubelet[12990]: W1018 22:22:43.170069   12990 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_13880_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_13880_systemd_test_default.slice: no such file or directory
Oct 18 22:22:43 k8s-master kubelet[12990]: E1018 22:22:43.127709   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:43 k8s-master kubelet[12990]: W1018 22:22:43.127636   12990 container.go:354] Failed to create summary reader for "/libcontainer_13877_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:22:43 k8s-master kubelet[12990]: W1018 22:22:43.122644   12990 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_13877_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_13877_systemd_test_default.slice: no such file or directory
Oct 18 22:22:43 k8s-master kubelet[12990]: E1018 22:22:43.055017   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:43 k8s-master kubelet[12990]: E1018 22:22:43.028177   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:43 k8s-master kubelet[12990]: W1018 22:22:43.027952   12990 container.go:354] Failed to create summary reader for "/libcontainer_13869_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:22:43 k8s-master kubelet[12990]: W1018 22:22:43.024351   12990 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_13869_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_13869_systemd_test_default.slice: no such file or directory
Oct 18 22:22:42 k8s-master kubelet[12990]: E1018 22:22:42.988757   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:42 k8s-master kubelet[12990]: W1018 22:22:42.988559   12990 container.go:354] Failed to create summary reader for "/libcontainer_13866_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:22:42 k8s-master kubelet[12990]: W1018 22:22:42.983083   12990 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_13866_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_13866_systemd_test_default.slice: no such file or directory
Oct 18 22:22:42 k8s-master kubelet[12990]: E1018 22:22:42.959136   12990 kubelet_node_status.go:378] Unable to update node status: update node status exceeds retry count
Oct 18 22:22:42 k8s-master kubelet[12990]: E1018 22:22:42.959127   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:42 k8s-master kubelet[12990]: E1018 22:22:42.959012   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:42 k8s-master kubelet[12990]: E1018 22:22:42.958852   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:42 k8s-master kubelet[12990]: E1018 22:22:42.958720   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:42 k8s-master kubelet[12990]: E1018 22:22:42.958519   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:42 k8s-master kubelet[12990]: E1018 22:22:42.957877   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:42 k8s-master kubelet[12990]: W1018 22:22:42.957755   12990 container.go:354] Failed to create summary reader for "/libcontainer_13863_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:22:42 k8s-master kubelet[12990]: W1018 22:22:42.944624   12990 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_13863_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_13863_systemd_test_default.slice: no such file or directory
Oct 18 22:22:42 k8s-master kubelet[12990]: E1018 22:22:42.890752   12990 kubelet.go:1612] Failed creating a mirror pod for "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:42 k8s-master kubelet[12990]: E1018 22:22:42.889909   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-scheduler-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:42 k8s-master kubelet[12990]: W1018 22:22:42.889447   12990 kubelet.go:1601] Deleting mirror pod "kube-scheduler-k8s-master_kube-system(4a7e7b8d-b40f-11e7-aa6c-08002782873b)" because it is outdated
Oct 18 22:22:42 k8s-master kubelet[12990]: W1018 22:22:42.871349   12990 status_manager.go:431] Failed to get status for pod "kube-scheduler-k8s-master_kube-system(f3e5e477637a31dd77e4d4e3534d2e23)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:42 k8s-master kubelet[12990]: W1018 22:22:42.868153   12990 status_manager.go:431] Failed to get status for pod "kube-scheduler-k8s-master_kube-system(5c60da2dd3a2204955600c1da11b55d9)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:42 k8s-master kubelet[12990]: E1018 22:22:42.867000   12990 prober_manager.go:154] Liveness probe already exists! kube-scheduler-k8s-master_kube-system(5c60da2dd3a2204955600c1da11b55d9) - kube-scheduler
Oct 18 22:22:42 k8s-master kubelet[12990]: E1018 22:22:42.857402   12990 file_linux.go:114] Can't process manifest file "/etc/kubernetes/manifests/4913": open /etc/kubernetes/manifests/4913: no such file or directory
Oct 18 22:22:42 k8s-master kubelet[12990]: E1018 22:22:42.529221   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:42 k8s-master kubelet[12990]: E1018 22:22:42.492371   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:42 k8s-master kubelet[12990]: E1018 22:22:42.475099   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:42 k8s-master kubelet[12990]: E1018 22:22:42.468299   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:41 k8s-master kubelet[12990]: E1018 22:22:41.544838   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:41 k8s-master kubelet[12990]: E1018 22:22:41.544633   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:41 k8s-master kubelet[12990]: E1018 22:22:41.490749   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:41 k8s-master kubelet[12990]: E1018 22:22:41.474469   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:41 k8s-master kubelet[12990]: E1018 22:22:41.466795   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:41 k8s-master kubelet[12990]: E1018 22:22:41.306056   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:40 k8s-master kubelet[12990]: E1018 22:22:40.867086   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:40 k8s-master kubelet[12990]: E1018 22:22:40.490147   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:40 k8s-master kubelet[12990]: E1018 22:22:40.473910   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:40 k8s-master kubelet[12990]: E1018 22:22:40.466233   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:40 k8s-master kubelet[12990]: E1018 22:22:40.186616   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:39 k8s-master kubelet[12990]: E1018 22:22:39.546080   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:39 k8s-master kubelet[12990]: E1018 22:22:39.545930   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:39 k8s-master kubelet[12990]: E1018 22:22:39.489387   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:39 k8s-master kubelet[12990]: E1018 22:22:39.472685   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:39 k8s-master kubelet[12990]: E1018 22:22:39.465701   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:38 k8s-master kubelet[12990]: E1018 22:22:38.488562   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:38 k8s-master kubelet[12990]: E1018 22:22:38.471968   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:38 k8s-master kubelet[12990]: E1018 22:22:38.465169   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:37 k8s-master kubelet[12990]: E1018 22:22:37.590592   12990 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:22:37 k8s-master kubelet[12990]: E1018 22:22:37.561297   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:37 k8s-master kubelet[12990]: E1018 22:22:37.561135   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:37 k8s-master kubelet[12990]: E1018 22:22:37.486589   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:37 k8s-master kubelet[12990]: E1018 22:22:37.470121   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:37 k8s-master kubelet[12990]: E1018 22:22:37.464524   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:36 k8s-master kubelet[12990]: E1018 22:22:36.485927   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:36 k8s-master kubelet[12990]: E1018 22:22:36.469489   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:36 k8s-master kubelet[12990]: E1018 22:22:36.463860   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:35 k8s-master kubelet[12990]: E1018 22:22:35.544210   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:35 k8s-master kubelet[12990]: E1018 22:22:35.544033   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:35 k8s-master kubelet[12990]: E1018 22:22:35.485287   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:35 k8s-master kubelet[12990]: E1018 22:22:35.468742   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:35 k8s-master kubelet[12990]: E1018 22:22:35.463292   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:34 k8s-master kubelet[12990]: E1018 22:22:34.484554   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:34 k8s-master kubelet[12990]: E1018 22:22:34.468145   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:34 k8s-master kubelet[12990]: E1018 22:22:34.462597   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:33 k8s-master kubelet[12990]: E1018 22:22:33.547592   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:33 k8s-master kubelet[12990]: E1018 22:22:33.547377   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:33 k8s-master kubelet[12990]: E1018 22:22:33.483864   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:33 k8s-master kubelet[12990]: E1018 22:22:33.467364   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:33 k8s-master kubelet[12990]: E1018 22:22:33.461881   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:33 k8s-master kubelet[12990]: I1018 22:22:33.013515   12990 reconciler.go:290] Volume detached for volume "ca-certs-etc-pki" (UniqueName: "kubernetes.io/host-path/e126af427288c2bf00e6a93d98e32e0b-ca-certs-etc-pki") on node "k8s-master" DevicePath ""
Oct 18 22:22:33 k8s-master kubelet[12990]: I1018 22:22:33.013506   12990 reconciler.go:290] Volume detached for volume "flexvolume-dir" (UniqueName: "kubernetes.io/host-path/e126af427288c2bf00e6a93d98e32e0b-flexvolume-dir") on node "k8s-master" DevicePath ""
Oct 18 22:22:33 k8s-master kubelet[12990]: I1018 22:22:33.013491   12990 reconciler.go:290] Volume detached for volume "ca-certs" (UniqueName: "kubernetes.io/host-path/e126af427288c2bf00e6a93d98e32e0b-ca-certs") on node "k8s-master" DevicePath ""
Oct 18 22:22:33 k8s-master kubelet[12990]: I1018 22:22:33.013482   12990 reconciler.go:290] Volume detached for volume "k8s-certs" (UniqueName: "kubernetes.io/host-path/e126af427288c2bf00e6a93d98e32e0b-k8s-certs") on node "k8s-master" DevicePath ""
Oct 18 22:22:33 k8s-master kubelet[12990]: I1018 22:22:33.013450   12990 reconciler.go:290] Volume detached for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/e126af427288c2bf00e6a93d98e32e0b-kubeconfig") on node "k8s-master" DevicePath ""
Oct 18 22:22:32 k8s-master kubelet[12990]: E1018 22:22:32.952754   12990 kubelet_node_status.go:378] Unable to update node status: update node status exceeds retry count
Oct 18 22:22:32 k8s-master kubelet[12990]: E1018 22:22:32.952744   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:32 k8s-master kubelet[12990]: E1018 22:22:32.952599   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:32 k8s-master kubelet[12990]: E1018 22:22:32.952381   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:32 k8s-master kubelet[12990]: E1018 22:22:32.952132   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:32 k8s-master kubelet[12990]: E1018 22:22:32.951891   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:32 k8s-master kubelet[12990]: I1018 22:22:32.913289   12990 operation_generator.go:535] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/e126af427288c2bf00e6a93d98e32e0b-ca-certs-etc-pki" (OuterVolumeSpecName: "ca-certs-etc-pki") pod "e126af427288c2bf00e6a93d98e32e0b" (UID: "e126af427288c2bf00e6a93d98e32e0b"). InnerVolumeSpecName "ca-certs-etc-pki". PluginName "kubernetes.io/host-path", VolumeGidValue ""
Oct 18 22:22:32 k8s-master kubelet[12990]: I1018 22:22:32.913272   12990 operation_generator.go:535] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/e126af427288c2bf00e6a93d98e32e0b-flexvolume-dir" (OuterVolumeSpecName: "flexvolume-dir") pod "e126af427288c2bf00e6a93d98e32e0b" (UID: "e126af427288c2bf00e6a93d98e32e0b"). InnerVolumeSpecName "flexvolume-dir". PluginName "kubernetes.io/host-path", VolumeGidValue ""
Oct 18 22:22:32 k8s-master kubelet[12990]: I1018 22:22:32.913257   12990 operation_generator.go:535] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/e126af427288c2bf00e6a93d98e32e0b-k8s-certs" (OuterVolumeSpecName: "k8s-certs") pod "e126af427288c2bf00e6a93d98e32e0b" (UID: "e126af427288c2bf00e6a93d98e32e0b"). InnerVolumeSpecName "k8s-certs". PluginName "kubernetes.io/host-path", VolumeGidValue ""
Oct 18 22:22:32 k8s-master kubelet[12990]: I1018 22:22:32.913229   12990 operation_generator.go:535] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/e126af427288c2bf00e6a93d98e32e0b-kubeconfig" (OuterVolumeSpecName: "kubeconfig") pod "e126af427288c2bf00e6a93d98e32e0b" (UID: "e126af427288c2bf00e6a93d98e32e0b"). InnerVolumeSpecName "kubeconfig". PluginName "kubernetes.io/host-path", VolumeGidValue ""
Oct 18 22:22:32 k8s-master kubelet[12990]: I1018 22:22:32.913198   12990 operation_generator.go:535] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/e126af427288c2bf00e6a93d98e32e0b-ca-certs" (OuterVolumeSpecName: "ca-certs") pod "e126af427288c2bf00e6a93d98e32e0b" (UID: "e126af427288c2bf00e6a93d98e32e0b"). InnerVolumeSpecName "ca-certs". PluginName "kubernetes.io/host-path", VolumeGidValue ""
Oct 18 22:22:32 k8s-master kubelet[12990]: I1018 22:22:32.913171   12990 reconciler.go:186] operationExecutor.UnmountVolume started for volume "ca-certs" (UniqueName: "kubernetes.io/host-path/e126af427288c2bf00e6a93d98e32e0b-ca-certs") pod "e126af427288c2bf00e6a93d98e32e0b" (UID: "e126af427288c2bf00e6a93d98e32e0b")
Oct 18 22:22:32 k8s-master kubelet[12990]: I1018 22:22:32.913150   12990 reconciler.go:186] operationExecutor.UnmountVolume started for volume "ca-certs-etc-pki" (UniqueName: "kubernetes.io/host-path/e126af427288c2bf00e6a93d98e32e0b-ca-certs-etc-pki") pod "e126af427288c2bf00e6a93d98e32e0b" (UID: "e126af427288c2bf00e6a93d98e32e0b")
Oct 18 22:22:32 k8s-master kubelet[12990]: I1018 22:22:32.913126   12990 reconciler.go:186] operationExecutor.UnmountVolume started for volume "flexvolume-dir" (UniqueName: "kubernetes.io/host-path/e126af427288c2bf00e6a93d98e32e0b-flexvolume-dir") pod "e126af427288c2bf00e6a93d98e32e0b" (UID: "e126af427288c2bf00e6a93d98e32e0b")
Oct 18 22:22:32 k8s-master kubelet[12990]: I1018 22:22:32.913095   12990 reconciler.go:186] operationExecutor.UnmountVolume started for volume "k8s-certs" (UniqueName: "kubernetes.io/host-path/e126af427288c2bf00e6a93d98e32e0b-k8s-certs") pod "e126af427288c2bf00e6a93d98e32e0b" (UID: "e126af427288c2bf00e6a93d98e32e0b")
Oct 18 22:22:32 k8s-master kubelet[12990]: I1018 22:22:32.913043   12990 reconciler.go:186] operationExecutor.UnmountVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/e126af427288c2bf00e6a93d98e32e0b-kubeconfig") pod "e126af427288c2bf00e6a93d98e32e0b" (UID: "e126af427288c2bf00e6a93d98e32e0b")
Oct 18 22:22:32 k8s-master kubelet[12990]: E1018 22:22:32.483301   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:32 k8s-master kubelet[12990]: E1018 22:22:32.466266   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:32 k8s-master kubelet[12990]: E1018 22:22:32.461208   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:32 k8s-master kubelet[12990]: W1018 22:22:32.084170   12990 pod_container_deletor.go:77] Container "64fa0aa9175564b3c7cbd541f2ccffc16bee8be3c3b5f7059e0fb4657d3e0a87" not found in pod's containers
Oct 18 22:22:31 k8s-master kubelet[12990]: E1018 22:22:31.550303   12990 kuberuntime_container.go:66] Can't make a ref to pod "kube-controller-manager-k8s-master_kube-system(e126af427288c2bf00e6a93d98e32e0b)", container kube-controller-manager: selfLink was empty, can't make reference
Oct 18 22:22:31 k8s-master kubelet[12990]: E1018 22:22:31.547145   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:31 k8s-master kubelet[12990]: E1018 22:22:31.547078   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:31 k8s-master kubelet[12990]: E1018 22:22:31.482658   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:31 k8s-master kubelet[12990]: E1018 22:22:31.465747   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:31 k8s-master kubelet[12990]: E1018 22:22:31.460525   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:31 k8s-master kubelet[12990]: E1018 22:22:31.043358   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:31 k8s-master kubelet[12990]: W1018 22:22:31.043130   12990 container.go:354] Failed to create summary reader for "/libcontainer_13813_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:22:31 k8s-master kubelet[12990]: W1018 22:22:31.023645   12990 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_13813_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_13813_systemd_test_default.slice: no such file or directory
Oct 18 22:22:30 k8s-master kubelet[12990]: E1018 22:22:30.888644   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:30 k8s-master kubelet[12990]: W1018 22:22:30.888458   12990 container.go:354] Failed to create summary reader for "/libcontainer_13807_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:22:30 k8s-master kubelet[12990]: W1018 22:22:30.879531   12990 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_13807_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_13807_systemd_test_default.slice: no such file or directory
Oct 18 22:22:30 k8s-master kubelet[12990]: E1018 22:22:30.810845   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:30 k8s-master kubelet[12990]: E1018 22:22:30.744182   12990 kubelet.go:1612] Failed creating a mirror pod for "kube-controller-manager-k8s-master_kube-system(a61c40aac2e15292422c9e836cc018f8)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:30 k8s-master kubelet[12990]: E1018 22:22:30.743841   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-controller-manager-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:30 k8s-master kubelet[12990]: W1018 22:22:30.742580   12990 kubelet.go:1601] Deleting mirror pod "kube-controller-manager-k8s-master_kube-system(4522cb94-b40f-11e7-aa6c-08002782873b)" because it is outdated
Oct 18 22:22:30 k8s-master kubelet[12990]: W1018 22:22:30.716360   12990 status_manager.go:431] Failed to get status for pod "kube-controller-manager-k8s-master_kube-system(a61c40aac2e15292422c9e836cc018f8)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:30 k8s-master kubelet[12990]: W1018 22:22:30.716000   12990 status_manager.go:431] Failed to get status for pod "kube-controller-manager-k8s-master_kube-system(e126af427288c2bf00e6a93d98e32e0b)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:30 k8s-master kubelet[12990]: E1018 22:22:30.714823   12990 prober_manager.go:154] Liveness probe already exists! kube-controller-manager-k8s-master_kube-system(e126af427288c2bf00e6a93d98e32e0b) - kube-controller-manager
Oct 18 22:22:30 k8s-master kubelet[12990]: E1018 22:22:30.707514   12990 file_linux.go:114] Can't process manifest file "/etc/kubernetes/manifests/4913": open /etc/kubernetes/manifests/4913: no such file or directory
Oct 18 22:22:30 k8s-master kubelet[12990]: E1018 22:22:30.481969   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:30 k8s-master kubelet[12990]: E1018 22:22:30.465067   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:30 k8s-master kubelet[12990]: E1018 22:22:30.459837   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:29 k8s-master kubelet[12990]: E1018 22:22:29.544245   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:29 k8s-master kubelet[12990]: E1018 22:22:29.481345   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:29 k8s-master kubelet[12990]: E1018 22:22:29.464449   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:29 k8s-master kubelet[12990]: E1018 22:22:29.459218   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:28 k8s-master kubelet[12990]: E1018 22:22:28.480714   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:28 k8s-master kubelet[12990]: E1018 22:22:28.463829   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:28 k8s-master kubelet[12990]: E1018 22:22:28.458594   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:27 k8s-master kubelet[12990]: E1018 22:22:27.589894   12990 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:22:27 k8s-master kubelet[12990]: E1018 22:22:27.547695   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:27 k8s-master kubelet[12990]: E1018 22:22:27.480066   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:27 k8s-master kubelet[12990]: E1018 22:22:27.463125   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:27 k8s-master kubelet[12990]: E1018 22:22:27.457947   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:26 k8s-master kubelet[12990]: E1018 22:22:26.479465   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:26 k8s-master kubelet[12990]: E1018 22:22:26.462608   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:26 k8s-master kubelet[12990]: E1018 22:22:26.457171   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:25 k8s-master kubelet[12990]: E1018 22:22:25.546409   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:25 k8s-master kubelet[12990]: E1018 22:22:25.478718   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:25 k8s-master kubelet[12990]: E1018 22:22:25.461992   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:25 k8s-master kubelet[12990]: E1018 22:22:25.456392   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:24 k8s-master kubelet[12990]: E1018 22:22:24.478105   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:24 k8s-master kubelet[12990]: E1018 22:22:24.461387   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:24 k8s-master kubelet[12990]: E1018 22:22:24.455547   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:23 k8s-master kubelet[12990]: E1018 22:22:23.543968   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:23 k8s-master kubelet[12990]: E1018 22:22:23.477534   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:23 k8s-master kubelet[12990]: E1018 22:22:23.460783   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:23 k8s-master kubelet[12990]: E1018 22:22:23.454878   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:22 k8s-master kubelet[12990]: E1018 22:22:22.951228   12990 kubelet_node_status.go:378] Unable to update node status: update node status exceeds retry count
Oct 18 22:22:22 k8s-master kubelet[12990]: E1018 22:22:22.951213   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:22 k8s-master kubelet[12990]: E1018 22:22:22.951053   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:22 k8s-master kubelet[12990]: E1018 22:22:22.950916   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:22 k8s-master kubelet[12990]: E1018 22:22:22.950759   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:22 k8s-master kubelet[12990]: E1018 22:22:22.950573   12990 kubelet_node_status.go:386] Error updating node status, will retry: error getting node "k8s-master": Get https://192.168.0.155:6443/api/v1/nodes/k8s-master?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:22 k8s-master kubelet[12990]: E1018 22:22:22.476940   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:22 k8s-master kubelet[12990]: E1018 22:22:22.460242   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:22 k8s-master kubelet[12990]: E1018 22:22:22.454228   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:21 k8s-master kubelet[12990]: I1018 22:22:21.657015   12990 reconciler.go:290] Volume detached for volume "k8s-certs" (UniqueName: "kubernetes.io/host-path/c4260572c37ef028f5a8df0c2e45419f-k8s-certs") on node "k8s-master" DevicePath ""
Oct 18 22:22:21 k8s-master kubelet[12990]: I1018 22:22:21.657004   12990 reconciler.go:290] Volume detached for volume "ca-certs-etc-pki" (UniqueName: "kubernetes.io/host-path/c4260572c37ef028f5a8df0c2e45419f-ca-certs-etc-pki") on node "k8s-master" DevicePath ""
Oct 18 22:22:21 k8s-master kubelet[12990]: I1018 22:22:21.656966   12990 reconciler.go:290] Volume detached for volume "ca-certs" (UniqueName: "kubernetes.io/host-path/c4260572c37ef028f5a8df0c2e45419f-ca-certs") on node "k8s-master" DevicePath ""
Oct 18 22:22:21 k8s-master kubelet[12990]: E1018 22:22:21.580016   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:21 k8s-master kubelet[12990]: I1018 22:22:21.559410   12990 operation_generator.go:535] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/c4260572c37ef028f5a8df0c2e45419f-ca-certs-etc-pki" (OuterVolumeSpecName: "ca-certs-etc-pki") pod "c4260572c37ef028f5a8df0c2e45419f" (UID: "c4260572c37ef028f5a8df0c2e45419f"). InnerVolumeSpecName "ca-certs-etc-pki". PluginName "kubernetes.io/host-path", VolumeGidValue ""
Oct 18 22:22:21 k8s-master kubelet[12990]: I1018 22:22:21.559373   12990 operation_generator.go:535] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/c4260572c37ef028f5a8df0c2e45419f-ca-certs" (OuterVolumeSpecName: "ca-certs") pod "c4260572c37ef028f5a8df0c2e45419f" (UID: "c4260572c37ef028f5a8df0c2e45419f"). InnerVolumeSpecName "ca-certs". PluginName "kubernetes.io/host-path", VolumeGidValue ""
Oct 18 22:22:21 k8s-master kubelet[12990]: I1018 22:22:21.556770   12990 operation_generator.go:535] UnmountVolume.TearDown succeeded for volume "kubernetes.io/host-path/c4260572c37ef028f5a8df0c2e45419f-k8s-certs" (OuterVolumeSpecName: "k8s-certs") pod "c4260572c37ef028f5a8df0c2e45419f" (UID: "c4260572c37ef028f5a8df0c2e45419f"). InnerVolumeSpecName "k8s-certs". PluginName "kubernetes.io/host-path", VolumeGidValue ""
Oct 18 22:22:21 k8s-master kubelet[12990]: I1018 22:22:21.556715   12990 reconciler.go:186] operationExecutor.UnmountVolume started for volume "k8s-certs" (UniqueName: "kubernetes.io/host-path/c4260572c37ef028f5a8df0c2e45419f-k8s-certs") pod "c4260572c37ef028f5a8df0c2e45419f" (UID: "c4260572c37ef028f5a8df0c2e45419f")
Oct 18 22:22:21 k8s-master kubelet[12990]: I1018 22:22:21.556669   12990 reconciler.go:186] operationExecutor.UnmountVolume started for volume "ca-certs-etc-pki" (UniqueName: "kubernetes.io/host-path/c4260572c37ef028f5a8df0c2e45419f-ca-certs-etc-pki") pod "c4260572c37ef028f5a8df0c2e45419f" (UID: "c4260572c37ef028f5a8df0c2e45419f")
Oct 18 22:22:21 k8s-master kubelet[12990]: I1018 22:22:21.556621   12990 reconciler.go:186] operationExecutor.UnmountVolume started for volume "ca-certs" (UniqueName: "kubernetes.io/host-path/c4260572c37ef028f5a8df0c2e45419f-ca-certs") pod "c4260572c37ef028f5a8df0c2e45419f" (UID: "c4260572c37ef028f5a8df0c2e45419f")
Oct 18 22:22:21 k8s-master kubelet[12990]: E1018 22:22:21.476328   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:21 k8s-master kubelet[12990]: E1018 22:22:21.459700   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:21 k8s-master kubelet[12990]: E1018 22:22:21.452843   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:20 k8s-master kubelet[12990]: E1018 22:22:20.475724   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:20 k8s-master kubelet[12990]: E1018 22:22:20.459163   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:20 k8s-master kubelet[12990]: E1018 22:22:20.451120   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:19 k8s-master kubelet[12990]: W1018 22:22:19.980041   12990 pod_container_deletor.go:77] Container "d907231915c1145f62e98c02eade4ce0eeb037a387544c1ca66b12e432b70eb6" not found in pod's containers
Oct 18 22:22:19 k8s-master kubelet[12990]: E1018 22:22:19.743094   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:19 k8s-master kubelet[12990]: W1018 22:22:19.742975   12990 container.go:354] Failed to create summary reader for "/libcontainer_13761_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:22:19 k8s-master kubelet[12990]: W1018 22:22:19.731302   12990 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_13761_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_13761_systemd_test_default.slice: no such file or directory
Oct 18 22:22:19 k8s-master kubelet[12990]: E1018 22:22:19.686729   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:19 k8s-master kubelet[12990]: E1018 22:22:19.686147   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:19 k8s-master kubelet[12990]: W1018 22:22:19.685580   12990 container.go:354] Failed to create summary reader for "/libcontainer_13757_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:22:19 k8s-master kubelet[12990]: W1018 22:22:19.683533   12990 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_13757_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_13757_systemd_test_default.slice: no such file or directory
Oct 18 22:22:19 k8s-master kubelet[12990]: E1018 22:22:19.648202   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:19 k8s-master kubelet[12990]: W1018 22:22:19.647620   12990 container.go:354] Failed to create summary reader for "/libcontainer_13754_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:22:19 k8s-master kubelet[12990]: W1018 22:22:19.647620   12990 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_13754_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_13754_systemd_test_default.slice: no such file or directory
Oct 18 22:22:19 k8s-master kubelet[12990]: E1018 22:22:19.616840   12990 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:22:19 k8s-master kubelet[12990]: E1018 22:22:19.596202   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:19 k8s-master kubelet[12990]: E1018 22:22:19.590138   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:19 k8s-master kubelet[12990]: W1018 22:22:19.590000   12990 container.go:354] Failed to create summary reader for "/libcontainer_13751_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:22:19 k8s-master kubelet[12990]: W1018 22:22:19.571416   12990 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_13751_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_13751_systemd_test_default.slice: no such file or directory
Oct 18 22:22:19 k8s-master kubelet[12990]: E1018 22:22:19.486597   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:22:19 k8s-master kubelet[12990]: W1018 22:22:19.486597   12990 container.go:354] Failed to create summary reader for "/libcontainer_13743_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:22:19 k8s-master kubelet[12990]: W1018 22:22:19.484048   12990 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_13743_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_13743_systemd_test_default.slice: no such file or directory
Oct 18 22:22:19 k8s-master kubelet[12990]: E1018 22:22:19.450491   12990 reflector.go:315] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to watch *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=131&timeoutSeconds=401&watch=true: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:19 k8s-master kubelet[12990]: E1018 22:22:19.450462   12990 reflector.go:315] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to watch *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=198&timeoutSeconds=553&watch=true: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:19 k8s-master kubelet[12990]: E1018 22:22:19.450409   12990 reflector.go:315] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to watch *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=179&timeoutSeconds=505&watch=true: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:19 k8s-master kubelet[12990]: E1018 22:22:19.450039   12990 kubelet.go:1612] Failed creating a mirror pod for "kube-apiserver-k8s-master_kube-system(5bac89d6fa5c197aedb63c14ea44959a)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:22:19 k8s-master kubelet[12990]: E1018 22:22:19.449847   12990 mirror_client.go:88] Failed deleting a mirror pod "kube-apiserver-k8s-master_kube-system": Delete https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: read tcp 192.168.0.155:39427->192.168.0.155:6443: read: connection reset by peer; some request body already written
Oct 18 22:22:19 k8s-master kubelet[12990]: W1018 22:22:19.449797   12990 status_manager.go:431] Failed to get status for pod "kube-apiserver-k8s-master_kube-system(5bac89d6fa5c197aedb63c14ea44959a)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: read tcp 192.168.0.155:39427->192.168.0.155:6443: read: connection reset by peer
Oct 18 22:22:19 k8s-master kubelet[12990]: W1018 22:22:19.354601   12990 kubelet.go:1601] Deleting mirror pod "kube-apiserver-k8s-master_kube-system(42c0e5b5-b40f-11e7-aa6c-08002782873b)" because it is outdated
Oct 18 22:22:19 k8s-master kubelet[12990]: E1018 22:22:19.283499   12990 prober_manager.go:154] Liveness probe already exists! kube-apiserver-k8s-master_kube-system(c4260572c37ef028f5a8df0c2e45419f) - kube-apiserver
Oct 18 22:22:19 k8s-master kubelet[12990]: E1018 22:22:19.227847   12990 file_linux.go:114] Can't process manifest file "/etc/kubernetes/manifests/4913": open /etc/kubernetes/manifests/4913: no such file or directory
Oct 18 22:20:12 k8s-master kubelet[12990]: E1018 22:20:12.216861   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:18:24 k8s-master kubelet[12990]: E1018 22:18:24.828697   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:18:24 k8s-master kubelet[12990]: E1018 22:18:24.188829   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:18:19 k8s-master kubelet[12990]: E1018 22:18:19.619031   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:18:19 k8s-master kubelet[12990]: E1018 22:18:19.100014   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:18:18 k8s-master kubelet[12990]: E1018 22:18:18.863354   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:18:18 k8s-master kubelet[12990]: E1018 22:18:18.744346   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:18:17 k8s-master kubelet[12990]: E1018 22:18:17.481642   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:18:14 k8s-master kubelet[12990]: E1018 22:18:14.326947   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:18:13 k8s-master kubelet[12990]: E1018 22:18:13.000404   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:18:09 k8s-master kubelet[12990]: E1018 22:18:09.730334   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:18:08 k8s-master kubelet[12990]: E1018 22:18:08.908066   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:18:02 k8s-master kubelet[12990]: E1018 22:18:02.403527   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:17:58 k8s-master kubelet[12990]: E1018 22:17:58.304359   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:17:56 k8s-master kubelet[12990]: E1018 22:17:56.606638   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:17:52 k8s-master kubelet[12990]: E1018 22:17:52.974558   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:17:52 k8s-master kubelet[12990]: E1018 22:17:52.425464   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:17:50 k8s-master kubelet[12990]: E1018 22:17:50.367077   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:17:48 k8s-master kubelet[12990]: E1018 22:17:48.475717   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:17:48 k8s-master kubelet[12990]: E1018 22:17:48.085701   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:17:46 k8s-master kubelet[12990]: E1018 22:17:46.181608   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:17:45 k8s-master kubelet[12990]: E1018 22:17:45.233646   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:17:44 k8s-master kubelet[12990]: E1018 22:17:44.848160   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:17:42 k8s-master kubelet[12990]: I1018 22:17:42.559446   12990 kubelet_node_status.go:86] Successfully registered node k8s-master
Oct 18 22:17:39 k8s-master kubelet[12990]: E1018 22:17:39.828917   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:17:38 k8s-master kubelet[12990]: I1018 22:17:38.451347   12990 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:17:38 k8s-master kubelet[12990]: I1018 22:17:38.447720   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:38 k8s-master kubelet[12990]: E1018 22:17:38.008498   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:37 k8s-master kubelet[12990]: E1018 22:17:37.808696   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:37 k8s-master kubelet[12990]: E1018 22:17:37.608718   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:36 k8s-master kubelet[12990]: E1018 22:17:36.998402   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:36 k8s-master kubelet[12990]: E1018 22:17:36.807980   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:36 k8s-master kubelet[12990]: E1018 22:17:36.599161   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:36 k8s-master kubelet[12990]: E1018 22:17:36.375051   12990 kubelet.go:1612] Failed creating a mirror pod for "etcd-k8s-master_kube-system(632cf36548152dcede5f0a5254ea0e6d)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:36 k8s-master kubelet[12990]: I1018 22:17:36.371684   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:35 k8s-master kubelet[12990]: E1018 22:17:35.987893   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:35 k8s-master kubelet[12990]: E1018 22:17:35.828581   12990 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/default/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:17:35 k8s-master kubelet[12990]: E1018 22:17:35.828035   12990 eviction_manager.go:238] eviction manager: unexpected err: failed to get node info: node 'k8s-master' not found
Oct 18 22:17:35 k8s-master kubelet[12990]: E1018 22:17:35.787983   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:35 k8s-master kubelet[12990]: E1018 22:17:35.589228   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:35 k8s-master kubelet[12990]: E1018 22:17:35.384581   12990 kubelet.go:1612] Failed creating a mirror pod for "kube-controller-manager-k8s-master_kube-system(e126af427288c2bf00e6a93d98e32e0b)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:35 k8s-master kubelet[12990]: E1018 22:17:35.384487   12990 kubelet.go:1612] Failed creating a mirror pod for "kube-apiserver-k8s-master_kube-system(c4260572c37ef028f5a8df0c2e45419f)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:35 k8s-master kubelet[12990]: E1018 22:17:35.382013   12990 kubelet.go:1612] Failed creating a mirror pod for "kube-scheduler-k8s-master_kube-system(5c60da2dd3a2204955600c1da11b55d9)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:35 k8s-master kubelet[12990]: E1018 22:17:35.381931   12990 kubelet.go:1612] Failed creating a mirror pod for "etcd-k8s-master_kube-system(632cf36548152dcede5f0a5254ea0e6d)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:35 k8s-master kubelet[12990]: W1018 22:17:35.381876   12990 status_manager.go:431] Failed to get status for pod "etcd-k8s-master_kube-system(632cf36548152dcede5f0a5254ea0e6d)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/etcd-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:35 k8s-master kubelet[12990]: I1018 22:17:35.377133   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:35 k8s-master kubelet[12990]: I1018 22:17:35.376100   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:35 k8s-master kubelet[12990]: I1018 22:17:35.367143   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:35 k8s-master kubelet[12990]: I1018 22:17:35.366830   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:34 k8s-master kubelet[12990]: E1018 22:17:34.400364   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:34 k8s-master kubelet[12990]: E1018 22:17:34.400305   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:34 k8s-master kubelet[12990]: E1018 22:17:34.385587   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:34 k8s-master kubelet[12990]: E1018 22:17:34.357376   12990 kubelet.go:1612] Failed creating a mirror pod for "kube-scheduler-k8s-master_kube-system(5c60da2dd3a2204955600c1da11b55d9)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:34 k8s-master kubelet[12990]: W1018 22:17:34.357273   12990 status_manager.go:431] Failed to get status for pod "kube-scheduler-k8s-master_kube-system(5c60da2dd3a2204955600c1da11b55d9)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:34 k8s-master kubelet[12990]: I1018 22:17:34.343763   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:34 k8s-master kubelet[12990]: E1018 22:17:34.338727   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:17:34 k8s-master kubelet[12990]: E1018 22:17:34.313241   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:17:34 k8s-master kubelet[12990]: W1018 22:17:34.313149   12990 container.go:354] Failed to create summary reader for "/libcontainer_13269_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:17:34 k8s-master kubelet[12990]: W1018 22:17:34.313018   12990 container.go:354] Failed to create summary reader for "/libcontainer_13260_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:17:34 k8s-master kubelet[12990]: E1018 22:17:34.290207   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:17:34 k8s-master kubelet[12990]: E1018 22:17:34.289818   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:17:34 k8s-master kubelet[12990]: W1018 22:17:34.268606   12990 container.go:354] Failed to create summary reader for "/libcontainer_13242_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:17:34 k8s-master kubelet[12990]: W1018 22:17:34.268490   12990 container.go:354] Failed to create summary reader for "/libcontainer_13233_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:17:34 k8s-master kubelet[12990]: E1018 22:17:34.032129   12990 kubelet.go:1612] Failed creating a mirror pod for "kube-apiserver-k8s-master_kube-system(c4260572c37ef028f5a8df0c2e45419f)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:34 k8s-master kubelet[12990]: I1018 22:17:33.986275   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:34 k8s-master kubelet[12990]: E1018 22:17:33.981977   12990 kubelet.go:1612] Failed creating a mirror pod for "kube-controller-manager-k8s-master_kube-system(e126af427288c2bf00e6a93d98e32e0b)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:34 k8s-master kubelet[12990]: W1018 22:17:33.981889   12990 status_manager.go:431] Failed to get status for pod "kube-controller-manager-k8s-master_kube-system(e126af427288c2bf00e6a93d98e32e0b)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:34 k8s-master kubelet[12990]: I1018 22:17:33.974268   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:33 k8s-master kubelet[12990]: E1018 22:17:33.385555   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:33 k8s-master kubelet[12990]: E1018 22:17:33.385445   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:33 k8s-master kubelet[12990]: E1018 22:17:33.377304   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:32 k8s-master kubelet[12990]: E1018 22:17:32.946591   12990 kubelet.go:1612] Failed creating a mirror pod for "kube-apiserver-k8s-master_kube-system(c4260572c37ef028f5a8df0c2e45419f)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:32 k8s-master kubelet[12990]: W1018 22:17:32.946545   12990 status_manager.go:431] Failed to get status for pod "kube-apiserver-k8s-master_kube-system(c4260572c37ef028f5a8df0c2e45419f)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:32 k8s-master kubelet[12990]: W1018 22:17:32.945847   12990 pod_container_deletor.go:77] Container "d907231915c1145f62e98c02eade4ce0eeb037a387544c1ca66b12e432b70eb6" not found in pod's containers
Oct 18 22:17:32 k8s-master kubelet[12990]: I1018 22:17:32.943115   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:32 k8s-master kubelet[12990]: I1018 22:17:32.942710   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:32 k8s-master kubelet[12990]: W1018 22:17:32.938979   12990 pod_container_deletor.go:77] Container "64fa0aa9175564b3c7cbd541f2ccffc16bee8be3c3b5f7059e0fb4657d3e0a87" not found in pod's containers
Oct 18 22:17:32 k8s-master kubelet[12990]: I1018 22:17:32.927846   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:32 k8s-master kubelet[12990]: W1018 22:17:32.880421   12990 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_13260_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_13260_systemd_test_default.slice: no such file or directory
Oct 18 22:17:32 k8s-master kubelet[12990]: W1018 22:17:32.680479   12990 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_13233_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_13233_systemd_test_default.slice: no such file or directory
Oct 18 22:17:32 k8s-master kubelet[12990]: W1018 22:17:32.628259   12990 pod_container_deletor.go:77] Container "2acd19a6d0ac7789f6ccd31ef88306d78b5c732223f984ce5c4c8e8f9caa5980" not found in pod's containers
Oct 18 22:17:32 k8s-master kubelet[12990]: I1018 22:17:32.525417   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:32 k8s-master kubelet[12990]: W1018 22:17:32.524601   12990 docker_container.go:202] Deleted previously existing symlink file: "/var/log/pods/5c60da2dd3a2204955600c1da11b55d9/kube-scheduler_0.log"
Oct 18 22:17:32 k8s-master kubelet[12990]: E1018 22:17:32.374875   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:32 k8s-master kubelet[12990]: E1018 22:17:32.374817   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:32 k8s-master kubelet[12990]: E1018 22:17:32.374741   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:32 k8s-master kubelet[12990]: E1018 22:17:32.042459   12990 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:32 k8s-master kubelet[12990]: I1018 22:17:32.042064   12990 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:17:32 k8s-master kubelet[12990]: I1018 22:17:32.036654   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:31 k8s-master kubelet[12990]: W1018 22:17:31.946654   12990 pod_container_deletor.go:77] Container "ae752291e088659adba78f215ccad2d35546490ffa09bc55105b0b57fa989df8" not found in pod's containers
Oct 18 22:17:31 k8s-master kubelet[12990]: I1018 22:17:31.901104   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:31 k8s-master kubelet[12990]: E1018 22:17:31.364257   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:31 k8s-master kubelet[12990]: E1018 22:17:31.364208   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:31 k8s-master kubelet[12990]: E1018 22:17:31.364157   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:31 k8s-master kubelet[12990]: E1018 22:17:31.362956   12990 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:17:31 k8s-master kubelet[12990]: W1018 22:17:31.362628   12990 container.go:354] Failed to create summary reader for "/libcontainer_13058_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:17:31 k8s-master kubelet[12990]: W1018 22:17:31.330126   12990 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_13058_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_13058_systemd_test_default.slice: no such file or directory
Oct 18 22:17:30 k8s-master kubelet[12990]: E1018 22:17:30.645319   12990 kubelet.go:1612] Failed creating a mirror pod for "kube-controller-manager-k8s-master_kube-system(e126af427288c2bf00e6a93d98e32e0b)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:30 k8s-master kubelet[12990]: I1018 22:17:30.644776   12990 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/5c60da2dd3a2204955600c1da11b55d9-kubeconfig") pod "kube-scheduler-k8s-master" (UID: "5c60da2dd3a2204955600c1da11b55d9")
Oct 18 22:17:30 k8s-master kubelet[12990]: I1018 22:17:30.644750   12990 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs-etc-pki" (UniqueName: "kubernetes.io/host-path/e126af427288c2bf00e6a93d98e32e0b-ca-certs-etc-pki") pod "kube-controller-manager-k8s-master" (UID: "e126af427288c2bf00e6a93d98e32e0b")
Oct 18 22:17:30 k8s-master kubelet[12990]: I1018 22:17:30.644727   12990 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "flexvolume-dir" (UniqueName: "kubernetes.io/host-path/e126af427288c2bf00e6a93d98e32e0b-flexvolume-dir") pod "kube-controller-manager-k8s-master" (UID: "e126af427288c2bf00e6a93d98e32e0b")
Oct 18 22:17:30 k8s-master kubelet[12990]: I1018 22:17:30.644708   12990 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs" (UniqueName: "kubernetes.io/host-path/e126af427288c2bf00e6a93d98e32e0b-ca-certs") pod "kube-controller-manager-k8s-master" (UID: "e126af427288c2bf00e6a93d98e32e0b")
Oct 18 22:17:30 k8s-master kubelet[12990]: I1018 22:17:30.644687   12990 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "k8s-certs" (UniqueName: "kubernetes.io/host-path/e126af427288c2bf00e6a93d98e32e0b-k8s-certs") pod "kube-controller-manager-k8s-master" (UID: "e126af427288c2bf00e6a93d98e32e0b")
Oct 18 22:17:30 k8s-master kubelet[12990]: I1018 22:17:30.644668   12990 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs-etc-pki" (UniqueName: "kubernetes.io/host-path/c4260572c37ef028f5a8df0c2e45419f-ca-certs-etc-pki") pod "kube-apiserver-k8s-master" (UID: "c4260572c37ef028f5a8df0c2e45419f")
Oct 18 22:17:30 k8s-master kubelet[12990]: I1018 22:17:30.644648   12990 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs" (UniqueName: "kubernetes.io/host-path/c4260572c37ef028f5a8df0c2e45419f-ca-certs") pod "kube-apiserver-k8s-master" (UID: "c4260572c37ef028f5a8df0c2e45419f")
Oct 18 22:17:30 k8s-master kubelet[12990]: I1018 22:17:30.644629   12990 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "k8s-certs" (UniqueName: "kubernetes.io/host-path/c4260572c37ef028f5a8df0c2e45419f-k8s-certs") pod "kube-apiserver-k8s-master" (UID: "c4260572c37ef028f5a8df0c2e45419f")
Oct 18 22:17:30 k8s-master kubelet[12990]: I1018 22:17:30.644599   12990 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "etcd" (UniqueName: "kubernetes.io/host-path/632cf36548152dcede5f0a5254ea0e6d-etcd") pod "etcd-k8s-master" (UID: "632cf36548152dcede5f0a5254ea0e6d")
Oct 18 22:17:30 k8s-master kubelet[12990]: I1018 22:17:30.644565   12990 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/e126af427288c2bf00e6a93d98e32e0b-kubeconfig") pod "kube-controller-manager-k8s-master" (UID: "e126af427288c2bf00e6a93d98e32e0b")
Oct 18 22:17:30 k8s-master kubelet[12990]: E1018 22:17:30.625731   12990 kubelet.go:1612] Failed creating a mirror pod for "kube-apiserver-k8s-master_kube-system(c4260572c37ef028f5a8df0c2e45419f)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:30 k8s-master kubelet[12990]: E1018 22:17:30.607656   12990 kubelet.go:1612] Failed creating a mirror pod for "etcd-k8s-master_kube-system(632cf36548152dcede5f0a5254ea0e6d)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:30 k8s-master kubelet[12990]: E1018 22:17:30.566349   12990 kubelet.go:1612] Failed creating a mirror pod for "kube-scheduler-k8s-master_kube-system(5c60da2dd3a2204955600c1da11b55d9)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:30 k8s-master kubelet[12990]: W1018 22:17:30.566302   12990 status_manager.go:431] Failed to get status for pod "kube-scheduler-k8s-master_kube-system(5c60da2dd3a2204955600c1da11b55d9)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:30 k8s-master kubelet[12990]: W1018 22:17:30.564347   12990 status_manager.go:431] Failed to get status for pod "kube-controller-manager-k8s-master_kube-system(e126af427288c2bf00e6a93d98e32e0b)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:30 k8s-master kubelet[12990]: I1018 22:17:30.562393   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:30 k8s-master kubelet[12990]: W1018 22:17:30.559867   12990 status_manager.go:431] Failed to get status for pod "kube-apiserver-k8s-master_kube-system(c4260572c37ef028f5a8df0c2e45419f)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:30 k8s-master kubelet[12990]: I1018 22:17:30.559352   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:30 k8s-master kubelet[12990]: I1018 22:17:30.554178   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:30 k8s-master kubelet[12990]: W1018 22:17:30.546515   12990 status_manager.go:431] Failed to get status for pod "etcd-k8s-master_kube-system(632cf36548152dcede5f0a5254ea0e6d)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/etcd-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:30 k8s-master kubelet[12990]: I1018 22:17:30.546236   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:30 k8s-master kubelet[12990]: I1018 22:17:30.544870   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:30 k8s-master kubelet[12990]: I1018 22:17:30.541841   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:30 k8s-master kubelet[12990]: I1018 22:17:30.541415   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:30 k8s-master kubelet[12990]: I1018 22:17:30.538973   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:30 k8s-master kubelet[12990]: E1018 22:17:30.350810   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:30 k8s-master kubelet[12990]: E1018 22:17:30.337807   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:30 k8s-master kubelet[12990]: E1018 22:17:30.316125   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:29 k8s-master kubelet[12990]: E1018 22:17:29.350209   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:29 k8s-master kubelet[12990]: E1018 22:17:29.337187   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:29 k8s-master kubelet[12990]: E1018 22:17:29.315455   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:28 k8s-master kubelet[12990]: E1018 22:17:28.833906   12990 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:28 k8s-master kubelet[12990]: I1018 22:17:28.833595   12990 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:17:28 k8s-master kubelet[12990]: I1018 22:17:28.831035   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:28 k8s-master kubelet[12990]: E1018 22:17:28.349549   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:28 k8s-master kubelet[12990]: E1018 22:17:28.336487   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:28 k8s-master kubelet[12990]: E1018 22:17:28.312899   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:27 k8s-master kubelet[12990]: E1018 22:17:27.348959   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:27 k8s-master kubelet[12990]: E1018 22:17:27.335867   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:27 k8s-master kubelet[12990]: E1018 22:17:27.311964   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:27 k8s-master kubelet[12990]: E1018 22:17:27.230665   12990 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:27 k8s-master kubelet[12990]: I1018 22:17:27.230354   12990 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:17:27 k8s-master kubelet[12990]: I1018 22:17:27.228196   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:26 k8s-master kubelet[12990]: E1018 22:17:26.427822   12990 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:26 k8s-master kubelet[12990]: I1018 22:17:26.426781   12990 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:17:26 k8s-master kubelet[12990]: I1018 22:17:26.417801   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:26 k8s-master kubelet[12990]: E1018 22:17:26.348218   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:26 k8s-master kubelet[12990]: E1018 22:17:26.334626   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:26 k8s-master kubelet[12990]: E1018 22:17:26.311034   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:26 k8s-master kubelet[12990]: E1018 22:17:26.017434   12990 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:26 k8s-master kubelet[12990]: I1018 22:17:26.017024   12990 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:17:26 k8s-master kubelet[12990]: I1018 22:17:26.013918   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:25 k8s-master kubelet[12990]: E1018 22:17:25.813627   12990 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.812539   12990 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:17:25 k8s-master kubelet[12990]: E1018 22:17:25.811218   12990 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/default/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:17:25 k8s-master kubelet[12990]: E1018 22:17:25.810186   12990 eviction_manager.go:238] eviction manager: unexpected err: failed to get node info: node 'k8s-master' not found
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.705238   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.666109   12990 manager.go:316] Recovery completed
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.584189   12990 manager.go:311] Starting recovery of all containers
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.582987   12990 manager.go:1140] Started watching for new ooms in manager
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.582863   12990 factory.go:86] Registering Raw factory
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.582681   12990 factory.go:54] Registering systemd factory
Oct 18 22:17:25 k8s-master kubelet[12990]: W1018 22:17:25.582657   12990 manager.go:276] Registration of the crio container factory failed: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:17:25 k8s-master kubelet[12990]: W1018 22:17:25.582516   12990 manager.go:265] Registration of the rkt container factory failed: unable to communicate with Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.582487   12990 factory.go:355] Registering Docker factory
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.539116   12990 volume_manager.go:246] Starting Kubelet Volume Manager
Oct 18 22:17:25 k8s-master kubelet[12990]: E1018 22:17:25.539097   12990 container_manager_linux.go:603] [ContainerManager]: Fail to get rootfs information unable to find data for container /
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.538425   12990 kubelet.go:1779] skipping pod synchronization - [container runtime is down PLEG is not healthy: pleg was last seen active 2562047h47m16.854775807s ago; threshold is 3m0s]
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.538414   12990 kubelet.go:1768] Starting kubelet main sync loop.
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.538400   12990 status_manager.go:140] Starting to sync pod status with apiserver
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.538367   12990 fs_resource_analyzer.go:66] Starting FS ResourceAnalyzer
Oct 18 22:17:25 k8s-master kubelet[12990]: E1018 22:17:25.526991   12990 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/default/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.519599   12990 server.go:296] Adding debug handlers to kubelet server.
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.518682   12990 server.go:128] Starting to listen on 0.0.0.0:10250
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.518365   12990 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:17:25 k8s-master kubelet[12990]: E1018 22:17:25.517598   12990 kubelet.go:1234] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data for container /
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.517328   12990 server.go:718] Started kubelet v1.8.1
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.514759   12990 kuberuntime_manager.go:177] Container runtime docker initialized, version: 17.10.0-ce, apiVersion: 1.33.0
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.511959   12990 remote_runtime.go:43] Connecting to runtime service unix:///var/run/dockershim.sock
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.481130   12990 docker_service.go:224] Setting cgroupDriver to systemd
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.455976   12990 docker_service.go:207] Docker cri networking managed by kubernetes.io/no-op
Oct 18 22:17:25 k8s-master kubelet[12990]: W1018 22:17:25.419620   12990 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.347812   12990 kubelet.go:517] Hairpin mode set to "hairpin-veth"
Oct 18 22:17:25 k8s-master kubelet[12990]: W1018 22:17:25.347768   12990 kubelet_network.go:69] Hairpin mode set to "promiscuous-bridge" but kubenet is not enabled, falling back to "hairpin-veth"
Oct 18 22:17:25 k8s-master kubelet[12990]: E1018 22:17:25.310497   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:25 k8s-master kubelet[12990]: E1018 22:17:25.310448   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:25 k8s-master kubelet[12990]: E1018 22:17:25.310358   12990 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.258814   12990 kubelet.go:283] Watching apiserver
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.258771   12990 kubelet.go:273] Adding manifest file: /etc/kubernetes/manifests
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.258662   12990 container_manager_linux.go:288] Creating device plugin handler: false
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.258538   12990 container_manager_linux.go:257] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: ContainerRuntime:docker CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[{Signal:memory.available Operator:LessThan Value:{Quantity:100Mi Percentage:0} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.1} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.inodesFree Operator:LessThan Value:{Quantity:<nil> Percentage:0.05} GracePeriod:0s MinReclaim:<nil>}]} ExperimentalQOSReserved:map[] ExperimentalCPUManagerPolicy:none ExperimentalCPUManagerReconcilePeriod:10s}
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.258503   12990 container_manager_linux.go:252] container manager verified user specified cgroup-root exists: /
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.254569   12990 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.253956   12990 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.251334   12990 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.249750   12990 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0}]
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.249722   12990 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 22:17:25 k8s-master kubelet[12990]: W1018 22:17:25.230144   12990 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:17:25 k8s-master kubelet[12990]: W1018 22:17:25.230022   12990 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.208630   12990 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:17:25 k8s-master kubelet[12990]: E1018 22:17:25.208016   12990 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.206720   12990 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:17:25 k8s-master kubelet[12990]: W1018 22:17:25.181510   12990 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.181353   12990 feature_gate.go:156] feature gates: map[]
Oct 18 22:17:25 k8s-master kubelet[12990]: W1018 22:17:25.173297   12990 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.171598   12990 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.171561   12990 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.159455   12990 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.159450   12990 controller.go:114] kubelet config controller: starting controller
Oct 18 22:17:25 k8s-master kubelet[12990]: I1018 22:17:25.159287   12990 feature_gate.go:156] feature gates: map[]
Oct 18 22:17:25 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:17:25 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:17:25 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:17:14 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:17:14 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:17:14 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:17:14 k8s-master kubelet[12979]: error: failed to run Kubelet: unable to load bootstrap kubeconfig: stat /etc/kubernetes/bootstrap-kubelet.conf: no such file or directory
Oct 18 22:17:14 k8s-master kubelet[12979]: W1018 22:17:14.940293   12979 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:17:14 k8s-master kubelet[12979]: I1018 22:17:14.940134   12979 feature_gate.go:156] feature gates: map[]
Oct 18 22:17:14 k8s-master kubelet[12979]: W1018 22:17:14.883987   12979 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:17:14 k8s-master kubelet[12979]: I1018 22:17:14.880720   12979 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:17:14 k8s-master kubelet[12979]: I1018 22:17:14.880679   12979 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:17:14 k8s-master kubelet[12979]: I1018 22:17:14.842162   12979 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:17:14 k8s-master kubelet[12979]: I1018 22:17:14.842157   12979 controller.go:114] kubelet config controller: starting controller
Oct 18 22:17:14 k8s-master kubelet[12979]: I1018 22:17:14.842003   12979 feature_gate.go:156] feature gates: map[]
Oct 18 22:17:14 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:17:14 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:17:14 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:17:04 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:17:04 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:17:04 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:17:04 k8s-master kubelet[12948]: error: unable to load client CA file /etc/kubernetes/pki/ca.crt: open /etc/kubernetes/pki/ca.crt: no such file or directory
Oct 18 22:17:04 k8s-master kubelet[12948]: I1018 22:17:04.276737   12948 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:17:04 k8s-master kubelet[12948]: I1018 22:17:04.276732   12948 controller.go:114] kubelet config controller: starting controller
Oct 18 22:17:04 k8s-master kubelet[12948]: I1018 22:17:04.276631   12948 feature_gate.go:156] feature gates: map[]
Oct 18 22:17:04 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:17:04 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:17:04 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:16:48 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:16:48 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:16:48 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:16:48 k8s-master kubelet[12711]: error: unable to load client CA file /etc/kubernetes/pki/ca.crt: open /etc/kubernetes/pki/ca.crt: no such file or directory
Oct 18 22:16:47 k8s-master kubelet[12711]: I1018 22:16:47.594044   12711 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:16:47 k8s-master kubelet[12711]: I1018 22:16:47.593991   12711 controller.go:114] kubelet config controller: starting controller
Oct 18 22:16:47 k8s-master kubelet[12711]: I1018 22:16:47.593350   12711 feature_gate.go:156] feature gates: map[]
Oct 18 22:16:47 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:16:47 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:16:37 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:16:37 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:16:37 k8s-master systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Oct 18 22:16:37 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:16:37 k8s-master systemd[1]: Stopping kubelet: The Kubernetes Node Agent...
Oct 18 22:16:35 k8s-master kubelet[11793]: E1018 22:16:35.712651   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:16:35 k8s-master kubelet[11793]: W1018 22:16:35.711749   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:16:30 k8s-master kubelet[11793]: E1018 22:16:30.710816   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:16:30 k8s-master kubelet[11793]: W1018 22:16:30.710632   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:16:25 k8s-master kubelet[11793]: E1018 22:16:25.709563   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:16:25 k8s-master kubelet[11793]: W1018 22:16:25.709439   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:16:20 k8s-master kubelet[11793]: E1018 22:16:20.708514   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:16:20 k8s-master kubelet[11793]: W1018 22:16:20.708316   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:16:15 k8s-master kubelet[11793]: E1018 22:16:15.705825   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:16:15 k8s-master kubelet[11793]: W1018 22:16:15.705707   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:16:10 k8s-master kubelet[11793]: E1018 22:16:10.704297   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:16:10 k8s-master kubelet[11793]: W1018 22:16:10.704169   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:16:05 k8s-master kubelet[11793]: E1018 22:16:05.701163   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:16:05 k8s-master kubelet[11793]: W1018 22:16:05.701034   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:16:00 k8s-master kubelet[11793]: E1018 22:16:00.700178   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:16:00 k8s-master kubelet[11793]: W1018 22:16:00.700053   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:15:55 k8s-master kubelet[11793]: E1018 22:15:55.699098   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:15:55 k8s-master kubelet[11793]: W1018 22:15:55.698969   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:15:50 k8s-master kubelet[11793]: E1018 22:15:50.697782   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:15:50 k8s-master kubelet[11793]: W1018 22:15:50.697537   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:15:45 k8s-master kubelet[11793]: E1018 22:15:45.695986   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:15:45 k8s-master kubelet[11793]: W1018 22:15:45.695856   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:15:40 k8s-master kubelet[11793]: E1018 22:15:40.694953   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:15:40 k8s-master kubelet[11793]: W1018 22:15:40.694822   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:15:35 k8s-master kubelet[11793]: E1018 22:15:35.693876   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:15:35 k8s-master kubelet[11793]: W1018 22:15:35.693677   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:15:30 k8s-master kubelet[11793]: E1018 22:15:30.692391   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:15:30 k8s-master kubelet[11793]: W1018 22:15:30.684195   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:15:25 k8s-master kubelet[11793]: E1018 22:15:25.682790   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:15:25 k8s-master kubelet[11793]: W1018 22:15:25.682660   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:15:20 k8s-master kubelet[11793]: E1018 22:15:20.681716   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:15:20 k8s-master kubelet[11793]: W1018 22:15:20.681452   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:15:15 k8s-master kubelet[11793]: E1018 22:15:15.679382   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:15:15 k8s-master kubelet[11793]: W1018 22:15:15.679260   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:15:10 k8s-master kubelet[11793]: E1018 22:15:10.678316   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:15:10 k8s-master kubelet[11793]: W1018 22:15:10.678194   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:15:05 k8s-master kubelet[11793]: E1018 22:15:05.677231   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:15:05 k8s-master kubelet[11793]: W1018 22:15:05.677085   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:15:00 k8s-master kubelet[11793]: E1018 22:15:00.675746   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:15:00 k8s-master kubelet[11793]: W1018 22:15:00.675574   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:14:55 k8s-master kubelet[11793]: E1018 22:14:55.673980   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:14:55 k8s-master kubelet[11793]: W1018 22:14:55.673783   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:14:50 k8s-master kubelet[11793]: E1018 22:14:50.670540   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:14:50 k8s-master kubelet[11793]: W1018 22:14:50.670370   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:14:45 k8s-master kubelet[11793]: E1018 22:14:45.668989   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:14:45 k8s-master kubelet[11793]: W1018 22:14:45.668813   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:14:40 k8s-master kubelet[11793]: E1018 22:14:40.667218   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:14:40 k8s-master kubelet[11793]: W1018 22:14:40.667080   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:14:35 k8s-master kubelet[11793]: E1018 22:14:35.665096   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:14:35 k8s-master kubelet[11793]: W1018 22:14:35.664936   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:14:30 k8s-master kubelet[11793]: E1018 22:14:30.663719   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:14:30 k8s-master kubelet[11793]: W1018 22:14:30.663588   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:14:25 k8s-master kubelet[11793]: E1018 22:14:25.662455   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:14:25 k8s-master kubelet[11793]: W1018 22:14:25.662335   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:14:20 k8s-master kubelet[11793]: E1018 22:14:20.660620   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:14:20 k8s-master kubelet[11793]: W1018 22:14:20.660434   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:14:15 k8s-master kubelet[11793]: E1018 22:14:15.659534   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:14:15 k8s-master kubelet[11793]: W1018 22:14:15.659408   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:14:10 k8s-master kubelet[11793]: E1018 22:14:10.658402   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:14:10 k8s-master kubelet[11793]: W1018 22:14:10.658212   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:14:05 k8s-master kubelet[11793]: E1018 22:14:05.656303   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:14:05 k8s-master kubelet[11793]: W1018 22:14:05.656148   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:14:00 k8s-master kubelet[11793]: E1018 22:14:00.654582   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:14:00 k8s-master kubelet[11793]: W1018 22:14:00.654465   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:13:55 k8s-master kubelet[11793]: E1018 22:13:55.653469   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:13:55 k8s-master kubelet[11793]: W1018 22:13:55.653274   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:13:50 k8s-master kubelet[11793]: E1018 22:13:50.652372   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:13:50 k8s-master kubelet[11793]: W1018 22:13:50.652122   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:13:45 k8s-master kubelet[11793]: E1018 22:13:45.651008   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:13:45 k8s-master kubelet[11793]: W1018 22:13:45.650884   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:13:40 k8s-master kubelet[11793]: E1018 22:13:40.645968   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:13:40 k8s-master kubelet[11793]: W1018 22:13:40.645377   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:13:35 k8s-master kubelet[11793]: E1018 22:13:35.644242   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:13:35 k8s-master kubelet[11793]: W1018 22:13:35.642352   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:13:30 k8s-master kubelet[11793]: E1018 22:13:30.627421   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:13:30 k8s-master kubelet[11793]: W1018 22:13:30.627269   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:13:25 k8s-master kubelet[11793]: E1018 22:13:25.626342   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:13:25 k8s-master kubelet[11793]: W1018 22:13:25.626187   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:13:20 k8s-master kubelet[11793]: E1018 22:13:20.625025   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:13:20 k8s-master kubelet[11793]: W1018 22:13:20.624931   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:13:15 k8s-master kubelet[11793]: E1018 22:13:15.622324   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:13:15 k8s-master kubelet[11793]: W1018 22:13:15.621752   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:13:10 k8s-master kubelet[11793]: E1018 22:13:10.619494   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:13:10 k8s-master kubelet[11793]: W1018 22:13:10.619240   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:13:05 k8s-master kubelet[11793]: E1018 22:13:05.617001   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:13:05 k8s-master kubelet[11793]: W1018 22:13:05.616840   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:13:00 k8s-master kubelet[11793]: E1018 22:13:00.615636   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:13:00 k8s-master kubelet[11793]: W1018 22:13:00.615504   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:12:55 k8s-master kubelet[11793]: E1018 22:12:55.614504   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:12:55 k8s-master kubelet[11793]: W1018 22:12:55.614330   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:12:50 k8s-master kubelet[11793]: E1018 22:12:50.613275   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:12:50 k8s-master kubelet[11793]: W1018 22:12:50.612340   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:12:45 k8s-master kubelet[11793]: E1018 22:12:45.610861   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:12:45 k8s-master kubelet[11793]: W1018 22:12:45.610579   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:12:40 k8s-master kubelet[11793]: E1018 22:12:40.609210   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:12:40 k8s-master kubelet[11793]: W1018 22:12:40.609074   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:12:35 k8s-master kubelet[11793]: E1018 22:12:35.608035   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:12:35 k8s-master kubelet[11793]: W1018 22:12:35.607916   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:12:30 k8s-master kubelet[11793]: E1018 22:12:30.606996   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:12:30 k8s-master kubelet[11793]: W1018 22:12:30.606892   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:12:25 k8s-master kubelet[11793]: E1018 22:12:25.605305   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:12:25 k8s-master kubelet[11793]: W1018 22:12:25.604886   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:12:20 k8s-master kubelet[11793]: E1018 22:12:20.602881   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:12:20 k8s-master kubelet[11793]: W1018 22:12:20.602734   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:12:15 k8s-master kubelet[11793]: E1018 22:12:15.601663   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:12:15 k8s-master kubelet[11793]: W1018 22:12:15.601493   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:12:10 k8s-master kubelet[11793]: E1018 22:12:10.600477   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:12:10 k8s-master kubelet[11793]: W1018 22:12:10.600153   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:12:05 k8s-master kubelet[11793]: E1018 22:12:05.598738   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:12:05 k8s-master kubelet[11793]: W1018 22:12:05.598570   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:12:00 k8s-master kubelet[11793]: E1018 22:12:00.597400   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:12:00 k8s-master kubelet[11793]: W1018 22:12:00.595466   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:11:55 k8s-master kubelet[11793]: E1018 22:11:55.593758   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:11:55 k8s-master kubelet[11793]: W1018 22:11:55.593621   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:11:50 k8s-master kubelet[11793]: E1018 22:11:50.592628   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:11:50 k8s-master kubelet[11793]: W1018 22:11:50.592507   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:11:45 k8s-master kubelet[11793]: E1018 22:11:45.577735   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:11:45 k8s-master kubelet[11793]: W1018 22:11:45.577586   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:11:40 k8s-master kubelet[11793]: E1018 22:11:40.576346   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:11:40 k8s-master kubelet[11793]: W1018 22:11:40.574779   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:11:35 k8s-master kubelet[11793]: E1018 22:11:35.558409   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:11:35 k8s-master kubelet[11793]: W1018 22:11:35.558228   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:11:30 k8s-master kubelet[11793]: E1018 22:11:30.556620   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:11:30 k8s-master kubelet[11793]: W1018 22:11:30.556227   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:11:25 k8s-master kubelet[11793]: E1018 22:11:25.539269   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:11:25 k8s-master kubelet[11793]: W1018 22:11:25.539087   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:11:20 k8s-master kubelet[11793]: E1018 22:11:20.538127   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:11:20 k8s-master kubelet[11793]: W1018 22:11:20.538007   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:11:15 k8s-master kubelet[11793]: E1018 22:11:15.526076   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:11:15 k8s-master kubelet[11793]: W1018 22:11:15.525895   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:11:10 k8s-master kubelet[11793]: E1018 22:11:10.522760   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:11:10 k8s-master kubelet[11793]: W1018 22:11:10.522613   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:11:05 k8s-master kubelet[11793]: E1018 22:11:05.508166   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:11:05 k8s-master kubelet[11793]: W1018 22:11:05.507965   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:11:00 k8s-master kubelet[11793]: E1018 22:11:00.506906   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:11:00 k8s-master kubelet[11793]: W1018 22:11:00.506773   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:10:55 k8s-master kubelet[11793]: E1018 22:10:55.491548   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:10:55 k8s-master kubelet[11793]: W1018 22:10:55.491405   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:10:50 k8s-master kubelet[11793]: E1018 22:10:50.490284   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:10:50 k8s-master kubelet[11793]: W1018 22:10:50.476982   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:10:45 k8s-master kubelet[11793]: E1018 22:10:45.473499   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:10:45 k8s-master kubelet[11793]: W1018 22:10:45.473376   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:10:40 k8s-master kubelet[11793]: E1018 22:10:40.472456   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:10:40 k8s-master kubelet[11793]: W1018 22:10:40.452774   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:10:35 k8s-master kubelet[11793]: E1018 22:10:35.449283   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:10:35 k8s-master kubelet[11793]: W1018 22:10:35.449138   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:10:35 k8s-master kubelet[11793]: E1018 22:10:35.141529   11793 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:10:30 k8s-master kubelet[11793]: E1018 22:10:30.448161   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:10:30 k8s-master kubelet[11793]: W1018 22:10:30.448036   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:10:25 k8s-master kubelet[11793]: E1018 22:10:25.431618   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:10:25 k8s-master kubelet[11793]: W1018 22:10:25.431422   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:10:20 k8s-master kubelet[11793]: E1018 22:10:20.889407   11793 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:10:20 k8s-master kubelet[11793]: E1018 22:10:20.430439   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:10:20 k8s-master kubelet[11793]: W1018 22:10:20.430245   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:10:15 k8s-master kubelet[11793]: E1018 22:10:15.398331   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:10:15 k8s-master kubelet[11793]: W1018 22:10:15.398148   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:10:10 k8s-master kubelet[11793]: E1018 22:10:10.397105   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:10:10 k8s-master kubelet[11793]: W1018 22:10:10.386557   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:10:05 k8s-master kubelet[11793]: E1018 22:10:05.383919   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:10:05 k8s-master kubelet[11793]: W1018 22:10:05.382168   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:10:00 k8s-master kubelet[11793]: E1018 22:10:00.380821   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:10:00 k8s-master kubelet[11793]: W1018 22:10:00.366103   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:09:55 k8s-master kubelet[11793]: E1018 22:09:55.363140   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:09:55 k8s-master kubelet[11793]: W1018 22:09:55.363015   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:09:50 k8s-master kubelet[11793]: E1018 22:09:50.362138   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:09:50 k8s-master kubelet[11793]: W1018 22:09:50.360876   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:09:45 k8s-master kubelet[11793]: E1018 22:09:45.342674   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:09:45 k8s-master kubelet[11793]: W1018 22:09:45.342523   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:09:40 k8s-master kubelet[11793]: E1018 22:09:40.341501   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:09:40 k8s-master kubelet[11793]: W1018 22:09:40.341365   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:09:35 k8s-master kubelet[11793]: E1018 22:09:35.324654   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:09:35 k8s-master kubelet[11793]: W1018 22:09:35.324503   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:09:30 k8s-master kubelet[11793]: E1018 22:09:30.323503   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:09:30 k8s-master kubelet[11793]: W1018 22:09:30.323364   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:09:25 k8s-master kubelet[11793]: E1018 22:09:25.302283   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:09:25 k8s-master kubelet[11793]: W1018 22:09:25.302120   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:09:20 k8s-master kubelet[11793]: E1018 22:09:20.301018   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:09:20 k8s-master kubelet[11793]: W1018 22:09:20.300891   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:09:15 k8s-master kubelet[11793]: E1018 22:09:15.288932   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:09:15 k8s-master kubelet[11793]: W1018 22:09:15.288798   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:09:10 k8s-master kubelet[11793]: E1018 22:09:10.287820   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:09:10 k8s-master kubelet[11793]: W1018 22:09:10.286857   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:09:05 k8s-master kubelet[11793]: E1018 22:09:05.259920   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:09:05 k8s-master kubelet[11793]: W1018 22:09:05.259697   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:09:00 k8s-master kubelet[11793]: E1018 22:09:00.258275   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:09:00 k8s-master kubelet[11793]: W1018 22:09:00.258016   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:08:55 k8s-master kubelet[11793]: E1018 22:08:55.244934   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:08:55 k8s-master kubelet[11793]: W1018 22:08:55.244479   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:08:50 k8s-master kubelet[11793]: E1018 22:08:50.241883   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:08:50 k8s-master kubelet[11793]: W1018 22:08:50.241724   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:08:48 k8s-master kubelet[11793]: E1018 22:08:48.164483   11793 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:08:47 k8s-master kubelet[11793]: E1018 22:08:47.868094   11793 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:08:46 k8s-master kubelet[11793]: E1018 22:08:46.590212   11793 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:08:46 k8s-master kubelet[11793]: E1018 22:08:46.407406   11793 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:08:45 k8s-master kubelet[11793]: E1018 22:08:45.772129   11793 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:08:45 k8s-master kubelet[11793]: E1018 22:08:45.223976   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:08:45 k8s-master kubelet[11793]: W1018 22:08:45.223811   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:08:40 k8s-master kubelet[11793]: E1018 22:08:40.220202   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:08:40 k8s-master kubelet[11793]: W1018 22:08:40.220044   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:08:38 k8s-master kubelet[11793]: E1018 22:08:38.710887   11793 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:08:35 k8s-master kubelet[11793]: E1018 22:08:35.190194   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:08:35 k8s-master kubelet[11793]: W1018 22:08:35.190061   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:08:32 k8s-master kubelet[11793]: E1018 22:08:32.482415   11793 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:08:30 k8s-master kubelet[11793]: E1018 22:08:30.188845   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:08:30 k8s-master kubelet[11793]: W1018 22:08:30.188706   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:08:29 k8s-master kubelet[11793]: E1018 22:08:29.137135   11793 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:08:28 k8s-master kubelet[11793]: E1018 22:08:28.656620   11793 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:08:26 k8s-master kubelet[11793]: E1018 22:08:26.651880   11793 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:08:25 k8s-master kubelet[11793]: E1018 22:08:25.183923   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:08:25 k8s-master kubelet[11793]: W1018 22:08:25.183798   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:08:23 k8s-master kubelet[11793]: E1018 22:08:23.691004   11793 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:08:23 k8s-master kubelet[11793]: E1018 22:08:23.209719   11793 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:08:20 k8s-master kubelet[11793]: E1018 22:08:20.968169   11793 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:08:20 k8s-master kubelet[11793]: E1018 22:08:20.182801   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:08:20 k8s-master kubelet[11793]: W1018 22:08:20.182587   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:08:19 k8s-master kubelet[11793]: E1018 22:08:19.817451   11793 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:08:15 k8s-master kubelet[11793]: E1018 22:08:15.145231   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:08:15 k8s-master kubelet[11793]: W1018 22:08:15.145081   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:08:13 k8s-master kubelet[11793]: E1018 22:08:13.358280   11793 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:08:13 k8s-master kubelet[11793]: E1018 22:08:13.052879   11793 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:08:12 k8s-master kubelet[11793]: E1018 22:08:12.513612   11793 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:08:10 k8s-master kubelet[11793]: E1018 22:08:10.143797   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:08:10 k8s-master kubelet[11793]: W1018 22:08:10.143641   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:08:09 k8s-master kubelet[11793]: E1018 22:08:09.976756   11793 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:08:09 k8s-master kubelet[11793]: I1018 22:08:09.798946   11793 kubelet_node_status.go:86] Successfully registered node k8s-master
Oct 18 22:08:09 k8s-master kubelet[11793]: I1018 22:08:09.793653   11793 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:08:09 k8s-master kubelet[11793]: I1018 22:08:09.790419   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:08:09 k8s-master kubelet[11793]: E1018 22:08:09.144306   11793 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:08:07 k8s-master kubelet[11793]: E1018 22:08:07.397135   11793 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:08:05 k8s-master kubelet[11793]: E1018 22:08:05.130523   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:08:05 k8s-master kubelet[11793]: W1018 22:08:05.130382   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:08:03 k8s-master kubelet[11793]: E1018 22:08:03.168170   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:08:02 k8s-master kubelet[11793]: E1018 22:08:02.968291   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:08:02 k8s-master kubelet[11793]: E1018 22:08:02.790096   11793 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:08:02 k8s-master kubelet[11793]: I1018 22:08:02.789748   11793 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:08:02 k8s-master kubelet[11793]: I1018 22:08:02.787658   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:08:02 k8s-master kubelet[11793]: E1018 22:08:02.349354   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:08:02 k8s-master kubelet[11793]: E1018 22:08:02.159298   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:08:01 k8s-master kubelet[11793]: E1018 22:08:01.960224   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:08:01 k8s-master kubelet[11793]: E1018 22:08:01.348782   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:08:01 k8s-master kubelet[11793]: E1018 22:08:01.148164   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:08:00 k8s-master kubelet[11793]: E1018 22:08:00.955830   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:08:00 k8s-master kubelet[11793]: E1018 22:08:00.346336   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:08:00 k8s-master kubelet[11793]: E1018 22:08:00.135978   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:08:00 k8s-master kubelet[11793]: E1018 22:08:00.129380   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:08:00 k8s-master kubelet[11793]: W1018 22:08:00.129248   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:08:00 k8s-master kubelet[11793]: E1018 22:08:00.127871   11793 eviction_manager.go:238] eviction manager: unexpected err: failed to get node info: node 'k8s-master' not found
Oct 18 22:07:59 k8s-master kubelet[11793]: E1018 22:07:59.940530   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:59 k8s-master kubelet[11793]: E1018 22:07:59.735922   11793 kubelet.go:1612] Failed creating a mirror pod for "kube-scheduler-k8s-master_kube-system(5c60da2dd3a2204955600c1da11b55d9)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:59 k8s-master kubelet[11793]: E1018 22:07:59.582245   11793 kubelet.go:1612] Failed creating a mirror pod for "kube-apiserver-k8s-master_kube-system(5b22de7c48152d8b4288073d04aa4eb5)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:59 k8s-master kubelet[11793]: E1018 22:07:59.582111   11793 kubelet.go:1612] Failed creating a mirror pod for "etcd-k8s-master_kube-system(01ab851eda3eb6677bfd60d57c891841)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:59 k8s-master kubelet[11793]: E1018 22:07:59.577625   11793 kubelet.go:1612] Failed creating a mirror pod for "kube-controller-manager-k8s-master_kube-system(2da19bbcab5163c8a4e67bc7a65f9537)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:59 k8s-master kubelet[11793]: I1018 22:07:59.571516   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:59 k8s-master kubelet[11793]: I1018 22:07:59.571112   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:59 k8s-master kubelet[11793]: I1018 22:07:59.570923   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:59 k8s-master kubelet[11793]: I1018 22:07:59.570564   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:58 k8s-master kubelet[11793]: E1018 22:07:58.946382   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:58 k8s-master kubelet[11793]: E1018 22:07:58.748182   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:58 k8s-master kubelet[11793]: E1018 22:07:58.698613   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:58 k8s-master kubelet[11793]: E1018 22:07:58.561805   11793 kubelet.go:1612] Failed creating a mirror pod for "kube-controller-manager-k8s-master_kube-system(2da19bbcab5163c8a4e67bc7a65f9537)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:58 k8s-master kubelet[11793]: W1018 22:07:58.561681   11793 status_manager.go:431] Failed to get status for pod "kube-controller-manager-k8s-master_kube-system(2da19bbcab5163c8a4e67bc7a65f9537)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:58 k8s-master kubelet[11793]: I1018 22:07:58.557582   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:58 k8s-master kubelet[11793]: E1018 22:07:58.552529   11793 kubelet.go:1612] Failed creating a mirror pod for "etcd-k8s-master_kube-system(01ab851eda3eb6677bfd60d57c891841)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:58 k8s-master kubelet[11793]: W1018 22:07:58.552457   11793 status_manager.go:431] Failed to get status for pod "etcd-k8s-master_kube-system(01ab851eda3eb6677bfd60d57c891841)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/etcd-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:58 k8s-master kubelet[11793]: I1018 22:07:58.534250   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:58 k8s-master kubelet[11793]: E1018 22:07:58.529664   11793 kubelet.go:1612] Failed creating a mirror pod for "kube-apiserver-k8s-master_kube-system(5b22de7c48152d8b4288073d04aa4eb5)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:58 k8s-master kubelet[11793]: W1018 22:07:58.529585   11793 status_manager.go:431] Failed to get status for pod "kube-apiserver-k8s-master_kube-system(5b22de7c48152d8b4288073d04aa4eb5)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:58 k8s-master kubelet[11793]: I1018 22:07:58.510591   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:58 k8s-master kubelet[11793]: E1018 22:07:58.493450   11793 kubelet.go:1612] Failed creating a mirror pod for "kube-scheduler-k8s-master_kube-system(5c60da2dd3a2204955600c1da11b55d9)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:58 k8s-master kubelet[11793]: W1018 22:07:58.493381   11793 status_manager.go:431] Failed to get status for pod "kube-scheduler-k8s-master_kube-system(5c60da2dd3a2204955600c1da11b55d9)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:58 k8s-master kubelet[11793]: I1018 22:07:58.487419   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:57 k8s-master kubelet[11793]: E1018 22:07:57.721170   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:57 k8s-master kubelet[11793]: E1018 22:07:57.716668   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:57 k8s-master kubelet[11793]: E1018 22:07:57.696071   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:57 k8s-master kubelet[11793]: W1018 22:07:57.484964   11793 pod_container_deletor.go:77] Container "853ce201d531a0ceb6e36a1bb71e100b4aa6385ba2ae886e9ef9c43c1c225219" not found in pod's containers
Oct 18 22:07:57 k8s-master kubelet[11793]: E1018 22:07:57.461151   11793 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:07:57 k8s-master kubelet[11793]: W1018 22:07:57.461019   11793 container.go:354] Failed to create summary reader for "/libcontainer_12059_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:07:57 k8s-master kubelet[11793]: I1018 22:07:57.458909   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:56 k8s-master kubelet[11793]: W1018 22:07:56.986062   11793 pod_container_deletor.go:77] Container "09999614aca013d91b3a853710a3863605d283d406f0a1bf54a3aa9a034c29f7" not found in pod's containers
Oct 18 22:07:56 k8s-master kubelet[11793]: I1018 22:07:56.979706   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:56 k8s-master kubelet[11793]: W1018 22:07:56.933210   11793 docker_container.go:202] Deleted previously existing symlink file: "/var/log/pods/5c60da2dd3a2204955600c1da11b55d9/kube-scheduler_0.log"
Oct 18 22:07:56 k8s-master kubelet[11793]: E1018 22:07:56.923133   11793 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/default/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:07:56 k8s-master kubelet[11793]: E1018 22:07:56.718922   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:56 k8s-master kubelet[11793]: E1018 22:07:56.708839   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:56 k8s-master kubelet[11793]: E1018 22:07:56.688426   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:56 k8s-master kubelet[11793]: W1018 22:07:56.450738   11793 pod_container_deletor.go:77] Container "8b4321730578f951b1b2826cc9e4c3dcb113ba78341fe2d3b56824ffdda10d93" not found in pod's containers
Oct 18 22:07:56 k8s-master kubelet[11793]: E1018 22:07:56.449407   11793 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:07:56 k8s-master kubelet[11793]: W1018 22:07:56.444387   11793 container.go:354] Failed to create summary reader for "/libcontainer_11943_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:07:56 k8s-master kubelet[11793]: W1018 22:07:56.444261   11793 container.go:354] Failed to create summary reader for "/libcontainer_11926_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:07:56 k8s-master kubelet[11793]: I1018 22:07:56.390147   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:56 k8s-master kubelet[11793]: W1018 22:07:56.389996   11793 pod_container_deletor.go:77] Container "fdda8693171221dd9c2324d27cae4d6c602be12b962a8cd86617454a3e6a3bf5" not found in pod's containers
Oct 18 22:07:56 k8s-master kubelet[11793]: E1018 22:07:56.382729   11793 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:56 k8s-master kubelet[11793]: I1018 22:07:56.380441   11793 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:07:56 k8s-master kubelet[11793]: I1018 22:07:56.340291   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:56 k8s-master kubelet[11793]: I1018 22:07:56.339215   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:56 k8s-master kubelet[11793]: W1018 22:07:56.275926   11793 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_11943_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_11943_systemd_test_default.slice: no such file or directory
Oct 18 22:07:55 k8s-master kubelet[11793]: E1018 22:07:55.717111   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:55 k8s-master kubelet[11793]: E1018 22:07:55.706885   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:55 k8s-master kubelet[11793]: E1018 22:07:55.680729   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:55 k8s-master kubelet[11793]: E1018 22:07:55.120330   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:07:55 k8s-master kubelet[11793]: W1018 22:07:55.120178   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:07:55 k8s-master kubelet[11793]: I1018 22:07:55.005325   11793 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs-etc-pki" (UniqueName: "kubernetes.io/host-path/5b22de7c48152d8b4288073d04aa4eb5-ca-certs-etc-pki") pod "kube-apiserver-k8s-master" (UID: "5b22de7c48152d8b4288073d04aa4eb5")
Oct 18 22:07:55 k8s-master kubelet[11793]: I1018 22:07:55.005298   11793 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs" (UniqueName: "kubernetes.io/host-path/5b22de7c48152d8b4288073d04aa4eb5-ca-certs") pod "kube-apiserver-k8s-master" (UID: "5b22de7c48152d8b4288073d04aa4eb5")
Oct 18 22:07:55 k8s-master kubelet[11793]: I1018 22:07:55.005250   11793 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "k8s-certs" (UniqueName: "kubernetes.io/host-path/5b22de7c48152d8b4288073d04aa4eb5-k8s-certs") pod "kube-apiserver-k8s-master" (UID: "5b22de7c48152d8b4288073d04aa4eb5")
Oct 18 22:07:54 k8s-master kubelet[11793]: E1018 22:07:54.931071   11793 kubelet.go:1612] Failed creating a mirror pod for "kube-apiserver-k8s-master_kube-system(5b22de7c48152d8b4288073d04aa4eb5)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:54 k8s-master kubelet[11793]: I1018 22:07:54.913067   11793 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs-etc-pki" (UniqueName: "kubernetes.io/host-path/2da19bbcab5163c8a4e67bc7a65f9537-ca-certs-etc-pki") pod "kube-controller-manager-k8s-master" (UID: "2da19bbcab5163c8a4e67bc7a65f9537")
Oct 18 22:07:54 k8s-master kubelet[11793]: I1018 22:07:54.913026   11793 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "flexvolume-dir" (UniqueName: "kubernetes.io/host-path/2da19bbcab5163c8a4e67bc7a65f9537-flexvolume-dir") pod "kube-controller-manager-k8s-master" (UID: "2da19bbcab5163c8a4e67bc7a65f9537")
Oct 18 22:07:54 k8s-master kubelet[11793]: I1018 22:07:54.912996   11793 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/2da19bbcab5163c8a4e67bc7a65f9537-kubeconfig") pod "kube-controller-manager-k8s-master" (UID: "2da19bbcab5163c8a4e67bc7a65f9537")
Oct 18 22:07:54 k8s-master kubelet[11793]: I1018 22:07:54.912962   11793 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs" (UniqueName: "kubernetes.io/host-path/2da19bbcab5163c8a4e67bc7a65f9537-ca-certs") pod "kube-controller-manager-k8s-master" (UID: "2da19bbcab5163c8a4e67bc7a65f9537")
Oct 18 22:07:54 k8s-master kubelet[11793]: I1018 22:07:54.912937   11793 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "k8s-certs" (UniqueName: "kubernetes.io/host-path/2da19bbcab5163c8a4e67bc7a65f9537-k8s-certs") pod "kube-controller-manager-k8s-master" (UID: "2da19bbcab5163c8a4e67bc7a65f9537")
Oct 18 22:07:54 k8s-master kubelet[11793]: I1018 22:07:54.912907   11793 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "etcd" (UniqueName: "kubernetes.io/host-path/01ab851eda3eb6677bfd60d57c891841-etcd") pod "etcd-k8s-master" (UID: "01ab851eda3eb6677bfd60d57c891841")
Oct 18 22:07:54 k8s-master kubelet[11793]: I1018 22:07:54.912863   11793 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/5c60da2dd3a2204955600c1da11b55d9-kubeconfig") pod "kube-scheduler-k8s-master" (UID: "5c60da2dd3a2204955600c1da11b55d9")
Oct 18 22:07:54 k8s-master kubelet[11793]: E1018 22:07:54.904266   11793 kubelet.go:1612] Failed creating a mirror pod for "etcd-k8s-master_kube-system(01ab851eda3eb6677bfd60d57c891841)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:54 k8s-master kubelet[11793]: E1018 22:07:54.890181   11793 kubelet.go:1612] Failed creating a mirror pod for "kube-controller-manager-k8s-master_kube-system(2da19bbcab5163c8a4e67bc7a65f9537)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:54 k8s-master kubelet[11793]: W1018 22:07:54.845123   11793 status_manager.go:431] Failed to get status for pod "kube-apiserver-k8s-master_kube-system(5b22de7c48152d8b4288073d04aa4eb5)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:54 k8s-master kubelet[11793]: W1018 22:07:54.841327   11793 status_manager.go:431] Failed to get status for pod "etcd-k8s-master_kube-system(01ab851eda3eb6677bfd60d57c891841)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/etcd-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:54 k8s-master kubelet[11793]: I1018 22:07:54.840906   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:54 k8s-master kubelet[11793]: E1018 22:07:54.829032   11793 kubelet.go:1612] Failed creating a mirror pod for "kube-scheduler-k8s-master_kube-system(5c60da2dd3a2204955600c1da11b55d9)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:54 k8s-master kubelet[11793]: W1018 22:07:54.828960   11793 status_manager.go:431] Failed to get status for pod "kube-scheduler-k8s-master_kube-system(5c60da2dd3a2204955600c1da11b55d9)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:54 k8s-master kubelet[11793]: I1018 22:07:54.828453   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:54 k8s-master kubelet[11793]: I1018 22:07:54.824331   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:54 k8s-master kubelet[11793]: I1018 22:07:54.819407   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:54 k8s-master kubelet[11793]: I1018 22:07:54.818391   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:54 k8s-master kubelet[11793]: W1018 22:07:54.817139   11793 status_manager.go:431] Failed to get status for pod "kube-controller-manager-k8s-master_kube-system(2da19bbcab5163c8a4e67bc7a65f9537)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:54 k8s-master kubelet[11793]: I1018 22:07:54.811815   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:54 k8s-master kubelet[11793]: I1018 22:07:54.811444   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:54 k8s-master kubelet[11793]: I1018 22:07:54.807322   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:54 k8s-master kubelet[11793]: E1018 22:07:54.716413   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:54 k8s-master kubelet[11793]: E1018 22:07:54.701082   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:54 k8s-master kubelet[11793]: E1018 22:07:54.677577   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:53 k8s-master kubelet[11793]: E1018 22:07:53.715760   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:53 k8s-master kubelet[11793]: E1018 22:07:53.700409   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:53 k8s-master kubelet[11793]: E1018 22:07:53.676421   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:53 k8s-master kubelet[11793]: E1018 22:07:53.136078   11793 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:53 k8s-master kubelet[11793]: I1018 22:07:53.134341   11793 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:07:53 k8s-master kubelet[11793]: I1018 22:07:53.132256   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:52 k8s-master kubelet[11793]: E1018 22:07:52.706763   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:52 k8s-master kubelet[11793]: E1018 22:07:52.694136   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:52 k8s-master kubelet[11793]: E1018 22:07:52.675693   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:51 k8s-master kubelet[11793]: E1018 22:07:51.705969   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:51 k8s-master kubelet[11793]: E1018 22:07:51.693401   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:51 k8s-master kubelet[11793]: E1018 22:07:51.675098   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:51 k8s-master kubelet[11793]: E1018 22:07:51.531996   11793 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:51 k8s-master kubelet[11793]: I1018 22:07:51.531636   11793 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:07:51 k8s-master kubelet[11793]: I1018 22:07:51.529300   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:50 k8s-master kubelet[11793]: E1018 22:07:50.729061   11793 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:50 k8s-master kubelet[11793]: I1018 22:07:50.728760   11793 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:07:50 k8s-master kubelet[11793]: I1018 22:07:50.726690   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:50 k8s-master kubelet[11793]: E1018 22:07:50.704238   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:50 k8s-master kubelet[11793]: E1018 22:07:50.690466   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:50 k8s-master kubelet[11793]: E1018 22:07:50.674290   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:50 k8s-master kubelet[11793]: E1018 22:07:50.326499   11793 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:50 k8s-master kubelet[11793]: I1018 22:07:50.326114   11793 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:07:50 k8s-master kubelet[11793]: I1018 22:07:50.321552   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:50 k8s-master kubelet[11793]: E1018 22:07:50.120741   11793 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:50 k8s-master kubelet[11793]: I1018 22:07:50.120301   11793 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:07:50 k8s-master kubelet[11793]: E1018 22:07:50.119339   11793 eviction_manager.go:238] eviction manager: unexpected err: failed to get node info: node 'k8s-master' not found
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.979379   11793 manager.go:316] Recovery completed
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.907709   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.870238   11793 manager.go:311] Starting recovery of all containers
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.866413   11793 manager.go:1140] Started watching for new ooms in manager
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.866255   11793 factory.go:86] Registering Raw factory
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.866120   11793 factory.go:54] Registering systemd factory
Oct 18 22:07:49 k8s-master kubelet[11793]: W1018 22:07:49.866108   11793 manager.go:276] Registration of the crio container factory failed: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:07:49 k8s-master kubelet[11793]: W1018 22:07:49.865966   11793 manager.go:265] Registration of the rkt container factory failed: unable to communicate with Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.865926   11793 factory.go:355] Registering Docker factory
Oct 18 22:07:49 k8s-master kubelet[11793]: E1018 22:07:49.841460   11793 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:07:49 k8s-master kubelet[11793]: W1018 22:07:49.828080   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.804787   11793 volume_manager.go:246] Starting Kubelet Volume Manager
Oct 18 22:07:49 k8s-master kubelet[11793]: E1018 22:07:49.804762   11793 container_manager_linux.go:603] [ContainerManager]: Fail to get rootfs information unable to find data for container /
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.803903   11793 kubelet.go:1779] skipping pod synchronization - [container runtime is down PLEG is not healthy: pleg was last seen active 2562047h47m16.854775807s ago; threshold is 3m0s]
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.803887   11793 kubelet.go:1768] Starting kubelet main sync loop.
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.803869   11793 status_manager.go:140] Starting to sync pod status with apiserver
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.803836   11793 fs_resource_analyzer.go:66] Starting FS ResourceAnalyzer
Oct 18 22:07:49 k8s-master kubelet[11793]: E1018 22:07:49.785607   11793 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/default/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.780661   11793 server.go:296] Adding debug handlers to kubelet server.
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.779668   11793 server.go:128] Starting to listen on 0.0.0.0:10250
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.779304   11793 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:49 k8s-master kubelet[11793]: E1018 22:07:49.777577   11793 kubelet.go:1234] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data for container /
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.776749   11793 server.go:718] Started kubelet v1.8.1
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.770829   11793 kuberuntime_manager.go:177] Container runtime docker initialized, version: 17.10.0-ce, apiVersion: 1.33.0
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.766466   11793 remote_runtime.go:43] Connecting to runtime service unix:///var/run/dockershim.sock
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.740980   11793 docker_service.go:224] Setting cgroupDriver to systemd
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.717832   11793 docker_service.go:207] Docker cri networking managed by cni
Oct 18 22:07:49 k8s-master kubelet[11793]: W1018 22:07:49.717803   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:07:49 k8s-master kubelet[11793]: W1018 22:07:49.704890   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:07:49 k8s-master kubelet[11793]: W1018 22:07:49.702957   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.702756   11793 kubelet.go:517] Hairpin mode set to "hairpin-veth"
Oct 18 22:07:49 k8s-master kubelet[11793]: W1018 22:07:49.702728   11793 kubelet_network.go:69] Hairpin mode set to "promiscuous-bridge" but kubenet is not enabled, falling back to "hairpin-veth"
Oct 18 22:07:49 k8s-master kubelet[11793]: E1018 22:07:49.673689   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:49 k8s-master kubelet[11793]: E1018 22:07:49.673651   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:49 k8s-master kubelet[11793]: E1018 22:07:49.673591   11793 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.608851   11793 kubelet.go:283] Watching apiserver
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.608829   11793 kubelet.go:273] Adding manifest file: /etc/kubernetes/manifests
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.608715   11793 container_manager_linux.go:288] Creating device plugin handler: false
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.608584   11793 container_manager_linux.go:257] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: ContainerRuntime:docker CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[{Signal:memory.available Operator:LessThan Value:{Quantity:100Mi Percentage:0} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.1} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.inodesFree Operator:LessThan Value:{Quantity:<nil> Percentage:0.05} GracePeriod:0s MinReclaim:<nil>}]} ExperimentalQOSReserved:map[] ExperimentalCPUManagerPolicy:none ExperimentalCPUManagerReconcilePeriod:10s}
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.608547   11793 container_manager_linux.go:252] container manager verified user specified cgroup-root exists: /
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.599795   11793 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.599257   11793 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.597319   11793 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.594821   11793 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0}]
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.594790   11793 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 22:07:49 k8s-master kubelet[11793]: W1018 22:07:49.577868   11793 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:07:49 k8s-master kubelet[11793]: W1018 22:07:49.577743   11793 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.553243   11793 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:07:49 k8s-master kubelet[11793]: E1018 22:07:49.552575   11793 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.551454   11793 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:07:49 k8s-master kubelet[11793]: W1018 22:07:49.524329   11793 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.524153   11793 feature_gate.go:156] feature gates: map[]
Oct 18 22:07:49 k8s-master kubelet[11793]: W1018 22:07:49.513556   11793 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.511189   11793 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.511125   11793 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.492320   11793 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.492313   11793 controller.go:114] kubelet config controller: starting controller
Oct 18 22:07:49 k8s-master kubelet[11793]: I1018 22:07:49.492057   11793 feature_gate.go:156] feature gates: map[]
Oct 18 22:07:49 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:07:49 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:07:49 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:07:39 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:07:39 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:07:39 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:07:39 k8s-master kubelet[11781]: error: failed to run Kubelet: unable to load bootstrap kubeconfig: stat /etc/kubernetes/bootstrap-kubelet.conf: no such file or directory
Oct 18 22:07:39 k8s-master kubelet[11781]: W1018 22:07:39.094665   11781 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:07:39 k8s-master kubelet[11781]: I1018 22:07:39.094495   11781 feature_gate.go:156] feature gates: map[]
Oct 18 22:07:39 k8s-master kubelet[11781]: W1018 22:07:39.060487   11781 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:07:39 k8s-master kubelet[11781]: I1018 22:07:39.058196   11781 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:07:39 k8s-master kubelet[11781]: I1018 22:07:39.058150   11781 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:07:38 k8s-master kubelet[11781]: I1018 22:07:38.599359   11781 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:07:38 k8s-master kubelet[11781]: I1018 22:07:38.599354   11781 controller.go:114] kubelet config controller: starting controller
Oct 18 22:07:38 k8s-master kubelet[11781]: I1018 22:07:38.599166   11781 feature_gate.go:156] feature gates: map[]
Oct 18 22:07:38 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:07:38 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:07:30 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:07:30 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:07:30 k8s-master systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Oct 18 22:07:30 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:07:30 k8s-master systemd[1]: Stopping kubelet: The Kubernetes Node Agent...
Oct 18 22:07:30 k8s-master kubelet[11279]: E1018 22:07:30.568064   11279 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:07:30 k8s-master kubelet[11279]: W1018 22:07:30.567929   11279 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:07:26 k8s-master kubelet[11279]: E1018 22:07:26.337511   11279 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:07:25 k8s-master kubelet[11279]: E1018 22:07:25.566707   11279 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:07:25 k8s-master kubelet[11279]: W1018 22:07:25.566606   11279 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:07:25 k8s-master kubelet[11279]: E1018 22:07:25.195922   11279 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:07:21 k8s-master kubelet[11279]: E1018 22:07:21.836445   11279 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:07:20 k8s-master kubelet[11279]: E1018 22:07:20.777696   11279 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:07:20 k8s-master kubelet[11279]: E1018 22:07:20.531645   11279 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:07:20 k8s-master kubelet[11279]: W1018 22:07:20.531517   11279 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:07:17 k8s-master kubelet[11279]: E1018 22:07:17.270442   11279 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:07:15 k8s-master kubelet[11279]: E1018 22:07:15.912405   11279 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:07:15 k8s-master kubelet[11279]: E1018 22:07:15.530475   11279 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:07:15 k8s-master kubelet[11279]: W1018 22:07:15.530323   11279 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:07:10 k8s-master kubelet[11279]: E1018 22:07:10.825300   11279 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:07:10 k8s-master kubelet[11279]: E1018 22:07:10.517108   11279 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:07:10 k8s-master kubelet[11279]: W1018 22:07:10.516901   11279 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:07:05 k8s-master kubelet[11279]: E1018 22:07:05.514717   11279 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:07:05 k8s-master kubelet[11279]: W1018 22:07:05.514572   11279 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:07:02 k8s-master kubelet[11279]: E1018 22:07:02.362619   11279 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:07:02 k8s-master kubelet[11279]: E1018 22:07:02.160459   11279 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:07:02 k8s-master kubelet[11279]: I1018 22:07:02.135774   11279 kubelet_node_status.go:86] Successfully registered node k8s-master
Oct 18 22:07:02 k8s-master kubelet[11279]: I1018 22:07:02.131134   11279 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:07:02 k8s-master kubelet[11279]: I1018 22:07:02.128399   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:07:00 k8s-master kubelet[11279]: E1018 22:07:00.510408   11279 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:07:00 k8s-master kubelet[11279]: W1018 22:07:00.510210   11279 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:06:57 k8s-master kubelet[11279]: E1018 22:06:57.540022   11279 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:06:57 k8s-master kubelet[11279]: E1018 22:06:57.471724   11279 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:06:57 k8s-master kubelet[11279]: E1018 22:06:57.362273   11279 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:06:56 k8s-master kubelet[11279]: E1018 22:06:56.489381   11279 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:06:56 k8s-master kubelet[11279]: E1018 22:06:56.414370   11279 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:06:55 k8s-master kubelet[11279]: E1018 22:06:55.775638   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:55 k8s-master kubelet[11279]: E1018 22:06:55.757076   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:55 k8s-master kubelet[11279]: E1018 22:06:55.715681   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:55 k8s-master kubelet[11279]: E1018 22:06:55.508568   11279 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:06:55 k8s-master kubelet[11279]: W1018 22:06:55.508356   11279 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:06:55 k8s-master kubelet[11279]: E1018 22:06:55.467545   11279 eviction_manager.go:238] eviction manager: unexpected err: failed to get node info: node 'k8s-master' not found
Oct 18 22:06:55 k8s-master kubelet[11279]: E1018 22:06:55.128177   11279 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:55 k8s-master kubelet[11279]: I1018 22:06:55.127308   11279 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:06:55 k8s-master kubelet[11279]: I1018 22:06:55.124788   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:54 k8s-master kubelet[11279]: E1018 22:06:54.768772   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:54 k8s-master kubelet[11279]: E1018 22:06:54.750598   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:54 k8s-master kubelet[11279]: E1018 22:06:54.708093   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:53 k8s-master kubelet[11279]: E1018 22:06:53.790090   11279 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/default/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:06:53 k8s-master kubelet[11279]: E1018 22:06:53.748077   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:53 k8s-master kubelet[11279]: E1018 22:06:53.748077   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:53 k8s-master kubelet[11279]: E1018 22:06:53.699101   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:52 k8s-master kubelet[11279]: E1018 22:06:52.744229   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:52 k8s-master kubelet[11279]: E1018 22:06:52.744165   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:52 k8s-master kubelet[11279]: E1018 22:06:52.696909   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:51 k8s-master kubelet[11279]: E1018 22:06:51.729706   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:51 k8s-master kubelet[11279]: E1018 22:06:51.728262   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:51 k8s-master kubelet[11279]: E1018 22:06:51.688299   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:50 k8s-master kubelet[11279]: E1018 22:06:50.728377   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:50 k8s-master kubelet[11279]: E1018 22:06:50.707919   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:50 k8s-master kubelet[11279]: E1018 22:06:50.675651   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:50 k8s-master kubelet[11279]: E1018 22:06:50.493732   11279 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:06:50 k8s-master kubelet[11279]: W1018 22:06:50.493390   11279 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:06:49 k8s-master kubelet[11279]: E1018 22:06:49.726890   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:49 k8s-master kubelet[11279]: E1018 22:06:49.702942   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:49 k8s-master kubelet[11279]: E1018 22:06:49.669291   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:48 k8s-master kubelet[11279]: E1018 22:06:48.726237   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:48 k8s-master kubelet[11279]: E1018 22:06:48.702297   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:48 k8s-master kubelet[11279]: E1018 22:06:48.668569   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:48 k8s-master kubelet[11279]: E1018 22:06:48.109233   11279 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:48 k8s-master kubelet[11279]: I1018 22:06:48.108884   11279 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:06:48 k8s-master kubelet[11279]: I1018 22:06:48.106102   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:47 k8s-master kubelet[11279]: E1018 22:06:47.725586   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:47 k8s-master kubelet[11279]: E1018 22:06:47.699900   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:47 k8s-master kubelet[11279]: E1018 22:06:47.666951   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:46 k8s-master kubelet[11279]: E1018 22:06:46.724242   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:46 k8s-master kubelet[11279]: E1018 22:06:46.699290   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:46 k8s-master kubelet[11279]: E1018 22:06:46.666336   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:46 k8s-master kubelet[11279]: E1018 22:06:46.317937   11279 kubelet.go:1612] Failed creating a mirror pod for "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:46 k8s-master kubelet[11279]: E1018 22:06:46.317824   11279 kubelet.go:1612] Failed creating a mirror pod for "kube-scheduler-k8s-master_kube-system(5c60da2dd3a2204955600c1da11b55d9)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:46 k8s-master kubelet[11279]: E1018 22:06:46.314429   11279 kubelet.go:1612] Failed creating a mirror pod for "kube-controller-manager-k8s-master_kube-system(f36e1d93ccf23a215dceaf6196109d5e)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:46 k8s-master kubelet[11279]: E1018 22:06:46.314335   11279 kubelet.go:1612] Failed creating a mirror pod for "kube-apiserver-k8s-master_kube-system(46d49698141c16a22a3a11256b78f37e)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:46 k8s-master kubelet[11279]: I1018 22:06:46.308332   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:46 k8s-master kubelet[11279]: I1018 22:06:46.307233   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:46 k8s-master kubelet[11279]: I1018 22:06:46.307011   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:46 k8s-master kubelet[11279]: I1018 22:06:46.306691   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:45 k8s-master kubelet[11279]: E1018 22:06:45.723566   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:45 k8s-master kubelet[11279]: E1018 22:06:45.698575   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:45 k8s-master kubelet[11279]: E1018 22:06:45.665726   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:45 k8s-master kubelet[11279]: E1018 22:06:45.489372   11279 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:06:45 k8s-master kubelet[11279]: W1018 22:06:45.489242   11279 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:06:45 k8s-master kubelet[11279]: E1018 22:06:45.463836   11279 eviction_manager.go:238] eviction manager: unexpected err: failed to get node info: node 'k8s-master' not found
Oct 18 22:06:45 k8s-master kubelet[11279]: E1018 22:06:45.306101   11279 kubelet.go:1612] Failed creating a mirror pod for "kube-controller-manager-k8s-master_kube-system(f36e1d93ccf23a215dceaf6196109d5e)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:45 k8s-master kubelet[11279]: W1018 22:06:45.306040   11279 status_manager.go:431] Failed to get status for pod "kube-controller-manager-k8s-master_kube-system(f36e1d93ccf23a215dceaf6196109d5e)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:45 k8s-master kubelet[11279]: I1018 22:06:45.303104   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:45 k8s-master kubelet[11279]: E1018 22:06:45.298975   11279 kubelet.go:1612] Failed creating a mirror pod for "kube-scheduler-k8s-master_kube-system(5c60da2dd3a2204955600c1da11b55d9)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:45 k8s-master kubelet[11279]: W1018 22:06:45.298896   11279 status_manager.go:431] Failed to get status for pod "kube-scheduler-k8s-master_kube-system(5c60da2dd3a2204955600c1da11b55d9)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:45 k8s-master kubelet[11279]: I1018 22:06:45.282062   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:45 k8s-master kubelet[11279]: E1018 22:06:45.279224   11279 kubelet.go:1612] Failed creating a mirror pod for "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:45 k8s-master kubelet[11279]: W1018 22:06:45.279162   11279 status_manager.go:431] Failed to get status for pod "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/etcd-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:45 k8s-master kubelet[11279]: I1018 22:06:45.269977   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:45 k8s-master kubelet[11279]: E1018 22:06:45.265905   11279 kubelet.go:1612] Failed creating a mirror pod for "kube-apiserver-k8s-master_kube-system(46d49698141c16a22a3a11256b78f37e)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:45 k8s-master kubelet[11279]: W1018 22:06:45.265845   11279 status_manager.go:431] Failed to get status for pod "kube-apiserver-k8s-master_kube-system(46d49698141c16a22a3a11256b78f37e)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:45 k8s-master kubelet[11279]: I1018 22:06:45.246025   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:44 k8s-master kubelet[11279]: E1018 22:06:44.722867   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:44 k8s-master kubelet[11279]: E1018 22:06:44.697793   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:44 k8s-master kubelet[11279]: E1018 22:06:44.664639   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:44 k8s-master kubelet[11279]: E1018 22:06:44.536843   11279 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:06:44 k8s-master kubelet[11279]: W1018 22:06:44.536721   11279 container.go:354] Failed to create summary reader for "/libcontainer_11583_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:06:44 k8s-master kubelet[11279]: W1018 22:06:44.254177   11279 pod_container_deletor.go:77] Container "189a2914637c1ad38a042fc1880796c9c1d8171995fbc01b153baf64496a4bfd" not found in pod's containers
Oct 18 22:06:44 k8s-master kubelet[11279]: E1018 22:06:44.248693   11279 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:06:44 k8s-master kubelet[11279]: W1018 22:06:44.236300   11279 container.go:354] Failed to create summary reader for "/libcontainer_11560_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:06:44 k8s-master kubelet[11279]: I1018 22:06:44.233350   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:44 k8s-master kubelet[11279]: W1018 22:06:44.099805   11279 pod_container_deletor.go:77] Container "7962cd167bf17a13508ed2b98373c9bf0551ab7d1eb9ecc6cd6c9d6592e03500" not found in pod's containers
Oct 18 22:06:44 k8s-master kubelet[11279]: I1018 22:06:44.075816   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:43 k8s-master kubelet[11279]: E1018 22:06:43.776163   11279 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/default/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:06:43 k8s-master kubelet[11279]: E1018 22:06:43.720961   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:43 k8s-master kubelet[11279]: E1018 22:06:43.688351   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:43 k8s-master kubelet[11279]: E1018 22:06:43.663859   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:43 k8s-master kubelet[11279]: W1018 22:06:43.427352   11279 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_11560_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_11560_systemd_test_default.slice: no such file or directory
Oct 18 22:06:43 k8s-master kubelet[11279]: W1018 22:06:43.124938   11279 pod_container_deletor.go:77] Container "efee5de5fa2560e0e97249c7bc9030d8ee199a919357e27a8f7a214056983592" not found in pod's containers
Oct 18 22:06:43 k8s-master kubelet[11279]: I1018 22:06:43.098538   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:42 k8s-master kubelet[11279]: E1018 22:06:42.717905   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:42 k8s-master kubelet[11279]: E1018 22:06:42.686265   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:42 k8s-master kubelet[11279]: E1018 22:06:42.662736   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:42 k8s-master kubelet[11279]: W1018 22:06:42.525458   11279 pod_container_deletor.go:77] Container "34778f5b4a1c609810c4a76370fc948b1455b04abb9096286fa5c335e3d6a0c8" not found in pod's containers
Oct 18 22:06:42 k8s-master kubelet[11279]: I1018 22:06:42.478450   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:42 k8s-master kubelet[11279]: E1018 22:06:42.469469   11279 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:06:42 k8s-master kubelet[11279]: W1018 22:06:42.469341   11279 container.go:354] Failed to create summary reader for "/libcontainer_11426_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:06:42 k8s-master kubelet[11279]: E1018 22:06:42.421036   11279 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:06:42 k8s-master kubelet[11279]: E1018 22:06:42.420841   11279 helpers.go:468] PercpuUsage had 0 cpus, but the actual number is 1; ignoring extra CPUs
Oct 18 22:06:42 k8s-master kubelet[11279]: W1018 22:06:42.397886   11279 container.go:354] Failed to create summary reader for "/libcontainer_11410_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:06:42 k8s-master kubelet[11279]: W1018 22:06:42.397643   11279 container.go:354] Failed to create summary reader for "/libcontainer_11397_systemd_test_default.slice": none of the resources are being tracked.
Oct 18 22:06:41 k8s-master kubelet[11279]: E1018 22:06:41.702972   11279 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:41 k8s-master kubelet[11279]: I1018 22:06:41.702577   11279 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:06:41 k8s-master kubelet[11279]: I1018 22:06:41.698795   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:41 k8s-master kubelet[11279]: E1018 22:06:41.685826   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:41 k8s-master kubelet[11279]: E1018 22:06:41.684030   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:41 k8s-master kubelet[11279]: W1018 22:06:41.665106   11279 raw.go:87] Error while processing event ("/sys/fs/cgroup/memory/libcontainer_11426_systemd_test_default.slice": 0x40000100 == IN_CREATE|IN_ISDIR): inotify_add_watch /sys/fs/cgroup/memory/libcontainer_11426_systemd_test_default.slice: no such file or directory
Oct 18 22:06:41 k8s-master kubelet[11279]: E1018 22:06:41.659925   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:40 k8s-master kubelet[11279]: E1018 22:06:40.683991   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:40 k8s-master kubelet[11279]: E1018 22:06:40.683212   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:40 k8s-master kubelet[11279]: E1018 22:06:40.655844   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:40 k8s-master kubelet[11279]: E1018 22:06:40.487267   11279 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:06:40 k8s-master kubelet[11279]: W1018 22:06:40.486783   11279 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:06:40 k8s-master kubelet[11279]: I1018 22:06:40.315124   11279 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs" (UniqueName: "kubernetes.io/host-path/46d49698141c16a22a3a11256b78f37e-ca-certs") pod "kube-apiserver-k8s-master" (UID: "46d49698141c16a22a3a11256b78f37e")
Oct 18 22:06:40 k8s-master kubelet[11279]: I1018 22:06:40.315100   11279 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "k8s-certs" (UniqueName: "kubernetes.io/host-path/46d49698141c16a22a3a11256b78f37e-k8s-certs") pod "kube-apiserver-k8s-master" (UID: "46d49698141c16a22a3a11256b78f37e")
Oct 18 22:06:40 k8s-master kubelet[11279]: I1018 22:06:40.314979   11279 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs-etc-pki" (UniqueName: "kubernetes.io/host-path/46d49698141c16a22a3a11256b78f37e-ca-certs-etc-pki") pod "kube-apiserver-k8s-master" (UID: "46d49698141c16a22a3a11256b78f37e")
Oct 18 22:06:40 k8s-master kubelet[11279]: I1018 22:06:40.215036   11279 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/f36e1d93ccf23a215dceaf6196109d5e-kubeconfig") pod "kube-controller-manager-k8s-master" (UID: "f36e1d93ccf23a215dceaf6196109d5e")
Oct 18 22:06:40 k8s-master kubelet[11279]: I1018 22:06:40.215012   11279 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs" (UniqueName: "kubernetes.io/host-path/f36e1d93ccf23a215dceaf6196109d5e-ca-certs") pod "kube-controller-manager-k8s-master" (UID: "f36e1d93ccf23a215dceaf6196109d5e")
Oct 18 22:06:40 k8s-master kubelet[11279]: I1018 22:06:40.214984   11279 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "k8s-certs" (UniqueName: "kubernetes.io/host-path/f36e1d93ccf23a215dceaf6196109d5e-k8s-certs") pod "kube-controller-manager-k8s-master" (UID: "f36e1d93ccf23a215dceaf6196109d5e")
Oct 18 22:06:40 k8s-master kubelet[11279]: I1018 22:06:40.214952   11279 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "etcd" (UniqueName: "kubernetes.io/host-path/40eb0889c614345e2a2714d4ee7d1cc0-etcd") pod "etcd-k8s-master" (UID: "40eb0889c614345e2a2714d4ee7d1cc0")
Oct 18 22:06:40 k8s-master kubelet[11279]: I1018 22:06:40.214928   11279 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "kubeconfig" (UniqueName: "kubernetes.io/host-path/5c60da2dd3a2204955600c1da11b55d9-kubeconfig") pod "kube-scheduler-k8s-master" (UID: "5c60da2dd3a2204955600c1da11b55d9")
Oct 18 22:06:40 k8s-master kubelet[11279]: I1018 22:06:40.214893   11279 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "ca-certs-etc-pki" (UniqueName: "kubernetes.io/host-path/f36e1d93ccf23a215dceaf6196109d5e-ca-certs-etc-pki") pod "kube-controller-manager-k8s-master" (UID: "f36e1d93ccf23a215dceaf6196109d5e")
Oct 18 22:06:40 k8s-master kubelet[11279]: I1018 22:06:40.214854   11279 reconciler.go:212] operationExecutor.VerifyControllerAttachedVolume started for volume "flexvolume-dir" (UniqueName: "kubernetes.io/host-path/f36e1d93ccf23a215dceaf6196109d5e-flexvolume-dir") pod "kube-controller-manager-k8s-master" (UID: "f36e1d93ccf23a215dceaf6196109d5e")
Oct 18 22:06:40 k8s-master kubelet[11279]: E1018 22:06:40.212990   11279 kubelet.go:1612] Failed creating a mirror pod for "kube-apiserver-k8s-master_kube-system(46d49698141c16a22a3a11256b78f37e)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:40 k8s-master kubelet[11279]: E1018 22:06:40.183517   11279 kubelet.go:1612] Failed creating a mirror pod for "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:40 k8s-master kubelet[11279]: E1018 22:06:40.177131   11279 kubelet.go:1612] Failed creating a mirror pod for "kube-scheduler-k8s-master_kube-system(5c60da2dd3a2204955600c1da11b55d9)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:40 k8s-master kubelet[11279]: E1018 22:06:40.151164   11279 kubelet.go:1612] Failed creating a mirror pod for "kube-controller-manager-k8s-master_kube-system(f36e1d93ccf23a215dceaf6196109d5e)": Post https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:40 k8s-master kubelet[11279]: W1018 22:06:40.140051   11279 status_manager.go:431] Failed to get status for pod "kube-apiserver-k8s-master_kube-system(46d49698141c16a22a3a11256b78f37e)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-apiserver-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:40 k8s-master kubelet[11279]: W1018 22:06:40.137487   11279 status_manager.go:431] Failed to get status for pod "etcd-k8s-master_kube-system(40eb0889c614345e2a2714d4ee7d1cc0)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/etcd-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:40 k8s-master kubelet[11279]: I1018 22:06:40.137005   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:40 k8s-master kubelet[11279]: I1018 22:06:40.130100   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:40 k8s-master kubelet[11279]: I1018 22:06:40.129690   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:40 k8s-master kubelet[11279]: W1018 22:06:40.128380   11279 status_manager.go:431] Failed to get status for pod "kube-scheduler-k8s-master_kube-system(5c60da2dd3a2204955600c1da11b55d9)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-scheduler-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:40 k8s-master kubelet[11279]: W1018 22:06:40.125002   11279 status_manager.go:431] Failed to get status for pod "kube-controller-manager-k8s-master_kube-system(f36e1d93ccf23a215dceaf6196109d5e)": Get https://192.168.0.155:6443/api/v1/namespaces/kube-system/pods/kube-controller-manager-k8s-master: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:40 k8s-master kubelet[11279]: I1018 22:06:40.122506   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:40 k8s-master kubelet[11279]: I1018 22:06:40.122133   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:40 k8s-master kubelet[11279]: I1018 22:06:40.119661   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:40 k8s-master kubelet[11279]: I1018 22:06:40.119296   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:40 k8s-master kubelet[11279]: I1018 22:06:40.114027   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:39 k8s-master kubelet[11279]: E1018 22:06:39.675637   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:39 k8s-master kubelet[11279]: E1018 22:06:39.674483   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:39 k8s-master kubelet[11279]: E1018 22:06:39.655215   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:38 k8s-master kubelet[11279]: E1018 22:06:38.674460   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:38 k8s-master kubelet[11279]: E1018 22:06:38.673871   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:38 k8s-master kubelet[11279]: E1018 22:06:38.654519   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:38 k8s-master kubelet[11279]: E1018 22:06:38.498510   11279 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:38 k8s-master kubelet[11279]: I1018 22:06:38.498079   11279 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:06:38 k8s-master kubelet[11279]: I1018 22:06:38.495074   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:37 k8s-master kubelet[11279]: E1018 22:06:37.673381   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:37 k8s-master kubelet[11279]: E1018 22:06:37.670015   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:37 k8s-master kubelet[11279]: E1018 22:06:37.653832   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:36 k8s-master kubelet[11279]: E1018 22:06:36.894800   11279 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:36 k8s-master kubelet[11279]: I1018 22:06:36.894436   11279 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:06:36 k8s-master kubelet[11279]: I1018 22:06:36.891334   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:36 k8s-master kubelet[11279]: E1018 22:06:36.670845   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:36 k8s-master kubelet[11279]: E1018 22:06:36.669248   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:36 k8s-master kubelet[11279]: E1018 22:06:36.653117   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:36 k8s-master kubelet[11279]: E1018 22:06:36.091004   11279 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:36 k8s-master kubelet[11279]: I1018 22:06:36.090508   11279 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:06:36 k8s-master kubelet[11279]: I1018 22:06:36.085591   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:35 k8s-master kubelet[11279]: E1018 22:06:35.685278   11279 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:35 k8s-master kubelet[11279]: I1018 22:06:35.684753   11279 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:06:35 k8s-master kubelet[11279]: I1018 22:06:35.681481   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:35 k8s-master kubelet[11279]: E1018 22:06:35.670389   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:35 k8s-master kubelet[11279]: E1018 22:06:35.668599   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:35 k8s-master kubelet[11279]: E1018 22:06:35.652374   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:35 k8s-master kubelet[11279]: E1018 22:06:35.481248   11279 kubelet_node_status.go:107] Unable to register node "k8s-master" with API server: Post https://192.168.0.155:6443/api/v1/nodes: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:35 k8s-master kubelet[11279]: I1018 22:06:35.479454   11279 kubelet_node_status.go:83] Attempting to register node k8s-master
Oct 18 22:06:35 k8s-master kubelet[11279]: E1018 22:06:35.461137   11279 eviction_manager.go:238] eviction manager: unexpected err: failed to get node info: node 'k8s-master' not found
Oct 18 22:06:35 k8s-master kubelet[11279]: I1018 22:06:35.309852   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:35 k8s-master kubelet[11279]: I1018 22:06:35.261249   11279 manager.go:316] Recovery completed
Oct 18 22:06:35 k8s-master kubelet[11279]: I1018 22:06:35.150156   11279 manager.go:311] Starting recovery of all containers
Oct 18 22:06:35 k8s-master kubelet[11279]: I1018 22:06:35.149542   11279 manager.go:1140] Started watching for new ooms in manager
Oct 18 22:06:35 k8s-master kubelet[11279]: I1018 22:06:35.149375   11279 factory.go:86] Registering Raw factory
Oct 18 22:06:35 k8s-master kubelet[11279]: I1018 22:06:35.149187   11279 factory.go:54] Registering systemd factory
Oct 18 22:06:35 k8s-master kubelet[11279]: W1018 22:06:35.149168   11279 manager.go:276] Registration of the crio container factory failed: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:06:35 k8s-master kubelet[11279]: W1018 22:06:35.149002   11279 manager.go:265] Registration of the rkt container factory failed: unable to communicate with Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:06:35 k8s-master kubelet[11279]: I1018 22:06:35.148968   11279 factory.go:355] Registering Docker factory
Oct 18 22:06:35 k8s-master kubelet[11279]: E1018 22:06:35.125358   11279 kubelet.go:2095] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Oct 18 22:06:35 k8s-master kubelet[11279]: W1018 22:06:35.125015   11279 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:06:35 k8s-master kubelet[11279]: I1018 22:06:35.114593   11279 volume_manager.go:246] Starting Kubelet Volume Manager
Oct 18 22:06:35 k8s-master kubelet[11279]: E1018 22:06:35.114566   11279 container_manager_linux.go:603] [ContainerManager]: Fail to get rootfs information unable to find data for container /
Oct 18 22:06:35 k8s-master kubelet[11279]: I1018 22:06:35.113665   11279 kubelet.go:1779] skipping pod synchronization - [container runtime is down PLEG is not healthy: pleg was last seen active 2562047h47m16.854775807s ago; threshold is 3m0s]
Oct 18 22:06:35 k8s-master kubelet[11279]: I1018 22:06:35.113650   11279 kubelet.go:1768] Starting kubelet main sync loop.
Oct 18 22:06:35 k8s-master kubelet[11279]: I1018 22:06:35.113634   11279 status_manager.go:140] Starting to sync pod status with apiserver
Oct 18 22:06:35 k8s-master kubelet[11279]: I1018 22:06:35.113580   11279 fs_resource_analyzer.go:66] Starting FS ResourceAnalyzer
Oct 18 22:06:34 k8s-master kubelet[11279]: E1018 22:06:34.984976   11279 event.go:209] Unable to write event: 'Post https://192.168.0.155:6443/api/v1/namespaces/default/events: dial tcp 192.168.0.155:6443: getsockopt: connection refused' (may retry after sleeping)
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.980081   11279 server.go:296] Adding debug handlers to kubelet server.
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.979147   11279 server.go:128] Starting to listen on 0.0.0.0:10250
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.978753   11279 kubelet_node_status.go:276] Setting node annotation to enable volume controller attach/detach
Oct 18 22:06:34 k8s-master kubelet[11279]: E1018 22:06:34.977925   11279 kubelet.go:1234] Image garbage collection failed once. Stats initialization may not have completed yet: failed to get imageFs info: unable to find data for container /
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.943100   11279 server.go:718] Started kubelet v1.8.1
Oct 18 22:06:34 k8s-master kubelet[11279]: W1018 22:06:34.930177   11279 probe.go:215] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.730471   11279 kuberuntime_manager.go:177] Container runtime docker initialized, version: 17.10.0-ce, apiVersion: 1.33.0
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.727032   11279 remote_runtime.go:43] Connecting to runtime service unix:///var/run/dockershim.sock
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.699739   11279 docker_service.go:224] Setting cgroupDriver to systemd
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.678956   11279 docker_service.go:207] Docker cri networking managed by cni
Oct 18 22:06:34 k8s-master kubelet[11279]: W1018 22:06:34.678921   11279 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:06:34 k8s-master kubelet[11279]: W1018 22:06:34.672150   11279 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:06:34 k8s-master kubelet[11279]: W1018 22:06:34.669777   11279 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.669445   11279 kubelet.go:517] Hairpin mode set to "hairpin-veth"
Oct 18 22:06:34 k8s-master kubelet[11279]: W1018 22:06:34.669410   11279 kubelet_network.go:69] Hairpin mode set to "promiscuous-bridge" but kubenet is not enabled, falling back to "hairpin-veth"
Oct 18 22:06:34 k8s-master kubelet[11279]: E1018 22:06:34.648013   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:422: Failed to list *v1.Node: Get https://192.168.0.155:6443/api/v1/nodes?fieldSelector=metadata.name%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:34 k8s-master kubelet[11279]: E1018 22:06:34.647792   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/config/apiserver.go:47: Failed to list *v1.Pod: Get https://192.168.0.155:6443/api/v1/pods?fieldSelector=spec.nodeName%3Dk8s-master&resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:34 k8s-master kubelet[11279]: E1018 22:06:34.647704   11279 reflector.go:205] k8s.io/kubernetes/pkg/kubelet/kubelet.go:413: Failed to list *v1.Service: Get https://192.168.0.155:6443/api/v1/services?resourceVersion=0: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.568965   11279 kubelet.go:283] Watching apiserver
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.568928   11279 kubelet.go:273] Adding manifest file: /etc/kubernetes/manifests
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.568783   11279 container_manager_linux.go:288] Creating device plugin handler: false
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.568663   11279 container_manager_linux.go:257] Creating Container Manager object based on Node Config: {RuntimeCgroupsName: SystemCgroupsName: KubeletCgroupsName: ContainerRuntime:docker CgroupsPerQOS:true CgroupRoot:/ CgroupDriver:systemd ProtectKernelDefaults:false NodeAllocatableConfig:{KubeReservedCgroupName: SystemReservedCgroupName: EnforceNodeAllocatable:map[pods:{}] KubeReserved:map[] SystemReserved:map[] HardEvictionThresholds:[{Signal:memory.available Operator:LessThan Value:{Quantity:100Mi Percentage:0} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.available Operator:LessThan Value:{Quantity:<nil> Percentage:0.1} GracePeriod:0s MinReclaim:<nil>} {Signal:nodefs.inodesFree Operator:LessThan Value:{Quantity:<nil> Percentage:0.05} GracePeriod:0s MinReclaim:<nil>}]} ExperimentalQOSReserved:map[] ExperimentalCPUManagerPolicy:none ExperimentalCPUManagerReconcilePeriod:10s}
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.568622   11279 container_manager_linux.go:252] container manager verified user specified cgroup-root exists: /
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.565185   11279 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.564558   11279 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.561635   11279 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.559932   11279 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0}]
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.559890   11279 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 22:06:34 k8s-master kubelet[11279]: W1018 22:06:34.534573   11279 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:06:34 k8s-master kubelet[11279]: W1018 22:06:34.534435   11279 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.505227   11279 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:06:34 k8s-master kubelet[11279]: E1018 22:06:34.504306   11279 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.503599   11279 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:06:34 k8s-master kubelet[11279]: W1018 22:06:34.469137   11279 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.468984   11279 feature_gate.go:156] feature gates: map[]
Oct 18 22:06:34 k8s-master kubelet[11279]: W1018 22:06:34.447344   11279 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.445729   11279 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.445680   11279 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.430067   11279 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.430053   11279 controller.go:114] kubelet config controller: starting controller
Oct 18 22:06:34 k8s-master kubelet[11279]: I1018 22:06:34.429911   11279 feature_gate.go:156] feature gates: map[]
Oct 18 22:06:34 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:06:34 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:06:34 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:06:24 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:06:24 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:06:24 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:06:24 k8s-master kubelet[11248]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2160        -1]
Oct 18 22:06:24 k8s-master kubelet[11248]: I1018 22:06:24.017318   11248 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:06:24 k8s-master kubelet[11248]: I1018 22:06:24.016715   11248 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:06:24 k8s-master kubelet[11248]: I1018 22:06:24.013966   11248 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true}] DiskMap:map[8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq} 253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:06:24 k8s-master kubelet[11248]: I1018 22:06:24.012346   11248 fs.go:140] Filesystem partitions: map[/dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0} tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0}]
Oct 18 22:06:24 k8s-master kubelet[11248]: I1018 22:06:24.012300   11248 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 22:06:23 k8s-master kubelet[11248]: W1018 22:06:23.985810   11248 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:06:23 k8s-master kubelet[11248]: W1018 22:06:23.985604   11248 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:06:23 k8s-master kubelet[11248]: I1018 22:06:23.963830   11248 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:06:23 k8s-master kubelet[11248]: E1018 22:06:23.963072   11248 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:23 k8s-master kubelet[11248]: I1018 22:06:23.962621   11248 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:06:23 k8s-master kubelet[11248]: W1018 22:06:23.934679   11248 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:06:23 k8s-master kubelet[11248]: I1018 22:06:23.934539   11248 feature_gate.go:156] feature gates: map[]
Oct 18 22:06:23 k8s-master kubelet[11248]: W1018 22:06:23.923688   11248 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:06:23 k8s-master kubelet[11248]: I1018 22:06:23.919496   11248 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:06:23 k8s-master kubelet[11248]: I1018 22:06:23.919439   11248 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:06:23 k8s-master kubelet[11248]: I1018 22:06:23.911193   11248 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:06:23 k8s-master kubelet[11248]: I1018 22:06:23.911188   11248 controller.go:114] kubelet config controller: starting controller
Oct 18 22:06:23 k8s-master kubelet[11248]: I1018 22:06:23.911090   11248 feature_gate.go:156] feature gates: map[]
Oct 18 22:06:23 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:06:23 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:06:23 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:06:13 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:06:13 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:06:13 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:06:13 k8s-master kubelet[11228]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2176        -1]
Oct 18 22:06:13 k8s-master kubelet[11228]: I1018 22:06:13.613053   11228 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:06:13 k8s-master kubelet[11228]: I1018 22:06:13.612602   11228 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:06:13 k8s-master kubelet[11228]: I1018 22:06:13.609201   11228 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:06:13 k8s-master kubelet[11228]: I1018 22:06:13.603663   11228 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0}]
Oct 18 22:06:13 k8s-master kubelet[11228]: I1018 22:06:13.603594   11228 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 22:06:13 k8s-master kubelet[11228]: W1018 22:06:13.578843   11228 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:06:13 k8s-master kubelet[11228]: W1018 22:06:13.578612   11228 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:06:13 k8s-master kubelet[11228]: I1018 22:06:13.556889   11228 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:06:13 k8s-master kubelet[11228]: E1018 22:06:13.555371   11228 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:13 k8s-master kubelet[11228]: I1018 22:06:13.554850   11228 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:06:13 k8s-master kubelet[11228]: W1018 22:06:13.528598   11228 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:06:13 k8s-master kubelet[11228]: I1018 22:06:13.528432   11228 feature_gate.go:156] feature gates: map[]
Oct 18 22:06:13 k8s-master kubelet[11228]: W1018 22:06:13.516160   11228 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:06:13 k8s-master kubelet[11228]: I1018 22:06:13.511763   11228 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:06:13 k8s-master kubelet[11228]: I1018 22:06:13.511703   11228 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:06:13 k8s-master kubelet[11228]: I1018 22:06:13.496794   11228 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:06:13 k8s-master kubelet[11228]: I1018 22:06:13.496774   11228 controller.go:114] kubelet config controller: starting controller
Oct 18 22:06:13 k8s-master kubelet[11228]: I1018 22:06:13.496615   11228 feature_gate.go:156] feature gates: map[]
Oct 18 22:06:13 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:06:13 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:06:13 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:06:03 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:06:03 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:06:03 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:06:03 k8s-master kubelet[11208]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2192        -1]
Oct 18 22:06:03 k8s-master kubelet[11208]: I1018 22:06:03.254904   11208 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:06:03 k8s-master kubelet[11208]: I1018 22:06:03.254194   11208 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:06:03 k8s-master kubelet[11208]: I1018 22:06:03.251057   11208 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:06:03 k8s-master kubelet[11208]: I1018 22:06:03.249670   11208 fs.go:140] Filesystem partitions: map[/dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0} tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0}]
Oct 18 22:06:03 k8s-master kubelet[11208]: I1018 22:06:03.249626   11208 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 22:06:03 k8s-master kubelet[11208]: W1018 22:06:03.230054   11208 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:06:03 k8s-master kubelet[11208]: W1018 22:06:03.229933   11208 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:06:03 k8s-master kubelet[11208]: I1018 22:06:03.214647   11208 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:06:03 k8s-master kubelet[11208]: E1018 22:06:03.214090   11208 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:06:03 k8s-master kubelet[11208]: I1018 22:06:03.213648   11208 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:06:03 k8s-master kubelet[11208]: W1018 22:06:03.185759   11208 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:06:03 k8s-master kubelet[11208]: I1018 22:06:03.185595   11208 feature_gate.go:156] feature gates: map[]
Oct 18 22:06:03 k8s-master kubelet[11208]: W1018 22:06:03.176826   11208 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:06:03 k8s-master kubelet[11208]: I1018 22:06:03.174068   11208 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:06:03 k8s-master kubelet[11208]: I1018 22:06:03.174028   11208 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:06:03 k8s-master kubelet[11208]: I1018 22:06:03.165618   11208 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:06:03 k8s-master kubelet[11208]: I1018 22:06:03.165613   11208 controller.go:114] kubelet config controller: starting controller
Oct 18 22:06:03 k8s-master kubelet[11208]: I1018 22:06:03.165526   11208 feature_gate.go:156] feature gates: map[]
Oct 18 22:06:03 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:06:03 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:06:03 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:05:53 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:05:53 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:05:53 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:05:53 k8s-master kubelet[11188]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2208        -1]
Oct 18 22:05:53 k8s-master kubelet[11188]: I1018 22:05:53.016530   11188 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:05:53 k8s-master kubelet[11188]: I1018 22:05:53.011476   11188 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:05:53 k8s-master kubelet[11188]: I1018 22:05:53.009461   11188 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:05:53 k8s-master kubelet[11188]: I1018 22:05:53.004917   11188 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0}]
Oct 18 22:05:53 k8s-master kubelet[11188]: I1018 22:05:53.004781   11188 fs.go:139] Filesystem UUIDs: map[a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0 752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1]
Oct 18 22:05:52 k8s-master kubelet[11188]: W1018 22:05:52.977632   11188 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:05:52 k8s-master kubelet[11188]: W1018 22:05:52.977491   11188 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:05:52 k8s-master kubelet[11188]: I1018 22:05:52.961268   11188 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:05:52 k8s-master kubelet[11188]: E1018 22:05:52.960669   11188 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:05:52 k8s-master kubelet[11188]: I1018 22:05:52.960258   11188 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:05:52 k8s-master kubelet[11188]: W1018 22:05:52.934132   11188 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:05:52 k8s-master kubelet[11188]: I1018 22:05:52.933985   11188 feature_gate.go:156] feature gates: map[]
Oct 18 22:05:52 k8s-master kubelet[11188]: W1018 22:05:52.925371   11188 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:05:52 k8s-master kubelet[11188]: I1018 22:05:52.923492   11188 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:05:52 k8s-master kubelet[11188]: I1018 22:05:52.923306   11188 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:05:52 k8s-master kubelet[11188]: I1018 22:05:52.912512   11188 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:05:52 k8s-master kubelet[11188]: I1018 22:05:52.912506   11188 controller.go:114] kubelet config controller: starting controller
Oct 18 22:05:52 k8s-master kubelet[11188]: I1018 22:05:52.912420   11188 feature_gate.go:156] feature gates: map[]
Oct 18 22:05:52 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:05:52 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:05:52 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:05:42 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:05:42 k8s-master kubelet[11168]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2232        -1]
Oct 18 22:05:42 k8s-master kubelet[11168]: I1018 22:05:42.572842   11168 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:05:42 k8s-master kubelet[11168]: I1018 22:05:42.572004   11168 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:05:42 k8s-master kubelet[11168]: I1018 22:05:42.570524   11168 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true}] DiskMap:map[8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq} 253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:05:42 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:05:42 k8s-master kubelet[11168]: I1018 22:05:42.568534   11168 fs.go:140] Filesystem partitions: map[/dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0} tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0}]
Oct 18 22:05:42 k8s-master kubelet[11168]: I1018 22:05:42.568492   11168 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 22:05:42 k8s-master kubelet[11168]: W1018 22:05:42.541101   11168 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:05:42 k8s-master kubelet[11168]: W1018 22:05:42.540886   11168 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:05:42 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:05:42 k8s-master kubelet[11168]: I1018 22:05:42.513936   11168 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:05:42 k8s-master kubelet[11168]: E1018 22:05:42.512963   11168 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:05:42 k8s-master kubelet[11168]: I1018 22:05:42.512279   11168 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:05:42 k8s-master kubelet[11168]: W1018 22:05:42.474953   11168 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:05:42 k8s-master kubelet[11168]: I1018 22:05:42.474735   11168 feature_gate.go:156] feature gates: map[]
Oct 18 22:05:42 k8s-master kubelet[11168]: W1018 22:05:42.458148   11168 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:05:42 k8s-master kubelet[11168]: I1018 22:05:42.456570   11168 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:05:42 k8s-master kubelet[11168]: I1018 22:05:42.456520   11168 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:05:42 k8s-master kubelet[11168]: I1018 22:05:42.441842   11168 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:05:42 k8s-master kubelet[11168]: I1018 22:05:42.441831   11168 controller.go:114] kubelet config controller: starting controller
Oct 18 22:05:42 k8s-master kubelet[11168]: I1018 22:05:42.441682   11168 feature_gate.go:156] feature gates: map[]
Oct 18 22:05:42 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:05:42 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:05:42 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:05:32 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:05:32 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:05:32 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:05:32 k8s-master kubelet[11148]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2240        -1]
Oct 18 22:05:32 k8s-master kubelet[11148]: I1018 22:05:32.024482   11148 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:05:32 k8s-master kubelet[11148]: I1018 22:05:32.020229   11148 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:05:32 k8s-master kubelet[11148]: I1018 22:05:32.017521   11148 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true}] DiskMap:map[8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq} 253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:05:32 k8s-master kubelet[11148]: I1018 22:05:32.016367   11148 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0}]
Oct 18 22:05:32 k8s-master kubelet[11148]: I1018 22:05:32.016333   11148 fs.go:139] Filesystem UUIDs: map[a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0 752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1]
Oct 18 22:05:31 k8s-master kubelet[11148]: W1018 22:05:31.998507   11148 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:05:31 k8s-master kubelet[11148]: W1018 22:05:31.998348   11148 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:05:31 k8s-master kubelet[11148]: I1018 22:05:31.981162   11148 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:05:31 k8s-master kubelet[11148]: E1018 22:05:31.980597   11148 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:05:31 k8s-master kubelet[11148]: I1018 22:05:31.980101   11148 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:05:31 k8s-master kubelet[11148]: W1018 22:05:31.953030   11148 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:05:31 k8s-master kubelet[11148]: I1018 22:05:31.952862   11148 feature_gate.go:156] feature gates: map[]
Oct 18 22:05:31 k8s-master kubelet[11148]: W1018 22:05:31.942761   11148 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:05:31 k8s-master kubelet[11148]: I1018 22:05:31.940884   11148 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:05:31 k8s-master kubelet[11148]: I1018 22:05:31.940815   11148 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:05:31 k8s-master kubelet[11148]: I1018 22:05:31.926017   11148 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:05:31 k8s-master kubelet[11148]: I1018 22:05:31.926010   11148 controller.go:114] kubelet config controller: starting controller
Oct 18 22:05:31 k8s-master kubelet[11148]: I1018 22:05:31.925907   11148 feature_gate.go:156] feature gates: map[]
Oct 18 22:05:31 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:05:31 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:05:31 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:05:21 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:05:21 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:05:21 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:05:21 k8s-master kubelet[11120]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2260        -1]
Oct 18 22:05:21 k8s-master kubelet[11120]: I1018 22:05:21.531285   11120 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:05:21 k8s-master kubelet[11120]: I1018 22:05:21.530487   11120 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:05:21 k8s-master kubelet[11120]: I1018 22:05:21.527725   11120 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:05:21 k8s-master kubelet[11120]: I1018 22:05:21.524500   11120 fs.go:140] Filesystem partitions: map[/dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0} tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0}]
Oct 18 22:05:21 k8s-master kubelet[11120]: I1018 22:05:21.524348   11120 fs.go:139] Filesystem UUIDs: map[a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0 752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1]
Oct 18 22:05:21 k8s-master kubelet[11120]: W1018 22:05:21.502744   11120 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:05:21 k8s-master kubelet[11120]: W1018 22:05:21.502604   11120 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:05:21 k8s-master kubelet[11120]: I1018 22:05:21.482496   11120 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:05:21 k8s-master kubelet[11120]: E1018 22:05:21.481463   11120 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:05:21 k8s-master kubelet[11120]: I1018 22:05:21.480902   11120 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:05:21 k8s-master kubelet[11120]: W1018 22:05:21.454201   11120 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:05:21 k8s-master kubelet[11120]: I1018 22:05:21.454047   11120 feature_gate.go:156] feature gates: map[]
Oct 18 22:05:21 k8s-master kubelet[11120]: W1018 22:05:21.443522   11120 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:05:21 k8s-master kubelet[11120]: I1018 22:05:21.441089   11120 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:05:21 k8s-master kubelet[11120]: I1018 22:05:21.441038   11120 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:05:21 k8s-master kubelet[11120]: I1018 22:05:21.427146   11120 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:05:21 k8s-master kubelet[11120]: I1018 22:05:21.427137   11120 controller.go:114] kubelet config controller: starting controller
Oct 18 22:05:21 k8s-master kubelet[11120]: I1018 22:05:21.427030   11120 feature_gate.go:156] feature gates: map[]
Oct 18 22:05:21 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:05:21 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:05:21 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:05:11 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:05:11 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:05:11 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:05:11 k8s-master kubelet[11100]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2276        -1]
Oct 18 22:05:11 k8s-master kubelet[11100]: I1018 22:05:11.252552   11100 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:05:11 k8s-master kubelet[11100]: I1018 22:05:11.251923   11100 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:05:11 k8s-master kubelet[11100]: I1018 22:05:11.249141   11100 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:05:11 k8s-master kubelet[11100]: I1018 22:05:11.247898   11100 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0}]
Oct 18 22:05:11 k8s-master kubelet[11100]: I1018 22:05:11.247861   11100 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 22:05:11 k8s-master kubelet[11100]: W1018 22:05:11.229772   11100 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:05:11 k8s-master kubelet[11100]: W1018 22:05:11.229637   11100 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:05:11 k8s-master kubelet[11100]: I1018 22:05:11.213408   11100 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:05:11 k8s-master kubelet[11100]: E1018 22:05:11.212792   11100 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:05:11 k8s-master kubelet[11100]: I1018 22:05:11.212327   11100 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:05:11 k8s-master kubelet[11100]: W1018 22:05:11.185257   11100 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:05:11 k8s-master kubelet[11100]: I1018 22:05:11.185092   11100 feature_gate.go:156] feature gates: map[]
Oct 18 22:05:11 k8s-master kubelet[11100]: W1018 22:05:11.176047   11100 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:05:11 k8s-master kubelet[11100]: I1018 22:05:11.174561   11100 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:05:11 k8s-master kubelet[11100]: I1018 22:05:11.174517   11100 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:05:11 k8s-master kubelet[11100]: I1018 22:05:11.165130   11100 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:05:11 k8s-master kubelet[11100]: I1018 22:05:11.165121   11100 controller.go:114] kubelet config controller: starting controller
Oct 18 22:05:11 k8s-master kubelet[11100]: I1018 22:05:11.165021   11100 feature_gate.go:156] feature gates: map[]
Oct 18 22:05:11 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:05:11 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:05:11 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:05:01 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:05:01 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:05:01 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:05:00 k8s-master kubelet[11080]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2292        -1]
Oct 18 22:05:00 k8s-master kubelet[11080]: I1018 22:05:00.995299   11080 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:05:00 k8s-master kubelet[11080]: I1018 22:05:00.994721   11080 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:05:00 k8s-master kubelet[11080]: I1018 22:05:00.989805   11080 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true}] DiskMap:map[253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq} 253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:05:00 k8s-master kubelet[11080]: I1018 22:05:00.988472   11080 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0}]
Oct 18 22:05:00 k8s-master kubelet[11080]: I1018 22:05:00.988432   11080 fs.go:139] Filesystem UUIDs: map[f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0 752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1]
Oct 18 22:05:00 k8s-master kubelet[11080]: W1018 22:05:00.970589   11080 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:05:00 k8s-master kubelet[11080]: W1018 22:05:00.970456   11080 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:05:00 k8s-master kubelet[11080]: I1018 22:05:00.953556   11080 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:05:00 k8s-master kubelet[11080]: E1018 22:05:00.952891   11080 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:05:00 k8s-master kubelet[11080]: I1018 22:05:00.952429   11080 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:05:00 k8s-master kubelet[11080]: W1018 22:05:00.927960   11080 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:05:00 k8s-master kubelet[11080]: I1018 22:05:00.927820   11080 feature_gate.go:156] feature gates: map[]
Oct 18 22:05:00 k8s-master kubelet[11080]: W1018 22:05:00.916523   11080 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:05:00 k8s-master kubelet[11080]: I1018 22:05:00.914552   11080 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:05:00 k8s-master kubelet[11080]: I1018 22:05:00.914509   11080 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:05:00 k8s-master kubelet[11080]: I1018 22:05:00.903978   11080 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:05:00 k8s-master kubelet[11080]: I1018 22:05:00.903973   11080 controller.go:114] kubelet config controller: starting controller
Oct 18 22:05:00 k8s-master kubelet[11080]: I1018 22:05:00.903890   11080 feature_gate.go:156] feature gates: map[]
Oct 18 22:05:00 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:05:00 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:05:00 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:04:50 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:04:50 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:04:50 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:04:50 k8s-master kubelet[11060]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2292        -1]
Oct 18 22:04:50 k8s-master kubelet[11060]: I1018 22:04:50.569004   11060 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:04:50 k8s-master kubelet[11060]: I1018 22:04:50.568240   11060 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:04:50 k8s-master kubelet[11060]: I1018 22:04:50.563605   11060 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true}] DiskMap:map[253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq} 253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:04:50 k8s-master kubelet[11060]: I1018 22:04:50.561452   11060 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0}]
Oct 18 22:04:50 k8s-master kubelet[11060]: I1018 22:04:50.561393   11060 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 22:04:50 k8s-master kubelet[11060]: W1018 22:04:50.539514   11060 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:04:50 k8s-master kubelet[11060]: W1018 22:04:50.539362   11060 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:04:50 k8s-master kubelet[11060]: I1018 22:04:50.520772   11060 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:04:50 k8s-master kubelet[11060]: E1018 22:04:50.519706   11060 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:04:50 k8s-master kubelet[11060]: I1018 22:04:50.518696   11060 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:04:50 k8s-master kubelet[11060]: W1018 22:04:50.482767   11060 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:04:50 k8s-master kubelet[11060]: I1018 22:04:50.482420   11060 feature_gate.go:156] feature gates: map[]
Oct 18 22:04:50 k8s-master kubelet[11060]: W1018 22:04:50.470465   11060 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:04:50 k8s-master kubelet[11060]: I1018 22:04:50.468225   11060 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:04:50 k8s-master kubelet[11060]: I1018 22:04:50.468172   11060 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:04:50 k8s-master kubelet[11060]: I1018 22:04:50.456815   11060 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:04:50 k8s-master kubelet[11060]: I1018 22:04:50.456799   11060 controller.go:114] kubelet config controller: starting controller
Oct 18 22:04:50 k8s-master kubelet[11060]: I1018 22:04:50.456593   11060 feature_gate.go:156] feature gates: map[]
Oct 18 22:04:50 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:04:50 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:04:50 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:04:40 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:04:40 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:04:40 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:04:40 k8s-master kubelet[11040]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2292        -1]
Oct 18 22:04:40 k8s-master kubelet[11040]: I1018 22:04:40.257351   11040 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:04:40 k8s-master kubelet[11040]: I1018 22:04:40.256340   11040 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:04:40 k8s-master kubelet[11040]: I1018 22:04:40.252666   11040 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:04:40 k8s-master kubelet[11040]: I1018 22:04:40.250987   11040 fs.go:140] Filesystem partitions: map[/dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0} tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0}]
Oct 18 22:04:40 k8s-master kubelet[11040]: I1018 22:04:40.250943   11040 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 22:04:40 k8s-master kubelet[11040]: W1018 22:04:40.227912   11040 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:04:40 k8s-master kubelet[11040]: W1018 22:04:40.227732   11040 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:04:40 k8s-master kubelet[11040]: I1018 22:04:40.208086   11040 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:04:40 k8s-master kubelet[11040]: E1018 22:04:40.207503   11040 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:04:40 k8s-master kubelet[11040]: I1018 22:04:40.206971   11040 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:04:40 k8s-master kubelet[11040]: W1018 22:04:40.176786   11040 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:04:40 k8s-master kubelet[11040]: I1018 22:04:40.176606   11040 feature_gate.go:156] feature gates: map[]
Oct 18 22:04:40 k8s-master kubelet[11040]: W1018 22:04:40.168866   11040 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:04:40 k8s-master kubelet[11040]: I1018 22:04:40.166968   11040 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:04:40 k8s-master kubelet[11040]: I1018 22:04:40.166918   11040 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:04:40 k8s-master kubelet[11040]: I1018 22:04:40.158687   11040 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:04:40 k8s-master kubelet[11040]: I1018 22:04:40.158679   11040 controller.go:114] kubelet config controller: starting controller
Oct 18 22:04:40 k8s-master kubelet[11040]: I1018 22:04:40.158586   11040 feature_gate.go:156] feature gates: map[]
Oct 18 22:04:40 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:04:40 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:04:40 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:04:29 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:04:29 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:04:29 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:04:29 k8s-master kubelet[11020]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2296        -1]
Oct 18 22:04:29 k8s-master kubelet[11020]: I1018 22:04:29.792279   11020 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:04:29 k8s-master kubelet[11020]: I1018 22:04:29.789925   11020 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:04:29 k8s-master kubelet[11020]: I1018 22:04:29.786302   11020 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true}] DiskMap:map[253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq} 253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:04:29 k8s-master kubelet[11020]: I1018 22:04:29.784566   11020 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0}]
Oct 18 22:04:29 k8s-master kubelet[11020]: I1018 22:04:29.784511   11020 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 22:04:29 k8s-master kubelet[11020]: W1018 22:04:29.758737   11020 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:04:29 k8s-master kubelet[11020]: W1018 22:04:29.758431   11020 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:04:29 k8s-master kubelet[11020]: I1018 22:04:29.737235   11020 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:04:29 k8s-master kubelet[11020]: E1018 22:04:29.734010   11020 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:04:29 k8s-master kubelet[11020]: I1018 22:04:29.733202   11020 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:04:29 k8s-master kubelet[11020]: W1018 22:04:29.705579   11020 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:04:29 k8s-master kubelet[11020]: I1018 22:04:29.705442   11020 feature_gate.go:156] feature gates: map[]
Oct 18 22:04:29 k8s-master kubelet[11020]: W1018 22:04:29.695585   11020 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:04:29 k8s-master kubelet[11020]: I1018 22:04:29.692978   11020 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:04:29 k8s-master kubelet[11020]: I1018 22:04:29.692934   11020 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:04:29 k8s-master kubelet[11020]: I1018 22:04:29.679798   11020 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:04:29 k8s-master kubelet[11020]: I1018 22:04:29.679783   11020 controller.go:114] kubelet config controller: starting controller
Oct 18 22:04:29 k8s-master kubelet[11020]: I1018 22:04:29.679577   11020 feature_gate.go:156] feature gates: map[]
Oct 18 22:04:29 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:04:29 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:04:29 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:04:19 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:04:19 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:04:19 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:04:19 k8s-master kubelet[10992]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2296        -1]
Oct 18 22:04:19 k8s-master kubelet[10992]: I1018 22:04:19.279642   10992 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:04:19 k8s-master kubelet[10992]: I1018 22:04:19.279100   10992 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:04:19 k8s-master kubelet[10992]: I1018 22:04:19.276893   10992 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:04:19 k8s-master kubelet[10992]: I1018 22:04:19.274289   10992 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0}]
Oct 18 22:04:19 k8s-master kubelet[10992]: I1018 22:04:19.274262   10992 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 22:04:19 k8s-master kubelet[10992]: W1018 22:04:19.256667   10992 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:04:19 k8s-master kubelet[10992]: W1018 22:04:19.256507   10992 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:04:19 k8s-master kubelet[10992]: I1018 22:04:19.237081   10992 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:04:19 k8s-master kubelet[10992]: E1018 22:04:19.236356   10992 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:04:19 k8s-master kubelet[10992]: I1018 22:04:19.235902   10992 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:04:19 k8s-master kubelet[10992]: W1018 22:04:19.206611   10992 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:04:19 k8s-master kubelet[10992]: I1018 22:04:19.206429   10992 feature_gate.go:156] feature gates: map[]
Oct 18 22:04:19 k8s-master kubelet[10992]: W1018 22:04:19.196583   10992 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:04:19 k8s-master kubelet[10992]: I1018 22:04:19.194402   10992 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:04:19 k8s-master kubelet[10992]: I1018 22:04:19.194357   10992 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:04:19 k8s-master kubelet[10992]: I1018 22:04:19.184175   10992 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:04:19 k8s-master kubelet[10992]: I1018 22:04:19.184167   10992 controller.go:114] kubelet config controller: starting controller
Oct 18 22:04:19 k8s-master kubelet[10992]: I1018 22:04:19.184078   10992 feature_gate.go:156] feature gates: map[]
Oct 18 22:04:19 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:04:19 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:04:19 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:04:08 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:04:08 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:04:08 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:04:08 k8s-master kubelet[10972]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2296        -1]
Oct 18 22:04:08 k8s-master kubelet[10972]: I1018 22:04:08.784313   10972 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:04:08 k8s-master kubelet[10972]: I1018 22:04:08.783329   10972 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:04:08 k8s-master kubelet[10972]: I1018 22:04:08.780253   10972 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:04:08 k8s-master kubelet[10972]: I1018 22:04:08.778877   10972 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0}]
Oct 18 22:04:08 k8s-master kubelet[10972]: I1018 22:04:08.778846   10972 fs.go:139] Filesystem UUIDs: map[a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0 752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1]
Oct 18 22:04:08 k8s-master kubelet[10972]: W1018 22:04:08.743947   10972 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:04:08 k8s-master kubelet[10972]: W1018 22:04:08.743691   10972 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:04:08 k8s-master kubelet[10972]: I1018 22:04:08.720116   10972 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:04:08 k8s-master kubelet[10972]: E1018 22:04:08.719750   10972 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:04:08 k8s-master kubelet[10972]: I1018 22:04:08.719286   10972 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:04:08 k8s-master kubelet[10972]: W1018 22:04:08.695272   10972 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:04:08 k8s-master kubelet[10972]: I1018 22:04:08.695077   10972 feature_gate.go:156] feature gates: map[]
Oct 18 22:04:08 k8s-master kubelet[10972]: W1018 22:04:08.685513   10972 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:04:08 k8s-master kubelet[10972]: I1018 22:04:08.683418   10972 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:04:08 k8s-master kubelet[10972]: I1018 22:04:08.683381   10972 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:04:08 k8s-master kubelet[10972]: I1018 22:04:08.671294   10972 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:04:08 k8s-master kubelet[10972]: I1018 22:04:08.671252   10972 controller.go:114] kubelet config controller: starting controller
Oct 18 22:04:08 k8s-master kubelet[10972]: I1018 22:04:08.670995   10972 feature_gate.go:156] feature gates: map[]
Oct 18 22:04:08 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:04:08 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:04:08 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:03:58 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:03:58 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:03:58 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:03:58 k8s-master kubelet[10952]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2300        -1]
Oct 18 22:03:58 k8s-master kubelet[10952]: I1018 22:03:58.487627   10952 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:03:58 k8s-master kubelet[10952]: I1018 22:03:58.486781   10952 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:03:58 k8s-master kubelet[10952]: I1018 22:03:58.483568   10952 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:03:58 k8s-master kubelet[10952]: I1018 22:03:58.481805   10952 fs.go:140] Filesystem partitions: map[/dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0} tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0}]
Oct 18 22:03:58 k8s-master kubelet[10952]: I1018 22:03:58.481758   10952 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 22:03:58 k8s-master kubelet[10952]: W1018 22:03:58.464875   10952 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:03:58 k8s-master kubelet[10952]: W1018 22:03:58.464739   10952 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:03:58 k8s-master kubelet[10952]: I1018 22:03:58.448070   10952 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:03:58 k8s-master kubelet[10952]: E1018 22:03:58.447452   10952 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:03:58 k8s-master kubelet[10952]: I1018 22:03:58.446962   10952 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:03:58 k8s-master kubelet[10952]: W1018 22:03:58.422707   10952 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:03:58 k8s-master kubelet[10952]: I1018 22:03:58.422567   10952 feature_gate.go:156] feature gates: map[]
Oct 18 22:03:58 k8s-master kubelet[10952]: W1018 22:03:58.413182   10952 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:03:58 k8s-master kubelet[10952]: I1018 22:03:58.411307   10952 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:03:58 k8s-master kubelet[10952]: I1018 22:03:58.411263   10952 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:03:58 k8s-master kubelet[10952]: I1018 22:03:58.401113   10952 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:03:58 k8s-master kubelet[10952]: I1018 22:03:58.401107   10952 controller.go:114] kubelet config controller: starting controller
Oct 18 22:03:58 k8s-master kubelet[10952]: I1018 22:03:58.401019   10952 feature_gate.go:156] feature gates: map[]
Oct 18 22:03:58 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:03:58 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:03:58 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:03:48 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:03:48 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:03:48 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:03:48 k8s-master kubelet[10932]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2300        -1]
Oct 18 22:03:48 k8s-master kubelet[10932]: I1018 22:03:48.057223   10932 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:03:48 k8s-master kubelet[10932]: I1018 22:03:48.056620   10932 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:03:48 k8s-master kubelet[10932]: I1018 22:03:48.053188   10932 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:03:48 k8s-master kubelet[10932]: I1018 22:03:48.051390   10932 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0}]
Oct 18 22:03:48 k8s-master kubelet[10932]: I1018 22:03:48.051351   10932 fs.go:139] Filesystem UUIDs: map[a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0 752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1]
Oct 18 22:03:48 k8s-master kubelet[10932]: W1018 22:03:48.026878   10932 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:03:48 k8s-master kubelet[10932]: W1018 22:03:48.026718   10932 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:03:48 k8s-master kubelet[10932]: I1018 22:03:48.003451   10932 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:03:48 k8s-master kubelet[10932]: E1018 22:03:48.002428   10932 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:03:48 k8s-master kubelet[10932]: I1018 22:03:48.001986   10932 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:03:47 k8s-master kubelet[10932]: W1018 22:03:47.975368   10932 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:03:47 k8s-master kubelet[10932]: I1018 22:03:47.975216   10932 feature_gate.go:156] feature gates: map[]
Oct 18 22:03:47 k8s-master kubelet[10932]: W1018 22:03:47.964442   10932 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:03:47 k8s-master kubelet[10932]: I1018 22:03:47.961623   10932 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:03:47 k8s-master kubelet[10932]: I1018 22:03:47.961568   10932 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:03:47 k8s-master kubelet[10932]: I1018 22:03:47.946831   10932 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:03:47 k8s-master kubelet[10932]: I1018 22:03:47.946817   10932 controller.go:114] kubelet config controller: starting controller
Oct 18 22:03:47 k8s-master kubelet[10932]: I1018 22:03:47.946600   10932 feature_gate.go:156] feature gates: map[]
Oct 18 22:03:47 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:03:47 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:03:47 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:03:37 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:03:37 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:03:37 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:03:37 k8s-master kubelet[10912]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2300        -1]
Oct 18 22:03:37 k8s-master kubelet[10912]: I1018 22:03:37.733001   10912 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:03:37 k8s-master kubelet[10912]: I1018 22:03:37.732508   10912 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:03:37 k8s-master kubelet[10912]: I1018 22:03:37.730152   10912 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:03:37 k8s-master kubelet[10912]: I1018 22:03:37.729005   10912 fs.go:140] Filesystem partitions: map[/dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0} tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0}]
Oct 18 22:03:37 k8s-master kubelet[10912]: I1018 22:03:37.728979   10912 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 22:03:37 k8s-master kubelet[10912]: W1018 22:03:37.713372   10912 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:03:37 k8s-master kubelet[10912]: W1018 22:03:37.713182   10912 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:03:37 k8s-master kubelet[10912]: I1018 22:03:37.698757   10912 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:03:37 k8s-master kubelet[10912]: E1018 22:03:37.698217   10912 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:03:37 k8s-master kubelet[10912]: I1018 22:03:37.697718   10912 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:03:37 k8s-master kubelet[10912]: W1018 22:03:37.674337   10912 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:03:37 k8s-master kubelet[10912]: I1018 22:03:37.674202   10912 feature_gate.go:156] feature gates: map[]
Oct 18 22:03:37 k8s-master kubelet[10912]: W1018 22:03:37.664844   10912 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:03:37 k8s-master kubelet[10912]: I1018 22:03:37.662706   10912 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:03:37 k8s-master kubelet[10912]: I1018 22:03:37.662668   10912 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:03:37 k8s-master kubelet[10912]: I1018 22:03:37.654165   10912 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:03:37 k8s-master kubelet[10912]: I1018 22:03:37.654155   10912 controller.go:114] kubelet config controller: starting controller
Oct 18 22:03:37 k8s-master kubelet[10912]: I1018 22:03:37.654059   10912 feature_gate.go:156] feature gates: map[]
Oct 18 22:03:37 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:03:37 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:03:37 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:03:27 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:03:27 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:03:27 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:03:27 k8s-master kubelet[10882]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2300        -1]
Oct 18 22:03:27 k8s-master kubelet[10882]: I1018 22:03:27.360687   10882 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:03:27 k8s-master kubelet[10882]: I1018 22:03:27.359838   10882 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:03:27 k8s-master kubelet[10882]: I1018 22:03:27.356578   10882 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:03:27 k8s-master kubelet[10882]: I1018 22:03:27.352953   10882 fs.go:140] Filesystem partitions: map[/dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0} tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0}]
Oct 18 22:03:27 k8s-master kubelet[10882]: I1018 22:03:27.352640   10882 fs.go:139] Filesystem UUIDs: map[a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0 752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1]
Oct 18 22:03:27 k8s-master kubelet[10882]: W1018 22:03:27.332953   10882 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:03:27 k8s-master kubelet[10882]: W1018 22:03:27.332737   10882 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:03:27 k8s-master kubelet[10882]: I1018 22:03:27.311304   10882 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:03:27 k8s-master kubelet[10882]: E1018 22:03:27.310576   10882 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:03:27 k8s-master kubelet[10882]: I1018 22:03:27.310016   10882 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:03:27 k8s-master kubelet[10882]: W1018 22:03:27.272771   10882 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:03:27 k8s-master kubelet[10882]: I1018 22:03:27.272573   10882 feature_gate.go:156] feature gates: map[]
Oct 18 22:03:27 k8s-master kubelet[10882]: W1018 22:03:27.250430   10882 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:03:27 k8s-master kubelet[10882]: I1018 22:03:27.244351   10882 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:03:27 k8s-master kubelet[10882]: I1018 22:03:27.244302   10882 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:03:27 k8s-master kubelet[10882]: I1018 22:03:27.228925   10882 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:03:27 k8s-master kubelet[10882]: I1018 22:03:27.228902   10882 controller.go:114] kubelet config controller: starting controller
Oct 18 22:03:27 k8s-master kubelet[10882]: I1018 22:03:27.224226   10882 feature_gate.go:156] feature gates: map[]
Oct 18 22:03:27 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:03:27 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:03:27 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:03:16 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:03:16 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:03:16 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:03:16 k8s-master kubelet[10862]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2300        -1]
Oct 18 22:03:16 k8s-master kubelet[10862]: I1018 22:03:16.788506   10862 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:03:16 k8s-master kubelet[10862]: I1018 22:03:16.786829   10862 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:03:16 k8s-master kubelet[10862]: I1018 22:03:16.784154   10862 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:03:16 k8s-master kubelet[10862]: I1018 22:03:16.782824   10862 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0}]
Oct 18 22:03:16 k8s-master kubelet[10862]: I1018 22:03:16.782789   10862 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 22:03:16 k8s-master kubelet[10862]: W1018 22:03:16.766341   10862 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:03:16 k8s-master kubelet[10862]: W1018 22:03:16.766151   10862 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:03:16 k8s-master kubelet[10862]: I1018 22:03:16.742563   10862 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:03:16 k8s-master kubelet[10862]: E1018 22:03:16.740517   10862 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:03:16 k8s-master kubelet[10862]: I1018 22:03:16.739001   10862 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:03:16 k8s-master kubelet[10862]: W1018 22:03:16.709743   10862 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:03:16 k8s-master kubelet[10862]: I1018 22:03:16.709596   10862 feature_gate.go:156] feature gates: map[]
Oct 18 22:03:16 k8s-master kubelet[10862]: W1018 22:03:16.698167   10862 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:03:16 k8s-master kubelet[10862]: I1018 22:03:16.695317   10862 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:03:16 k8s-master kubelet[10862]: I1018 22:03:16.695206   10862 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:03:16 k8s-master kubelet[10862]: I1018 22:03:16.686240   10862 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:03:16 k8s-master kubelet[10862]: I1018 22:03:16.686234   10862 controller.go:114] kubelet config controller: starting controller
Oct 18 22:03:16 k8s-master kubelet[10862]: I1018 22:03:16.686140   10862 feature_gate.go:156] feature gates: map[]
Oct 18 22:03:16 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:03:16 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:03:16 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:03:06 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:03:06 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:03:06 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:03:06 k8s-master kubelet[10842]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2300        -1]
Oct 18 22:03:06 k8s-master kubelet[10842]: I1018 22:03:06.268589   10842 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:03:06 k8s-master kubelet[10842]: I1018 22:03:06.267942   10842 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:03:06 k8s-master kubelet[10842]: I1018 22:03:06.261182   10842 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:03:06 k8s-master kubelet[10842]: I1018 22:03:06.258556   10842 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0}]
Oct 18 22:03:06 k8s-master kubelet[10842]: I1018 22:03:06.258518   10842 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 22:03:06 k8s-master kubelet[10842]: W1018 22:03:06.242314   10842 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:03:06 k8s-master kubelet[10842]: W1018 22:03:06.242186   10842 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:03:06 k8s-master kubelet[10842]: I1018 22:03:06.224119   10842 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:03:06 k8s-master kubelet[10842]: E1018 22:03:06.223495   10842 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:03:06 k8s-master kubelet[10842]: I1018 22:03:06.223012   10842 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:03:06 k8s-master kubelet[10842]: W1018 22:03:06.193553   10842 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:03:06 k8s-master kubelet[10842]: I1018 22:03:06.193332   10842 feature_gate.go:156] feature gates: map[]
Oct 18 22:03:06 k8s-master kubelet[10842]: W1018 22:03:06.180123   10842 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:03:06 k8s-master kubelet[10842]: I1018 22:03:06.177375   10842 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:03:06 k8s-master kubelet[10842]: I1018 22:03:06.177315   10842 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:03:06 k8s-master kubelet[10842]: I1018 22:03:06.161821   10842 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:03:06 k8s-master kubelet[10842]: I1018 22:03:06.161808   10842 controller.go:114] kubelet config controller: starting controller
Oct 18 22:03:06 k8s-master kubelet[10842]: I1018 22:03:06.161716   10842 feature_gate.go:156] feature gates: map[]
Oct 18 22:03:06 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:03:06 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:03:06 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:02:55 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:02:55 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:02:55 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:02:55 k8s-master kubelet[10822]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2300        -1]
Oct 18 22:02:55 k8s-master kubelet[10822]: I1018 22:02:55.766422   10822 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:02:55 k8s-master kubelet[10822]: I1018 22:02:55.765417   10822 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:02:55 k8s-master kubelet[10822]: I1018 22:02:55.761413   10822 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:02:55 k8s-master kubelet[10822]: I1018 22:02:55.760104   10822 fs.go:140] Filesystem partitions: map[/dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0} tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0}]
Oct 18 22:02:55 k8s-master kubelet[10822]: I1018 22:02:55.760077   10822 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 22:02:55 k8s-master kubelet[10822]: W1018 22:02:55.745444   10822 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:02:55 k8s-master kubelet[10822]: W1018 22:02:55.745314   10822 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:02:55 k8s-master kubelet[10822]: I1018 22:02:55.730084   10822 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:02:55 k8s-master kubelet[10822]: E1018 22:02:55.729389   10822 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:02:55 k8s-master kubelet[10822]: I1018 22:02:55.728932   10822 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:02:55 k8s-master kubelet[10822]: W1018 22:02:55.704166   10822 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:02:55 k8s-master kubelet[10822]: I1018 22:02:55.704023   10822 feature_gate.go:156] feature gates: map[]
Oct 18 22:02:55 k8s-master kubelet[10822]: W1018 22:02:55.694931   10822 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:02:55 k8s-master kubelet[10822]: I1018 22:02:55.692128   10822 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:02:55 k8s-master kubelet[10822]: I1018 22:02:55.692087   10822 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:02:55 k8s-master kubelet[10822]: I1018 22:02:55.682844   10822 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:02:55 k8s-master kubelet[10822]: I1018 22:02:55.682838   10822 controller.go:114] kubelet config controller: starting controller
Oct 18 22:02:55 k8s-master kubelet[10822]: I1018 22:02:55.682734   10822 feature_gate.go:156] feature gates: map[]
Oct 18 22:02:55 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:02:55 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:02:55 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:02:45 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:02:45 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:02:45 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:02:45 k8s-master kubelet[10801]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2300        -1]
Oct 18 22:02:45 k8s-master kubelet[10801]: I1018 22:02:45.274558   10801 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:02:45 k8s-master kubelet[10801]: I1018 22:02:45.273965   10801 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:02:45 k8s-master kubelet[10801]: I1018 22:02:45.270506   10801 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true}] DiskMap:map[8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq} 253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:02:45 k8s-master kubelet[10801]: I1018 22:02:45.269367   10801 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0}]
Oct 18 22:02:45 k8s-master kubelet[10801]: I1018 22:02:45.269325   10801 fs.go:139] Filesystem UUIDs: map[f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0 752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1]
Oct 18 22:02:45 k8s-master kubelet[10801]: W1018 22:02:45.248629   10801 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:02:45 k8s-master kubelet[10801]: W1018 22:02:45.248460   10801 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:02:45 k8s-master kubelet[10801]: I1018 22:02:45.223604   10801 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:02:45 k8s-master kubelet[10801]: E1018 22:02:45.222211   10801 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:02:45 k8s-master kubelet[10801]: I1018 22:02:45.221809   10801 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:02:45 k8s-master kubelet[10801]: W1018 22:02:45.196373   10801 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:02:45 k8s-master kubelet[10801]: I1018 22:02:45.196231   10801 feature_gate.go:156] feature gates: map[]
Oct 18 22:02:45 k8s-master kubelet[10801]: W1018 22:02:45.184056   10801 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:02:45 k8s-master kubelet[10801]: I1018 22:02:45.181810   10801 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:02:45 k8s-master kubelet[10801]: I1018 22:02:45.181770   10801 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:02:45 k8s-master kubelet[10801]: I1018 22:02:45.167208   10801 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:02:45 k8s-master kubelet[10801]: I1018 22:02:45.167166   10801 controller.go:114] kubelet config controller: starting controller
Oct 18 22:02:45 k8s-master kubelet[10801]: I1018 22:02:45.166603   10801 feature_gate.go:156] feature gates: map[]
Oct 18 22:02:45 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:02:45 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:02:45 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:02:34 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:02:34 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:02:34 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:02:34 k8s-master kubelet[10781]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2300        -1]
Oct 18 22:02:34 k8s-master kubelet[10781]: I1018 22:02:34.891911   10781 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:02:34 k8s-master kubelet[10781]: I1018 22:02:34.891345   10781 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:02:34 k8s-master kubelet[10781]: I1018 22:02:34.888186   10781 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true}] DiskMap:map[8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq} 253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:02:34 k8s-master kubelet[10781]: I1018 22:02:34.885908   10781 fs.go:140] Filesystem partitions: map[/dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0} tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0}]
Oct 18 22:02:34 k8s-master kubelet[10781]: I1018 22:02:34.885877   10781 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 22:02:34 k8s-master kubelet[10781]: W1018 22:02:34.867295   10781 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:02:34 k8s-master kubelet[10781]: W1018 22:02:34.867086   10781 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:02:34 k8s-master kubelet[10781]: I1018 22:02:34.810249   10781 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:02:34 k8s-master kubelet[10781]: E1018 22:02:34.809047   10781 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:02:34 k8s-master kubelet[10781]: I1018 22:02:34.804229   10781 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:02:34 k8s-master kubelet[10781]: W1018 22:02:34.769485   10781 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:02:34 k8s-master kubelet[10781]: I1018 22:02:34.769329   10781 feature_gate.go:156] feature gates: map[]
Oct 18 22:02:34 k8s-master kubelet[10781]: W1018 22:02:34.751094   10781 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:02:34 k8s-master kubelet[10781]: I1018 22:02:34.747821   10781 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:02:34 k8s-master kubelet[10781]: I1018 22:02:34.747783   10781 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:02:34 k8s-master kubelet[10781]: I1018 22:02:34.727541   10781 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:02:34 k8s-master kubelet[10781]: I1018 22:02:34.727525   10781 controller.go:114] kubelet config controller: starting controller
Oct 18 22:02:34 k8s-master kubelet[10781]: I1018 22:02:34.727102   10781 feature_gate.go:156] feature gates: map[]
Oct 18 22:02:34 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:02:34 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:02:34 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:02:24 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:02:24 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:02:24 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:02:24 k8s-master kubelet[10632]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2416        -1]
Oct 18 22:02:24 k8s-master kubelet[10632]: I1018 22:02:24.305762   10632 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:02:24 k8s-master kubelet[10632]: I1018 22:02:24.305086   10632 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:02:24 k8s-master kubelet[10632]: I1018 22:02:24.302019   10632 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:02:24 k8s-master kubelet[10632]: I1018 22:02:24.300528   10632 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0}]
Oct 18 22:02:24 k8s-master kubelet[10632]: I1018 22:02:24.300497   10632 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 22:02:24 k8s-master kubelet[10632]: W1018 22:02:24.282420   10632 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:02:24 k8s-master kubelet[10632]: W1018 22:02:24.282267   10632 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:02:24 k8s-master kubelet[10632]: I1018 22:02:24.262353   10632 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:02:24 k8s-master kubelet[10632]: E1018 22:02:24.261752   10632 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:02:24 k8s-master kubelet[10632]: I1018 22:02:24.261284   10632 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:02:24 k8s-master kubelet[10632]: W1018 22:02:24.233539   10632 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:02:24 k8s-master kubelet[10632]: I1018 22:02:24.233394   10632 feature_gate.go:156] feature gates: map[]
Oct 18 22:02:24 k8s-master kubelet[10632]: W1018 22:02:24.222143   10632 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:02:24 k8s-master kubelet[10632]: I1018 22:02:24.218630   10632 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:02:24 k8s-master kubelet[10632]: I1018 22:02:24.218568   10632 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:02:24 k8s-master kubelet[10632]: I1018 22:02:24.201202   10632 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:02:24 k8s-master kubelet[10632]: I1018 22:02:24.201196   10632 controller.go:114] kubelet config controller: starting controller
Oct 18 22:02:24 k8s-master kubelet[10632]: I1018 22:02:24.201095   10632 feature_gate.go:156] feature gates: map[]
Oct 18 22:02:24 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:02:24 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:02:24 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:02:14 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:02:14 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:02:14 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:02:14 k8s-master kubelet[10610]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2420        -1]
Oct 18 22:02:14 k8s-master kubelet[10610]: I1018 22:02:14.011145   10610 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:02:14 k8s-master kubelet[10610]: I1018 22:02:14.010557   10610 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:02:14 k8s-master kubelet[10610]: I1018 22:02:14.007477   10610 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true}] DiskMap:map[253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq} 253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:02:14 k8s-master kubelet[10610]: I1018 22:02:14.003631   10610 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0}]
Oct 18 22:02:14 k8s-master kubelet[10610]: I1018 22:02:14.003582   10610 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 22:02:13 k8s-master kubelet[10610]: W1018 22:02:13.986967   10610 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:02:13 k8s-master kubelet[10610]: W1018 22:02:13.986819   10610 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:02:13 k8s-master kubelet[10610]: I1018 22:02:13.969805   10610 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:02:13 k8s-master kubelet[10610]: E1018 22:02:13.969027   10610 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:02:13 k8s-master kubelet[10610]: I1018 22:02:13.968525   10610 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:02:13 k8s-master kubelet[10610]: W1018 22:02:13.942861   10610 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:02:13 k8s-master kubelet[10610]: I1018 22:02:13.942722   10610 feature_gate.go:156] feature gates: map[]
Oct 18 22:02:13 k8s-master kubelet[10610]: W1018 22:02:13.933864   10610 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:02:13 k8s-master kubelet[10610]: I1018 22:02:13.932234   10610 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:02:13 k8s-master kubelet[10610]: I1018 22:02:13.932200   10610 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:02:13 k8s-master kubelet[10610]: I1018 22:02:13.920634   10610 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:02:13 k8s-master kubelet[10610]: I1018 22:02:13.920624   10610 controller.go:114] kubelet config controller: starting controller
Oct 18 22:02:13 k8s-master kubelet[10610]: I1018 22:02:13.920528   10610 feature_gate.go:156] feature gates: map[]
Oct 18 22:02:13 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:02:13 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:02:13 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:02:03 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:02:03 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:02:03 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:02:03 k8s-master kubelet[10590]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2420        -1]
Oct 18 22:02:03 k8s-master kubelet[10590]: I1018 22:02:03.545085   10590 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:02:03 k8s-master kubelet[10590]: I1018 22:02:03.544316   10590 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:02:03 k8s-master kubelet[10590]: I1018 22:02:03.537423   10590 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true}] DiskMap:map[8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq} 253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:02:03 k8s-master kubelet[10590]: I1018 22:02:03.534143   10590 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0}]
Oct 18 22:02:03 k8s-master kubelet[10590]: I1018 22:02:03.534114   10590 fs.go:139] Filesystem UUIDs: map[f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0 752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1]
Oct 18 22:02:03 k8s-master kubelet[10590]: W1018 22:02:03.512228   10590 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:02:03 k8s-master kubelet[10590]: W1018 22:02:03.512012   10590 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:02:03 k8s-master kubelet[10590]: I1018 22:02:03.490843   10590 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:02:03 k8s-master kubelet[10590]: E1018 22:02:03.490100   10590 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:02:03 k8s-master kubelet[10590]: I1018 22:02:03.489630   10590 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:02:03 k8s-master kubelet[10590]: W1018 22:02:03.461136   10590 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:02:03 k8s-master kubelet[10590]: I1018 22:02:03.460991   10590 feature_gate.go:156] feature gates: map[]
Oct 18 22:02:03 k8s-master kubelet[10590]: W1018 22:02:03.448439   10590 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:02:03 k8s-master kubelet[10590]: I1018 22:02:03.446499   10590 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:02:03 k8s-master kubelet[10590]: I1018 22:02:03.446444   10590 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:02:03 k8s-master kubelet[10590]: I1018 22:02:03.433750   10590 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:02:03 k8s-master kubelet[10590]: I1018 22:02:03.433744   10590 controller.go:114] kubelet config controller: starting controller
Oct 18 22:02:03 k8s-master kubelet[10590]: I1018 22:02:03.433640   10590 feature_gate.go:156] feature gates: map[]
Oct 18 22:02:03 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:02:03 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:02:03 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:01:53 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:01:53 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:01:53 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:01:53 k8s-master kubelet[10570]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2420        -1]
Oct 18 22:01:53 k8s-master kubelet[10570]: I1018 22:01:53.032514   10570 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:01:53 k8s-master kubelet[10570]: I1018 22:01:53.031904   10570 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:01:53 k8s-master kubelet[10570]: I1018 22:01:53.025573   10570 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true} {Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true}] DiskMap:map[8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq} 253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:01:53 k8s-master kubelet[10570]: I1018 22:01:53.023011   10570 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0}]
Oct 18 22:01:53 k8s-master kubelet[10570]: I1018 22:01:53.022973   10570 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 22:01:53 k8s-master kubelet[10570]: W1018 22:01:53.004100   10570 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:01:53 k8s-master kubelet[10570]: W1018 22:01:53.003898   10570 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:01:52 k8s-master kubelet[10570]: I1018 22:01:52.983429   10570 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:01:52 k8s-master kubelet[10570]: E1018 22:01:52.982528   10570 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:01:52 k8s-master kubelet[10570]: I1018 22:01:52.981898   10570 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:01:52 k8s-master kubelet[10570]: W1018 22:01:52.951638   10570 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:01:52 k8s-master kubelet[10570]: I1018 22:01:52.951422   10570 feature_gate.go:156] feature gates: map[]
Oct 18 22:01:52 k8s-master kubelet[10570]: W1018 22:01:52.942946   10570 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:01:52 k8s-master kubelet[10570]: I1018 22:01:52.938790   10570 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:01:52 k8s-master kubelet[10570]: I1018 22:01:52.938742   10570 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:01:52 k8s-master kubelet[10570]: I1018 22:01:52.929991   10570 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:01:52 k8s-master kubelet[10570]: I1018 22:01:52.929980   10570 controller.go:114] kubelet config controller: starting controller
Oct 18 22:01:52 k8s-master kubelet[10570]: I1018 22:01:52.929885   10570 feature_gate.go:156] feature gates: map[]
Oct 18 22:01:52 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:01:52 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:01:52 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:01:42 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:01:42 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:01:42 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:01:42 k8s-master kubelet[10550]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2420        -1]
Oct 18 22:01:42 k8s-master kubelet[10550]: I1018 22:01:42.582831   10550 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:01:42 k8s-master kubelet[10550]: I1018 22:01:42.581982   10550 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:01:42 k8s-master kubelet[10550]: I1018 22:01:42.578197   10550 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:01:42 k8s-master kubelet[10550]: I1018 22:01:42.576111   10550 fs.go:140] Filesystem partitions: map[/dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0} tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0}]
Oct 18 22:01:42 k8s-master kubelet[10550]: I1018 22:01:42.576066   10550 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 22:01:42 k8s-master kubelet[10550]: W1018 22:01:42.553916   10550 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:01:42 k8s-master kubelet[10550]: W1018 22:01:42.553794   10550 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:01:42 k8s-master kubelet[10550]: I1018 22:01:42.539668   10550 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:01:42 k8s-master kubelet[10550]: E1018 22:01:42.539094   10550 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:01:42 k8s-master kubelet[10550]: I1018 22:01:42.538630   10550 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:01:42 k8s-master kubelet[10550]: W1018 22:01:42.512823   10550 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:01:42 k8s-master kubelet[10550]: I1018 22:01:42.512671   10550 feature_gate.go:156] feature gates: map[]
Oct 18 22:01:42 k8s-master kubelet[10550]: W1018 22:01:42.498914   10550 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:01:42 k8s-master kubelet[10550]: I1018 22:01:42.496907   10550 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:01:42 k8s-master kubelet[10550]: I1018 22:01:42.496860   10550 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:01:42 k8s-master kubelet[10550]: I1018 22:01:42.484038   10550 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:01:42 k8s-master kubelet[10550]: I1018 22:01:42.484031   10550 controller.go:114] kubelet config controller: starting controller
Oct 18 22:01:42 k8s-master kubelet[10550]: I1018 22:01:42.483939   10550 feature_gate.go:156] feature gates: map[]
Oct 18 22:01:42 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:01:42 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:01:42 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:01:32 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:01:32 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:01:32 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:01:32 k8s-master kubelet[10530]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2420        -1]
Oct 18 22:01:32 k8s-master kubelet[10530]: I1018 22:01:32.028436   10530 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:01:32 k8s-master kubelet[10530]: I1018 22:01:32.022827   10530 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:01:32 k8s-master kubelet[10530]: I1018 22:01:32.019975   10530 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:01:32 k8s-master kubelet[10530]: I1018 22:01:32.018475   10530 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0}]
Oct 18 22:01:32 k8s-master kubelet[10530]: I1018 22:01:32.018433   10530 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 22:01:32 k8s-master kubelet[10530]: W1018 22:01:32.001377   10530 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:01:32 k8s-master kubelet[10530]: W1018 22:01:32.001177   10530 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:01:31 k8s-master kubelet[10530]: I1018 22:01:31.981475   10530 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:01:31 k8s-master kubelet[10530]: E1018 22:01:31.980927   10530 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:01:31 k8s-master kubelet[10530]: I1018 22:01:31.980442   10530 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:01:31 k8s-master kubelet[10530]: W1018 22:01:31.954789   10530 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:01:31 k8s-master kubelet[10530]: I1018 22:01:31.954644   10530 feature_gate.go:156] feature gates: map[]
Oct 18 22:01:31 k8s-master kubelet[10530]: W1018 22:01:31.941315   10530 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:01:31 k8s-master kubelet[10530]: I1018 22:01:31.936948   10530 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:01:31 k8s-master kubelet[10530]: I1018 22:01:31.936904   10530 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:01:31 k8s-master kubelet[10530]: I1018 22:01:31.928098   10530 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:01:31 k8s-master kubelet[10530]: I1018 22:01:31.928092   10530 controller.go:114] kubelet config controller: starting controller
Oct 18 22:01:31 k8s-master kubelet[10530]: I1018 22:01:31.927986   10530 feature_gate.go:156] feature gates: map[]
Oct 18 22:01:31 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:01:31 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:01:31 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:01:21 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:01:21 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:01:21 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:01:21 k8s-master kubelet[10502]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2420        -1]
Oct 18 22:01:21 k8s-master kubelet[10502]: I1018 22:01:21.750949   10502 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:01:21 k8s-master kubelet[10502]: I1018 22:01:21.750388   10502 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:01:21 k8s-master kubelet[10502]: I1018 22:01:21.746405   10502 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:01:21 k8s-master kubelet[10502]: I1018 22:01:21.745740   10502 fs.go:140] Filesystem partitions: map[/dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0} tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0}]
Oct 18 22:01:21 k8s-master kubelet[10502]: I1018 22:01:21.745713   10502 fs.go:139] Filesystem UUIDs: map[752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1 a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0]
Oct 18 22:01:21 k8s-master kubelet[10502]: W1018 22:01:21.730880   10502 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:01:21 k8s-master kubelet[10502]: W1018 22:01:21.730754   10502 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:01:21 k8s-master kubelet[10502]: I1018 22:01:21.712132   10502 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:01:21 k8s-master kubelet[10502]: E1018 22:01:21.711571   10502 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:01:21 k8s-master kubelet[10502]: I1018 22:01:21.710207   10502 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:01:21 k8s-master kubelet[10502]: W1018 22:01:21.686677   10502 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:01:21 k8s-master kubelet[10502]: I1018 22:01:21.686510   10502 feature_gate.go:156] feature gates: map[]
Oct 18 22:01:21 k8s-master kubelet[10502]: W1018 22:01:21.673920   10502 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:01:21 k8s-master kubelet[10502]: I1018 22:01:21.669560   10502 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:01:21 k8s-master kubelet[10502]: I1018 22:01:21.669525   10502 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:01:21 k8s-master kubelet[10502]: I1018 22:01:21.661374   10502 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:01:21 k8s-master kubelet[10502]: I1018 22:01:21.661361   10502 controller.go:114] kubelet config controller: starting controller
Oct 18 22:01:21 k8s-master kubelet[10502]: I1018 22:01:21.661218   10502 feature_gate.go:156] feature gates: map[]
Oct 18 22:01:21 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:01:21 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:01:21 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:01:11 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:01:11 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:01:11 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:01:11 k8s-master kubelet[10482]: error: failed to run Kubelet: Running with swap on is not supported, please disable swap! or set --fail-swap-on flag to false. /proc/swaps contained: [Filename                                Type                Size        Used        Priority /dev/dm-1                               partition        2097148        2428        -1]
Oct 18 22:01:11 k8s-master kubelet[10482]: I1018 22:01:11.274330   10482 server.go:422] --cgroups-per-qos enabled, but --cgroup-root was not specified.  defaulting to /
Oct 18 22:01:11 k8s-master kubelet[10482]: I1018 22:01:11.273727   10482 manager.go:222] Version: {KernelVersion:3.10.0-327.el7.x86_64 ContainerOsVersion:CentOS Linux 7 (Core) DockerVersion:17.10.0-ce DockerAPIVersion:1.33 CadvisorVersion: CadvisorRevision:}
Oct 18 22:01:11 k8s-master kubelet[10482]: I1018 22:01:11.270317   10482 manager.go:216] Machine: {NumCores:1 CpuFrequency:2197952 MemoryCapacity:1929736192 HugePages:[{PageSize:2048 NumPages:0}] MachineID:c66a6c8a21e94bc998b5565bb96f6592 SystemUUID:36916406-86BE-449C-85B5-77D56AA12F3C BootID:42bddc38-c775-49e9-8cae-eeb8a8751444 Filesystems:[{Device:tmpfs DeviceMajor:0 DeviceMinor:17 Capacity:964866048 Type:vfs Inodes:235563 HasInodes:true} {Device:/dev/mapper/centos-root DeviceMajor:253 DeviceMinor:0 Capacity:40207929344 Type:vfs Inodes:39284736 HasInodes:true} {Device:/dev/sda1 DeviceMajor:8 DeviceMinor:1 Capacity:520794112 Type:vfs Inodes:512000 HasInodes:true}] DiskMap:map[253:0:{Name:dm-0 Major:253 Minor:0 Size:40227569664 Scheduler:none} 253:1:{Name:dm-1 Major:253 Minor:1 Size:2147483648 Scheduler:none} 8:0:{Name:sda Major:8 Minor:0 Size:42949672960 Scheduler:cfq}] NetworkDevices:[{Name:enp0s3 MacAddress:08:00:27:82:87:3b Speed:1000 Mtu:1500} {Name:virbr0 MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500} {Name:virbr0-nic MacAddress:52:54:00:05:b0:63 Speed:0 Mtu:1500}] Topology:[{Id:0 Memory:2147016704 Cores:[{Id:0 Threads:[0] Caches:[{Size:32768 Type:Data Level:1} {Size:32768 Type:Data Level:1} {Size:6291456 Type:Data Level:2}]}] Caches:[]}] CloudProvider:Unknown InstanceType:Unknown InstanceID:None}
Oct 18 22:01:11 k8s-master kubelet[10482]: I1018 22:01:11.267131   10482 fs.go:140] Filesystem partitions: map[tmpfs:{mountpoint:/dev/shm major:0 minor:17 fsType:tmpfs blockSize:0} /dev/mapper/centos-root:{mountpoint:/var/lib/docker/overlay major:253 minor:0 fsType:xfs blockSize:0} /dev/sda1:{mountpoint:/boot major:8 minor:1 fsType:xfs blockSize:0}]
Oct 18 22:01:11 k8s-master kubelet[10482]: I1018 22:01:11.267101   10482 fs.go:139] Filesystem UUIDs: map[a6991ce7-bf54-4e6b-a5a7-c44ae3961b76:/dev/sda1 f193d77f-0133-40d1-a260-a05d4a8f2b7e:/dev/dm-0 752e88d9-1b24-4fe7-a0dd-23967413b4c4:/dev/dm-1]
Oct 18 22:01:11 k8s-master kubelet[10482]: W1018 22:01:11.250220   10482 manager.go:166] unable to connect to CRI-O api service: Get http://%2Fvar%2Frun%2Fcrio.sock/info: dial unix /var/run/crio.sock: connect: no such file or directory
Oct 18 22:01:11 k8s-master kubelet[10482]: W1018 22:01:11.250098   10482 manager.go:157] unable to connect to Rkt api service: rkt: cannot tcp Dial rkt api service: dial tcp [::1]:15441: getsockopt: connection refused
Oct 18 22:01:11 k8s-master kubelet[10482]: I1018 22:01:11.224174   10482 manager.go:149] cAdvisor running in container: "/sys/fs/cgroup/cpu,cpuacct/system.slice/kubelet.service"
Oct 18 22:01:11 k8s-master kubelet[10482]: E1018 22:01:11.221878   10482 certificate_manager.go:284] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Post https://192.168.0.155:6443/apis/certificates.k8s.io/v1beta1/certificatesigningrequests: dial tcp 192.168.0.155:6443: getsockopt: connection refused
Oct 18 22:01:11 k8s-master kubelet[10482]: I1018 22:01:11.220967   10482 certificate_manager.go:361] Requesting new certificate.
Oct 18 22:01:11 k8s-master kubelet[10482]: W1018 22:01:11.186947   10482 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:01:11 k8s-master kubelet[10482]: I1018 22:01:11.186728   10482 feature_gate.go:156] feature gates: map[]
Oct 18 22:01:11 k8s-master kubelet[10482]: W1018 22:01:11.174006   10482 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:01:11 k8s-master kubelet[10482]: I1018 22:01:11.172065   10482 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:01:11 k8s-master kubelet[10482]: I1018 22:01:11.172026   10482 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:01:11 k8s-master kubelet[10482]: I1018 22:01:11.160859   10482 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:01:11 k8s-master kubelet[10482]: I1018 22:01:11.160853   10482 controller.go:114] kubelet config controller: starting controller
Oct 18 22:01:11 k8s-master kubelet[10482]: I1018 22:01:11.160613   10482 feature_gate.go:156] feature gates: map[]
Oct 18 22:01:11 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:01:11 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
Oct 18 22:01:11 k8s-master systemd[1]: kubelet.service holdoff time over, scheduling restart.
Oct 18 22:01:00 k8s-master systemd[1]: kubelet.service failed.
Oct 18 22:01:00 k8s-master systemd[1]: Unit kubelet.service entered failed state.
Oct 18 22:01:00 k8s-master systemd[1]: kubelet.service: main process exited, code=exited, status=1/FAILURE
Oct 18 22:01:00 k8s-master kubelet[10332]: error: failed to run Kubelet: unable to load bootstrap kubeconfig: stat /etc/kubernetes/bootstrap-kubelet.conf: no such file or directory
Oct 18 22:01:00 k8s-master kubelet[10332]: W1018 22:01:00.842070   10332 server.go:289] --cloud-provider=auto-detect is deprecated. The desired cloud provider should be set explicitly
Oct 18 22:01:00 k8s-master kubelet[10332]: I1018 22:01:00.841663   10332 feature_gate.go:156] feature gates: map[]
Oct 18 22:01:00 k8s-master kubelet[10332]: W1018 22:01:00.791660   10332 cni.go:196] Unable to update cni config: No networks found in /etc/cni/net.d
Oct 18 22:01:00 k8s-master kubelet[10332]: I1018 22:01:00.744256   10332 client.go:95] Start docker client with request timeout=2m0s
Oct 18 22:01:00 k8s-master kubelet[10332]: I1018 22:01:00.744213   10332 client.go:75] Connecting to docker on unix:///var/run/docker.sock
Oct 18 22:00:59 k8s-master kubelet[10332]: I1018 22:00:59.783097   10332 controller.go:118] kubelet config controller: validating combination of defaults and flags
Oct 18 22:00:59 k8s-master kubelet[10332]: I1018 22:00:59.783067   10332 controller.go:114] kubelet config controller: starting controller
Oct 18 22:00:59 k8s-master kubelet[10332]: I1018 22:00:59.782767   10332 feature_gate.go:156] feature gates: map[]
Oct 18 22:00:59 k8s-master systemd[1]: Starting kubelet: The Kubernetes Node Agent...
Oct 18 22:00:59 k8s-master systemd[1]: Started kubelet: The Kubernetes Node Agent.
